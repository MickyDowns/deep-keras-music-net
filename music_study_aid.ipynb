{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install image # install's PIL, the python image library\n",
    "# install portaudio is required by pyaudio (http://portaudio.com/docs/v19-doxydocs/tutorial_start.html)\n",
    "pip install pyaudio # for recording within python. used for audioSearch module\n",
    "pip install pygame # for realtime MIDI performance in midi.realtime module\n",
    "# install muscore for viewing and editing music notation (http://www.musescore.org)\n",
    "# install lilypond for displaying musical scores (http://lilypond.org/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# directory, files management, etc.\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "from StringIO import StringIO\n",
    "import time\n",
    "\n",
    "# arrays, dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# utilities\n",
    "import random as rand\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# audio\n",
    "import wave\n",
    "#import pyaudio\n",
    "\n",
    "# MIR\n",
    "import essentia\n",
    "import madmom as mad\n",
    "import music21 as m21\n",
    "\n",
    "# neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pitches, notes\n",
    "NOTE_CLASSES = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "NUM_PITCH_CLASS = len(NOTES)\n",
    "NUM_MIDI_PITCH = 127               # range of audible sounds\n",
    "RANGE_PIANO_PITCH = range(21,109)  # 88 notes (12 notes * 7.25 octave scale), midi 21:108\n",
    "NUM_PIANO_PITCH = len(RANGE_PIANO_PITCH)\n",
    "\n",
    "# chords\n",
    "NUM_COM_CHORD_OCTAVE = 5\n",
    "#NUM_ALL_CHORD_PITCH = NUM_PIANO_PITCH # midi 21:108 inclusive. 7.25 octave piano scale\n",
    "#RANGE_COM_CORD_PITCH = range(36,96) # midi 36:95 inclusive, 5 octave piano scale\n",
    "#NUM_COM_CHORD_PITCH = len(RANGE_COM_CHORD_PITCH)\n",
    "NUM_MAJOR_MINOR = 2\n",
    "CHORD_KEY_SETS = 1 # only 3-key chords \n",
    "\n",
    "# loudness\n",
    "RANGE_MEZZO_FORTE = range(60, 69)\n",
    "RANGE_PIANO_FORTE = range(32, 97)\n",
    "\n",
    "# classes\n",
    "NUM_NOT_TGT_CLASS = 1\n",
    "NOT_TGT_LABEL = [\"not target\"]\n",
    "NUM_PITCH_CLASSES = NUM_PIANO_PITCH + NUM_NOT_TGT_CLASS\n",
    "NUM_CHORD_CLASSES = (NUM_PITCH_CLASS * NUM_COM_CHORD_OCTAVE * NUM_MAJOR_MINOR * CHORD_KEY_SETS) + NUM_NOT_TGT_CLASS\n",
    "\n",
    "PITCH_CLASSES = RANGE_PIANO_PITCH\n",
    "PITCH_CLASSES = NOT_TGT_LABEL + PITCH_CLASSES\n",
    "\n",
    "CHORD_CLASSES = []\n",
    "for k in range(3,8):\n",
    "    for i in NOTES:\n",
    "        for j in [\"major\", \"minor\"]:\n",
    "            CHORD_CLASSES.append(str(i+str(k)+\"-\"+j+\" triad\"))\n",
    "CHORD_CLASSES = NOT_TGT_LABEL + CHORD_CLASSES\n",
    "\n",
    "# to standardize feature extract algos\n",
    "NUM_SAMPLES = 22050 # 44100\n",
    "NUM_FRAMES = 1024 # 2048\n",
    "NUM_HOPS = 512 # 441\n",
    "NUM_BANDS = 24 # 48\n",
    "\n",
    "# other \n",
    "dat_dir = 'data/maps/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shake_n_bake_chords():\n",
    "    \n",
    "    #chord_dirs = ['C0-3-7','C0-4-7'] \n",
    "    # 530 major minor triads. 22 (4%) not recognized due to onset time rounding (i.e., note(s) not started w/in 0.01)\n",
    "    # of 508 remaining, 231 are \"valid\", of 277 \"invalid\" 177 are out of octave range, 100 are flats\n",
    "\n",
    "    #chord_dirs = ['C0-3','C0-4','C0-3-6', 'C0-3-8','C0-4-6','C0-4-8','C0-3-7-9','C0-4-7-9']\n",
    "    # of 1,602 files (chords), 138 (9%) are \"valid\" chords. That is m21 considers 136 C0-3-8's and two C0-3-7-9's valid.\n",
    "    # e.g., [40,43,48] is C3-major triad, while [40,43,47] is E2-minor triad. bit strange, but given that we have to \n",
    "    # standardize on something, it may as well be m21. \n",
    "\n",
    "    #fnames = []\n",
    "    #for i in range(len(chord_dirs)):\n",
    "    #    dname = ''.join(['data/maps/*/UCHO/*/',chord_dirs[i],'/*.txt'])\n",
    "    #    fnames = fnames + glob.glob(dname)\n",
    "    \n",
    "    # so, : 1. run thru a ton of files flagging \"valids\", 2. set a baseline set of files and get valid mix, \n",
    "    # 3. re-sample to get to 1/3rd+ valids. \n",
    "\n",
    "    # find valid chords in UCHO (common chord) directories\n",
    "    fnames = glob.glob('data/maps/*/UCHO/*/*/*.txt') # 15,930 files, 826 valids\n",
    "\n",
    "    valids = []\n",
    "    for i in range(len(fnames)):\n",
    "        valids = valids + txt_to_y(fnames[i], 1.5, 150)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"files processed:\", i)\n",
    "            print(\"valid chords:\", sum(valids))\n",
    "            print\n",
    "\n",
    "    reSampUCHO = [fnames[i] for i, j in enumerate(valids) if j == 1]\n",
    "\n",
    "    # find valid chords in RANC (randomly generated chord) directories\n",
    "    fnames = glob.glob('data/maps/*/RAND/*/*/*/*.txt') # 10,800 files, 153 valids\n",
    "\n",
    "    valids = []\n",
    "    for i in range(len(fnames)):\n",
    "        valids = valids + txt_to_y(fnames[i], 1.5, 150)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"files processed:\", i)\n",
    "            print(\"valid chords:\", sum(valids))\n",
    "            print\n",
    "\n",
    "    reSampRAND = [fnames[i] for i, j in enumerate(valids) if j == 1]\n",
    "\n",
    "    # we want our chord algo to be able to select 5-octave major/minors out of commonly occuring chords\n",
    "    # so, we need to mix valids w/ not valids on someting like 1:2 ratio. Given 979 valids, that's 1,958 invalids\n",
    "\n",
    "    valids = reSampUCHO + reSampRAND\n",
    "    save_list(valids, 'data/wip/chords/', 'valids', '.txt')\n",
    "    print(len(valids))\n",
    "\n",
    "    invalids = glob.glob('data/maps/*/UCHO/*/*/*.txt')\n",
    "    invalids = [i for i in invalids if i not in valids]\n",
    "    # note: wile valids are valid UCHO + valid RAND, invalids are total UCHO - valid UCHO\n",
    "\n",
    "    # randomize invalids and select subset\n",
    "    rand.seed(123)\n",
    "    rand.shuffle(invalids)\n",
    "    save_list(invalids, 'data/wip/chords/', 'invalids', '.txt')\n",
    "\n",
    "    # select subset (i.e., first 1958 of randomized list)\n",
    "    invalids = invalids[0:1958]\n",
    "    print(len(invalids))\n",
    "\n",
    "    # add valids\n",
    "    valAndInval = valids + invalids\n",
    "\n",
    "    # randomize\n",
    "    rand.seed(123)\n",
    "    rand.shuffle(valAndInval)\n",
    "    save_list(valAndInval, 'data/wip/chords/', 'valAndInval', '.txt')\n",
    "    print(len(valAndInval))\n",
    "\n",
    "    return(valAndInval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''takes a text file of directories/filenames, samples those names, opens the \n",
    "files, extracts the midi pitch(s) and returns the mean and standard deviation'''\n",
    "\n",
    "def find_mean_sd(inFileList, sampDec):\n",
    "    \n",
    "    rand.seed(123)\n",
    "    rand.shuffle(inFileList)\n",
    "    files = inFileList[0:int(len(inFileList) * sampDec)]\n",
    "    \n",
    "    pitches = []        \n",
    "    for line in files:\n",
    "        onOffs = read_maps_note_file(line)\n",
    "        pitches = pitches + onOffs.MidiPitch.values.tolist()\n",
    "    \n",
    "    print(\"total nans:\", sum(np.isnan(pitches)))\n",
    "    return(len(pitches), np.round(np.nanmean(pitches),2), np.round(np.nanstd(pitches),2))\n",
    "\n",
    "#fnames=[]\n",
    "#for line in open('data/notes/wip/note_dirs.txt', 'U'):\n",
    "#        line = line.rstrip('\\n')\n",
    "#        fnames = fnames + glob.glob('data/maps/' + line + '*.txt')\n",
    "#find_mean_sd(fnames, 0.5) # results: (30723, 64.93, 25.23)\n",
    "note_mean = 64.93\n",
    "note_sd = 25.23\n",
    "\n",
    "#fnames = glob.glob('data/maps/*/UCHO/*/*/*.txt')\n",
    "#find_mean_sd(fnames, 0.5) # results: (29816, 64.76, 23.03)\n",
    "chord_mean = 64.76\n",
    "chord_sd = 23.03\n",
    "\n",
    "#fnames = glob.glob('data/maps/*/MUS/*.txt')\n",
    "#find_mean_sd(fnames, 0.5) # results: (318982, 64.680000000000007, 13.56)\n",
    "music_mean = 64.68\n",
    "music_sd = 13.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Acoustic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Train and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. notes\n",
    "    # a. return file names from ISOL/CH, ISOL/RE and ISOL/TR1-2)\n",
    "fnames = []\n",
    "\n",
    "for line in open('data/notes/wip/note_dirs.txt', 'U'):\n",
    "    line = line.rstrip('\\n')\n",
    "    fnames = fnames + glob.glob(dat_dir + line + '*.mid')\n",
    "    \n",
    "    # b. divide into train (80%) and test (20%), two folds each (divide validation subsequently) \n",
    "seq, folds = assign_files_to_folds(fnames) # returns list \n",
    "\n",
    "    # c. format x & y\n",
    "for i in range(len(seq)): format_xy(\"note\", seq[i], i, folds[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''processing notes in a file using RNNPianoNoteProcessor()'''\n",
    "\n",
    "proc = mad.features.notes.RNNPianoNoteProcessor()\n",
    "\n",
    "act = proc('data/maps/AkPnBcht/ISOL/CH/MAPS_ISOL_CH0.1_F_AkPnBcht.wav')\n",
    "\n",
    "preds = []\n",
    "for i in range(len(act)):\n",
    "    preds.append(act[i].argmax())\n",
    "\n",
    "plt.plot(preds)\n",
    "# NOTE: processed at 100fps. so, to extract actual note for output you'd need to:\n",
    "# 1. go to onset (delta and/or sequential notes), 2. translate midi to note. that's handled below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for a full 176 (i.e., 2*88) incrementing then decrementing piano scale, PT achieves 63% tpr (82% precision,\n",
    "65% recall, 72% f1, 57% accuracy). for pitches between 36 and 93: tpr 96%'''\n",
    "\n",
    "tmp = ! PianoTranscriptor single 'data/maps/AkPnBcht/ISOL/CH/MAPS_ISOL_CH0.1_F_AkPnBcht.wav'\n",
    "detections = mad.features.notes.load_notes(tmp)\n",
    "annotations = mad.utils.midi.process_notes('data/maps/AkPnBcht/ISOL/CH/MAPS_ISOL_CH0.1_F_AkPnBcht.mid')\n",
    "\n",
    "# p129: Evaluation class for measuring Precision, Recall and F-measure of notes.\n",
    "eval_obj = mad.evaluation.notes.NoteEvaluation(detections, annotations, window=0.025, delay=0)\n",
    "\n",
    "# p130 Class for averaging note evaluations.\n",
    "eval_mean = mad.evaluation.notes.NoteMeanEvaluation(eval_obj, name=None)\n",
    "\n",
    "plt.plot(detections[:,1])\n",
    "\n",
    "print(eval_mean.num_tp)\n",
    "print(eval_mean.num_fp)\n",
    "print(eval_mean.num_tn)\n",
    "print(eval_mean.num_fn)\n",
    "print(eval_mean.num_annotations)\n",
    "print(eval_mean.precision)\n",
    "print(eval_mean.recall)\n",
    "print(eval_mean.fmeasure)\n",
    "print(eval_mean.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using external evalute.py\n",
    "\n",
    "mad.features.notes.write_notes(detections, 'data/detections/tst1.notes', fmt=None)\n",
    "mad.features.notes.write_notes(annotations[:,0:2], 'data/annotations/tst1.notes', fmt=None)\n",
    "\n",
    "print(\"*** notes ***\")\n",
    "! evaluate notes -q -a '.notes' --ann_dir 'data/annotations' -d '.notes' --det_dir 'data/detections' 'tst1'\n",
    "print\n",
    "\n",
    "print(\"*** beats ***\")\n",
    "! evaluate beats -q -a '.notes' --ann_dir 'data/annotations' -d '.notes' --det_dir 'data/detections' 'tst1'\n",
    "print\n",
    "\n",
    "#print(\"*** tempo ***\") SEEMS TO REQUIRE STRENGTH\n",
    "#! evaluate tempo -q -a '.notes' --ann_dir 'data/annotations' -d '.notes' --det_dir 'data/detections' 'tst1'\n",
    "#print\n",
    "\n",
    "print(\"*** alignment ***\")\n",
    "! evaluate alignment -q -a '.notes' --ann_dir 'data/annotations' -d '.notes' --det_dir 'data/detections' 'tst1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. chords\n",
    "# problem: 0-3-7 and 0-4-7 target chords are rare in UCHO and RAND datasets. so, you either narrow file range\n",
    "# or just use music files. would be nice to know chords-only model accuracy. \n",
    "\n",
    "    # a. return UCHO and RAND file names\n",
    "        # RUN ONCE: function identifies valid chord files and mixes them among invalid UCHO files\n",
    "# fnames = shake_n_bake_chords()\n",
    "\n",
    "        # RUN AFTER: load resulting file\n",
    "fnames = [fnames.rstrip('\\n') for fnames in open('data/chords/wip/valAndInval.txt', 'U')]\n",
    "\n",
    "# b. divide into train (80%) and test (20%), two folds each (divide validation subsequently) \n",
    "seq, folds = assign_file_to_folds(fnames)\n",
    "\n",
    "# c. format x & y\n",
    "for i in range(len(seq)): format_xy(\"chord\", seq[i], i, folds[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('processing input file #', 0)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.notes.rnn.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdowns/.virtualenvs/audio_2.7_env/lib/python2.7/site-packages/ipykernel/__main__.py:9: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('processing input file #', 1)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 2)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-bach_847_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 3)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-bk_xmas5_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 4)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chp_op31_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 5)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn-p12_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 6)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn-p13_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 7)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn-p14_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 8)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn-p1_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 9)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn-p24_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 10)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn-p3_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 11)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn-p4_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 12)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn-p8_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 13)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn_op25_e2_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 14)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-chpn_op66_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 15)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-gra_esp_4_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 16)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-grieg_waechter_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 17)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-grieg_walzer_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 18)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-grieg_zwerge_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 19)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-hay_40_1_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 20)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-muss_5_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 21)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-mz_331_2_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 22)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-schu_143_1_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 23)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-schub_d760_1_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 24)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-schumm-6_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 25)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-scn15_3_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 26)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-scn15_7_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 27)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-scn16_2_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 28)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-ty_juli_AkPnBcht.notes.rnn.txt\n",
      "('processing input file #', 29)\n",
      "data/wip/AkPnBcht/MUS/MAPS_MUS-ty_mai_AkPnBcht.notes.rnn.txt\n"
     ]
    }
   ],
   "source": [
    "'''for testing performance of madmom crf/cnn chord identification modules. \n",
    "grabs directory files. tests both algorithms against text on/offset file. '''\n",
    "\n",
    "def process_wavs(inWavDir, outTxtDir, outWavDir=None):\n",
    "    '''\n",
    "    reads wav, writes note, cnn chord and dnn chord files to target dir\n",
    "    if tgtWavDir provided, appends chord, note and wav files incrementing. saves wav to target dir. \n",
    "    '''\n",
    "    \n",
    "    inWavs = glob.glob(inWavDir + '/*.wav')\n",
    "    wavs = []; begTm = 0; noteCtr = 0; chordCtr = 0\n",
    "    \n",
    "    for i in range(len(inWavs)):\n",
    "        print(\"processing input file #\", i)\n",
    "        \n",
    "        # process X: chord and note recognition from wave\n",
    "        dnm = os.path.dirname(inWavs[i])\n",
    "        fnm, _ = os.path.splitext(os.path.basename(inWavs[i]))\n",
    "        inNm = dnm + '/' + fnm + '.txt'\n",
    "        outNm = outTxtDir + '/' + fnm\n",
    "\n",
    "        #dnn_chord_rec(inWavs[i], savTo = outNm + '.chords.dnn.txt' )\n",
    "        #cnn_chord_rec(inWavs[i], savTo = outNm + '.chords.cnn.txt'  )\n",
    "        rnn_note_rec(\"single\", inWavs[i], outNm + '.notes.rnn.txt') # outNm + '.notes.txt'\n",
    "        \n",
    "        # MOVE THIS TO RE-FORMATTER \n",
    "        # note save format is onset + midi (as used by eval). note -> chord format is onset + offset + midi\n",
    "        rnnNotes = pd.DataFrame(mad.features.notes.load_notes(outNm + '.notes.rnn.txt'))\n",
    "        rnnNotes.insert(1, \"Offset\", pd.Series(np.zeros(rnnNotes.shape[0]), index=rnnNotes.index))\n",
    "        rnnNotes.columns = ['OnsetTime', 'OffsetTime', 'MidiPitch']\n",
    "        rnnNotes = rnnNotes.sort_values(['OnsetTime', 'MidiPitch'], # 'OffsetTime', \n",
    "                                        axis=0, ascending=True, inplace=False, \n",
    "                                        kind='quicksort', na_position='last')\n",
    "        rnnNotes[\"MidiPitch\"] = rnnNotes['MidiPitch'].astype(int)\n",
    "        \n",
    "        # get chord preds using RNN notes\n",
    "        N, C = txt_to_y(rnnNotes, \"thisFile\")\n",
    "        \n",
    "        np.savetxt(outNm + '.chords.rnn.txt', C,\n",
    "                   fmt=['%.3f', '%.3f', '%d', '%s', '%d', '%s', '%s', '%s', '%s'], delimiter='\\t')\n",
    "        \n",
    "        # process Y: corresponding y values from text files\n",
    "        N, C = txt_to_y(inNm)\n",
    "        \n",
    "        if N is not None:\n",
    "            N['Duration'] = N.OffsetTime - N.OnsetTime\n",
    "            N = N[['OnsetTime', 'MidiPitch', 'Duration' ]]\n",
    "            mad.features.notes.write_notes(np.array(N),\n",
    "                                           outNm + '.note.y.txt',\n",
    "                                           fmt=['%.3f', '%d', '%.3f'])\n",
    "            \n",
    "        if C is not None:\n",
    "            np.savetxt(outNm + '.chord.y.txt', C, \n",
    "                       fmt=['%.3f', '%.3f', '%d', '%s', '%d', '%s', '%s', '%s', '%s'], delimiter='\\t')\n",
    "\n",
    "#process_wavs(inWavDir = 'data/wip/tmp/',\n",
    "#             outTxtDir = 'data/wip/tmp', \n",
    "#             outWavDir=None)\n",
    "    \n",
    "process_wavs(inWavDir = 'data/maps/AkPnBcht/MUS',\n",
    "             outTxtDir = 'data/wip/AkPnBcht/MUS', \n",
    "             outWavDir=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1\n",
      "0  1  2\n",
      "1  3  4\n",
      "   0  1    2\n",
      "0  1  2  0.0\n",
      "1  3  4  0.0\n",
      "   0    3  1    2\n",
      "0  1  0.0  2  0.0\n",
      "1  3  0.0  4  0.0\n"
     ]
    }
   ],
   "source": [
    "# inserting columns into dataframes\n",
    "\n",
    "a = pd.DataFrame(np.array(np.mat('1 2; 3 4')))\n",
    "print(a)\n",
    "a['2'] = pd.Series(np.zeros(a.shape[0]), index=a.index)\n",
    "print(a)\n",
    "\n",
    "a.insert(1, \"3\", pd.Series(np.zeros(a.shape[0]), index=a.index))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''achieves 45% major/minor triad recogniation. significantly slower than dnn model'''\n",
    "\n",
    "def cnn_chord_rec(inWav, savTo=None):\n",
    "    \n",
    "    '''from Filip Korzeniowski and Gerhard Widmer, “A Fully Convolutional Deep Auditory Model for \n",
    "    Musical Chord Recognition”, Proceedings of IEEE International Workshop on Machine Learning for\n",
    "    Signal Processing (MLSP), 2016..'''\n",
    "    \n",
    "    # instantiate madmom CNNChordFeatureProcessor\n",
    "    featproc = mad.features.chords.CNNChordFeatureProcessor()\n",
    "    \n",
    "    # create DeepChromaChordRecognitionProcessor to decode chord sequence from extracted chromas\n",
    "    decode = mad.features.chords.CRFChordRecognitionProcessor()\n",
    "    \n",
    "    # SequentialProcessor links dcp and decode steps to transcribe chord(s)\n",
    "    chordrec = mad.processors.SequentialProcessor([featproc, decode])\n",
    "    \n",
    "    rtrn = chordrec(inWav)\n",
    "    \n",
    "    if savTo is not None: \n",
    "        mad.features.chords.write_chords(rtrn, savTo)\n",
    "    else:\n",
    "        return(pd.DataFrame(rtrn))\n",
    "\n",
    "#cnn_chord_rec('data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav',\n",
    "#              'data/wip/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.madChordCnn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''achieves 35% major/minor triad recogniation'''\n",
    "\n",
    "def dnn_chord_rec(inWav, savTo=None):\n",
    "    \n",
    "    '''from Filip Korzeniowski and Gerhard Widmer, “Feature Learning for Chord Recognition: The Deep\n",
    "    Chroma Extractor”, Proceedings of the 17th International Society for Music Information Retrieval\n",
    "    Conference (ISMIR), 2016.'''\n",
    "    \n",
    "    # instantiate madmom deep chroma processor to extract chroma vectors\n",
    "    dcp = mad.audio.chroma.DeepChromaProcessor()\n",
    "    \n",
    "    # create DeepChromaChordRecognitionProcessor to decode chord sequence from extracted chromas\n",
    "    decode = mad.features.chords.DeepChromaChordRecognitionProcessor()\n",
    "    \n",
    "    # SequentialProcessor links dcp and decode steps to transcribe chord(s)\n",
    "    chordrec = mad.processors.SequentialProcessor([dcp, decode])\n",
    "    \n",
    "    rtrn = chordrec(inWav)\n",
    "    \n",
    "    if savTo is not None: \n",
    "        mad.features.chords.write_chords(rtrn, savTo)\n",
    "    else:\n",
    "        return(pd.DataFrame(rtrn))\n",
    "\n",
    "#dcc_rslts = dcc_chord_rec('data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav')\n",
    "\n",
    "#dcc_rslts = dcc_chord_rec('data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav',\n",
    "#                          'data/wip/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.madChordDcc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_note_rec(mode, inFile, outFile):\n",
    "    \n",
    "    print(outFile)\n",
    "    \n",
    "    if mode == \"single\":\n",
    "        if outFile == None: \n",
    "            rtrn = ! PianoTranscriptor single {inFile}\n",
    "            return(rtrn)\n",
    "        else:\n",
    "            ! PianoTranscriptor single {inFile} -o {outFile}\n",
    "    \n",
    "    elif mode == \"batch\":\n",
    "        if outFile == None:\n",
    "            print(\"ERROR: need an outFile (really dir) when using using batch mode.\")\n",
    "        else:\n",
    "            # assumes inFile is a list of files. assumes outFile is a directory\n",
    "            for i in range(len(inFile)): \n",
    "                ! PianoTranscriptor batch {inFile[i]} -o {outFile}\n",
    "        \n",
    "    # https://www.safaribooksonline.com/blog/2014/02/12/using-shell-commands-effectively-ipython/\n",
    "\n",
    "#rnn_note_rec(['data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav', 'data/maps/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.wav'], 'data/wip/AkPnBcht/MUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''chord rec using external DCChordRecognition'''\n",
    "# $ DCChordRecogniser single INFILE [-o OUTFILE]\n",
    "dcc_chord_detect = ! DCChordRecognition single 'data/mix.wav'\n",
    "dcc_chord_detect = mad.features.chords.load_chords(crf_chord_detect)\n",
    "print(pd.DataFrame(dcc_chord_detect))\n",
    "# can't figure out how to pass LIST OF FILES\n",
    "# $ DCChordRecogniser batch [-o OUTPUT_DIR] [-s OUTPUT_SUFFIX] LIST OF FILES\n",
    "\n",
    "'''chord rec using external CNNChordRecognition'''\n",
    "cnn_chord_detect = ! CNNChordRecognition single 'data/mix.wav'\n",
    "cnn_chord_detect = mad.features.chords.load_chords(cnn_chord_detect)\n",
    "print(pd.DataFrame(cnn_chord_detect))\n",
    "\n",
    "'''note detection using external PianoTranscriptor'''\n",
    "rnn_note_detect = ! PianoTranscriptor single 'data/mix.wav'\n",
    "rnn_note_detect = mad.features.notes.load_notes(rnn_note_detect)\n",
    "pd.DataFrame(rnn_note_detect)\n",
    "\n",
    "#tmp = rnn_note_detect\n",
    "#new_col = []\n",
    "#for i in range(tmp.shape[0]):\n",
    "#    a = m21.note.Note(int(tmp[i,1]))\n",
    "#    new_col.append(a.nameWithOctave)\n",
    "#new_col = np.asarray(new_col)\n",
    "#new_col = new_col.reshape(tmp.shape[0], 1)\n",
    "#tmp = np.append(tmp, new_col, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** 1. how m21 classifies chords:\n",
      "notes:\n",
      "C2\n",
      "C3\n",
      "E4\n",
      "G4\n",
      "C5\n",
      "\n",
      "as taken from MAPS: [72,67,64,36,48]\n",
      "C5-major triad\n",
      "m21 classifies chords based on root, not bass\n",
      "('bass:', <music21.pitch.Pitch C2>)\n",
      "('root:', <music21.pitch.Pitch C5>)\n",
      "root is the note with the most 3rds above it i.e., a pivot point for inversions\n",
      "bass is the lowest note in the chord\n",
      "\n",
      "as sorted: [36,48,64,67,72]\n",
      "sort, shouldn't matter, but it does as classification changes...\n",
      "C2-major triad\n",
      "('bass:', <music21.pitch.Pitch C2>)\n",
      "('root:', <music21.pitch.Pitch C2>)\n",
      "\n",
      "notes:\n",
      "C2\n",
      "E-2\n",
      "G2\n",
      "\n",
      "if we take bottom 3 from MAPS: [43,39,36]\n",
      "C2-minor triad\n",
      "('bass:', <music21.pitch.Pitch C2>)\n",
      "('root:', <music21.pitch.Pitch C2>)\n",
      "\n",
      "bottom three sorted: [36,39,43]\n",
      "C2-minor triad\n",
      "...no problem for simple triads\n",
      "('bass:', <music21.pitch.Pitch C2>)\n",
      "('root:', <music21.pitch.Pitch C2>)\n",
      "\n",
      "******* 2. so, for classification levels:\n",
      "\n",
      "names:\n",
      "B3-phrygian trichord\n",
      "\n",
      "chord root:\n",
      "       root pitch class as str or int:\n",
      "B\n",
      "11\n",
      "       root pitch accidental:\n",
      "<accidental natural>\n",
      "       root pitch octave:\n",
      "3\n",
      "\n",
      "chord quality:\n",
      "minor\n",
      "\n",
      "triads:\n",
      "True\n",
      "False\n",
      "False\n",
      "inversions:\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 2, 4, 5, 6]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"***** 1. how m21 classifies chords:\")\n",
    "print(\"notes:\")\n",
    "print(m21.note.Note(36).pitch)\n",
    "print(m21.note.Note(48).pitch)\n",
    "print(m21.note.Note(64).pitch)\n",
    "print(m21.note.Note(67).pitch)\n",
    "print(m21.note.Note(72).pitch)\n",
    "print\n",
    "\n",
    "# raw data\n",
    "print(\"as taken from MAPS: [72,67,64,36,48]\")\n",
    "print(m21.chord.Chord([72,67,64,36,48]).pitchedCommonName) \n",
    "print(\"m21 classifies chords based on root, not bass\")\n",
    "print(\"bass:\", m21.chord.Chord([72,67,64,36,48]).bass())\n",
    "print(\"root:\", m21.chord.Chord([72,67,64,36,48]).root())\n",
    "print(\"root is the note with the most 3rds above it i.e., a pivot point for inversions\")\n",
    "print(\"bass is the lowest note in the chord\")\n",
    "print\n",
    "\n",
    "print(\"as sorted: [36,48,64,67,72]\")\n",
    "print(\"sort, shouldn't matter, but it does as classification changes...\")\n",
    "print(m21.chord.Chord([36,48,64,67,72]).pitchedCommonName)\n",
    "print(\"bass:\", m21.chord.Chord([36,48,64,67,72]).bass())\n",
    "print(\"root:\", m21.chord.Chord([36,48,64,67,72]).root())\n",
    "print\n",
    "\n",
    "#######\n",
    "\n",
    "print(\"notes:\")\n",
    "print(m21.note.Note(36).pitch)\n",
    "print(m21.note.Note(39).pitch)\n",
    "print(m21.note.Note(43).pitch)\n",
    "print\n",
    "\n",
    "print(\"if we take bottom 3 from MAPS: [43,39,36]\")\n",
    "print(m21.chord.Chord([43,39,36]).pitchedCommonName)\n",
    "print(\"bass:\", m21.chord.Chord([43,39,36]).bass())\n",
    "print(\"root:\", m21.chord.Chord([43,39,36]).root())\n",
    "print\n",
    "\n",
    "print(\"bottom three sorted: [36,39,43]\")\n",
    "print(m21.chord.Chord([36,39,43]).pitchedCommonName)\n",
    "print(\"...no problem for simple triads\")\n",
    "print(\"bass:\", m21.chord.Chord([36,39,43]).bass())\n",
    "print(\"root:\", m21.chord.Chord([36,39,43]).root())\n",
    "print\n",
    "\n",
    "##########\n",
    "\n",
    "print(\"******* 2. so, for classification levels:\")\n",
    "print\n",
    "print(\"names:\")\n",
    "print(m21.chord.Chord([59,62,72]).pitchedCommonName)\n",
    "print\n",
    "\n",
    "print(\"chord root:\")\n",
    "nt = m21.chord.Chord([59,62,72]).root()\n",
    "print(\"       root pitch class as str or int:\")\n",
    "print(nt.pitchClassString)\n",
    "print(nt.pitchClass)\n",
    "#print(nt.pitchClass)\n",
    "print(\"       root pitch accidental:\")\n",
    "print(nt.accidental)\n",
    "print(\"       root pitch octave:\")\n",
    "print(nt.octave)\n",
    "print\n",
    "\n",
    "print(\"chord quality:\")\n",
    "print(m21.chord.Chord([59,62,72]).quality)\n",
    "print\n",
    "\n",
    "print(\"triads:\")\n",
    "print(m21.chord.Chord([59,62,66]).containsTriad())\n",
    "print(m21.chord.Chord([59,62,72]).containsTriad())\n",
    "print(m21.chord.Chord([72,62,52,42]).containsTriad())\n",
    "\n",
    "print(\"inversions:\")\n",
    "print(m21.chord.Chord([59,62,72]).isPrimeFormInversion)\n",
    "print(m21.chord.Chord([59,62,66]).isPrimeFormInversion)\n",
    "\n",
    "m21.chord.Chord([\"D4\", \"D6\", \"F-4\", \"F4\", \"F#4\"]).pitchClasses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('notes:', 1282)\n",
      "('tp:', 1272)\n",
      "('fp:', 49)\n",
      "('tn:', 0)\n",
      "('fn:', 10)\n",
      "\n",
      "('precision:', 0.96)\n",
      "('recall:', 0.99)\n",
      "('fMeasure:', 0.98)\n",
      "('Accuracy:', 0.96)\n",
      "[[  0.51  60.  ]\n",
      " [  0.71  64.  ]\n",
      " [  0.91  67.  ]]\n",
      "(1321, 2)\n",
      "[[  0.5   60.     1.62]\n",
      " [  0.7   64.     1.62]\n",
      " [  0.91  67.     0.6 ]]\n",
      "(1282, 3)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "please implement multi-dim support",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-f9e2f79df4b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_closest_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mdowns/.virtualenvs/audio_2.7_env/lib/python2.7/site-packages/madmom/evaluation/__init__.pyc\u001b[0m in \u001b[0;36mfind_closest_matches\u001b[0;34m(detections, annotations)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# TODO: right now, it only works with 1D arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'please implement multi-dim support'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;31m# if no detections or annotations are given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: please implement multi-dim support"
     ]
    }
   ],
   "source": [
    "# Evaluate notes\n",
    "#annotations = mad.utils.midi.process_notes('data/maps/AkPnBcht/ISOL/CH/MAPS_ISOL_CH0.1_F_AkPnBcht.mid')\n",
    "annotations = mad.features.notes.load_notes('data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.note.y.txt') #data/mix.notes.txt\n",
    "detections = mad.features.notes.load_notes('data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.notes.rnn.txt')\n",
    "\n",
    "# p129: Evaluation class for measuring Precision, Recall and F-measure of notes.\n",
    "eval_obj = mad.evaluation.notes.NoteEvaluation(detections, annotations, window=0.025, delay=0)\n",
    "\n",
    "# p130 Class for averaging note evaluations.\n",
    "eval_mean = mad.evaluation.notes.NoteMeanEvaluation(eval_obj, name=None)\n",
    "\n",
    "#plt.plot(detections[:,1])\n",
    "#plt.plot(annotations[:,1])\n",
    "\n",
    "print(\"notes:\", int(eval_mean.num_annotations))\n",
    "print(\"tp:\", int(eval_mean.num_tp))\n",
    "print(\"fp:\", int(eval_mean.num_fp))\n",
    "print(\"tn:\", int(eval_mean.num_tn))\n",
    "print(\"fn:\", int(eval_mean.num_fn))\n",
    "print\n",
    "print(\"precision:\", round(eval_mean.precision, 2))\n",
    "print(\"recall:\", round(eval_mean.recall,2))\n",
    "print(\"fMeasure:\", round(eval_mean.fmeasure,2))\n",
    "print(\"Accuracy:\", round(eval_mean.accuracy,2))\n",
    "    \n",
    "# Evaluate chords\n",
    "'''In music theory, the concept of root denotes the idea that a chord can be represented and named by one of its notes. It is linked to harmonic thinking, that is, to the idea that vertical aggregates of notes can form a single unit, a chord. It is in this sense that one can speak of a \"C chord\", or a \"chord on C\", a chord built from \"C\" and of which the note (or pitch) \"C\" is the root. When a C chord is referred to in Classical music or popular music without a reference to what type of chord it is (either Major or minor, in most cases), this chord is assumed to be a C major triad, which contains the notes C, E and G. The root needs not be the bass note, the lowest note of the chord: the concept of root is linked to that of the inversion of chords, which is derived from the notion of invertible counterpoint. In this concept, chords can be inverted while still retaining their root.\n",
    "\n",
    "'''\n",
    "\n",
    "print(detections[0:3])\n",
    "print(detections.shape)\n",
    "\n",
    "print(annotations[0:3])\n",
    "print(annotations.shape)\n",
    "\n",
    "detections = np.asarray(detections, dtype=np.float)\n",
    "annotations = np.asarray(annotations[:,0:2], dtype=np.float)\n",
    "    \n",
    "mad.evaluation.find_closest_matches(detections, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p91\n",
    "\n",
    "# Load labelled chord segments from a file. One chord label per line with: \n",
    "# <start_time> <end_time> <chord_label>. All times should be given in seconds.\n",
    "madmom.features.chords.load_chords(filename)\n",
    "\n",
    "# Write chord segments to a file.\n",
    "madmom.features.chords.write_chords(chords, filename)\n",
    "\n",
    "'''use below when you \"roll-your-own nn. it goes from frame-based class ests to cord defs'''\n",
    "# Converts nn-generated major/minor chord class preds to human-readable chord labels. \n",
    "# class preds are assumed to be spaced equidistant in time as defined by the fps parameter i.e., one class pred per frame\n",
    "# classes used by their nn assign 0-11 for major chords starting with root ‘A’, and for 12-23 minor chords.\n",
    "# class 24 represents ‘N’, the no-chord class.\n",
    "madmom.features.chords.majmin_targets_to_chord_labels(targets, fps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# folds used in article: url = 'http://www.eecs.qmul.ac.uk/~sss31/TASLP/'\n",
    "\n",
    "fnames = ['train1.txt','test1.txt','train2.txt','test2.txt','train3.txt','test3.txt','train4.txt','test4.txt']\n",
    "seq = ['train','test','train','test','train','test','train','test',]\n",
    "\n",
    "# for each train and test set across four folds...\n",
    "for i in range(len(fnames)):\n",
    "    \n",
    "    #... retrieve file names\n",
    "    fold = [f.rstrip('\\n') for f in open('data/music/wip/' + fnames[i], 'U')]\n",
    "    \n",
    "    # pass them to formatter\n",
    "    format_xy(\"music\", seq[i], i, fold)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_files_to_folds(fnames):\n",
    "    \n",
    "    num_files = len(fnames); print(\"total files:\",num_files)\n",
    "    \n",
    "    rand.seed(123)\n",
    "    rand.shuffle(fnames)\n",
    "    size = int(num_files / 10)\n",
    "\n",
    "    trn1 = fnames[0:(4*size)]\n",
    "    trn2 = fnames[(4*size):(8*size)]\n",
    "    tst1 = fnames[(8*size):(9*size)]\n",
    "    tst2 = fnames[(9*size):(10*size)]\n",
    "    \n",
    "    seq = ['train', 'train', 'test', 'test']\n",
    "    fls = [trn1, trn2, tst1, tst2]\n",
    "    \n",
    "    return(seq, fls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_xy(soundType, trnTst, foldNum, fold):\n",
    "    \n",
    "    dname = ''.join('/data', sountType, '/fold', str(foldNum))\n",
    "    max_rows = 150000\n",
    "    batch_ctr = 1\n",
    "    new_stack = 1\n",
    "    \n",
    "    # generate x's and y's for each file in fold, append and save in batches\n",
    "    for line in range(fold.shape[0]):\n",
    "        x_note, wav_dur, wav_frames = wav_pitch_to_x(line)\n",
    "        x_chord, wav_dur, wav_frames = wav_chord_to_x_1(line)\n",
    "        y_pitch, y_chord = txt_to_y(line, wav_dur, wav_frames)\n",
    "        \n",
    "        if new_stack == 1 or xStack.shape[0] < max_rows:\n",
    "            if new_stack == 1:\n",
    "                xStack = x\n",
    "                yPitchStack = y_pitch\n",
    "                yChordStack = y_chord\n",
    "                \n",
    "                new_stack = 0\n",
    "            \n",
    "            else: \n",
    "                xStack.append(x)\n",
    "                yPitchStack.append(y_pitch)\n",
    "                yChordStack.append(y_chord)\n",
    "                \n",
    "        else:\n",
    "            # normalize X values\n",
    "            if soundType == \"note\":\n",
    "                xStack = (xStack - note_mean) / note_sd\n",
    "            elif soundType == \"chord\":\n",
    "                xStack = (xStack - chord_mean) / chord_sd\n",
    "            elif soundType == \"music\":\n",
    "                xStack = (xStack - music_mean) / music_sd\n",
    "            \n",
    "            # save off batch\n",
    "            fname = ''.join(dname,'/batch',str(batch_ctr),trnTst,'x.txt')\n",
    "            with open(fname, 'wb') as f:\n",
    "                pickle.dump(xStack, f)\n",
    "            print(\"pickled:\" fname)\n",
    "            \n",
    "            fname = ''.join(dname,'/batch',str(batch_ctr),trnTst,'y_pitch.txt')\n",
    "            with open(fname, 'wb') as f:\n",
    "                pickle.dump(yPitchStack, f)\n",
    "            print(\"pickled:\" fname)\n",
    "            \n",
    "            fname = ''.join(dname,'/batch',str(batch_ctr),trnTst,'y_chord.txt')\n",
    "            with open(fname, 'wb') as f:\n",
    "                pickle.dump(yChordStack, f)\n",
    "            print(\"pickled:\" fname)\n",
    "            \n",
    "            batch_ctr = batch_ctr + 1\n",
    "            new_stack == 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import maximum_filter\n",
    "\n",
    "def wav_pitch_to_x(wav_file):\n",
    "    \n",
    "    # load wav file\n",
    "    sig = mad.audio.signal.Signal(wav_file, sample_rate = NUM_SAMPLES, num_channels = 1, )\n",
    "    wav_dur = sig.shape[0] / sig.sample_rate\n",
    "    \n",
    "    # Use FramedSignal object to split the signal into frames w/ 2048 samples ea w/ 441 sample overlap \n",
    "    fs = mad.audio.signal.FramedSignal(sig, frame_size = NUM_FRAMES, hop_size = NUM_HOPS) # 2048 / 441\n",
    "    \n",
    "    # Using FramedSignal, calculate short term fourrier transform\n",
    "    stft = mad.audio.stft.STFT(fs[0])\n",
    "    \n",
    "    # spec = mad.audio.spectrogram.Spectrogram(stft); print(spec.shape)\n",
    "    spec = mad.audio.spectrogram.Spectrogram(sig, num_channels=1, frame_size = NUM_FRAMES, hop_size = NUM_HOPS)\n",
    "    \n",
    "    # what value can the second channel provide? CNN can handle multiple channels. so, may want a version\n",
    "    # of this which uses 2+ channels and, if necessary, doesn't run through problemantic down stream process. \n",
    "    \n",
    "    # spectral_flx creates linearly spaced magnitude spectogram filtered logarithmically (24 bands / octive)\n",
    "    sf = mad.features.onsets.spectral_flux(spec)\n",
    "    \n",
    "    # filtering\n",
    "    filt_spec = mad.audio.spectrogram.FilteredSpectrogram(spec,\n",
    "                                                          filterbank = mad.audio.filters.LogFilterbank,\n",
    "                                                          num_bands = NUM_BANDS)\n",
    "    \n",
    "    # To better match the perception of loudness of humans, the filtered spectrogram is also scaled logarithmically\n",
    "    log_spec = mad.audio.spectrogram.LogarithmicSpectrogram(filt_spec, add=1)\n",
    "    \n",
    "    # maximum filter size spreads over 3 frequency bins\n",
    "    size = (1, 3)\n",
    "    max_spec = maximum_filter(log_spec, size=size)\n",
    "\n",
    "    # Now we continue to calculate the difference w.r.t. this \"widened\" spectrogram.\n",
    "    diff = np.zeros_like(log_spec)\n",
    "    diff[1:] = (log_spec[1:] - max_spec[: -1])\n",
    "    pos_diff = np.maximum(0, diff)\n",
    "    \n",
    "    # sum everything to get the onset detection function\n",
    "    superflux = np.sum(pos_diff, axis=1)\n",
    "    \n",
    "    # comparison using single function call\n",
    "    #log_filt_spec = mad.audio.spectrogram.LogarithmicFilteredSpectrogram(wav_file,\n",
    "    #                                                                     sample_rate = samples,\n",
    "    #                                                                     num_channels = 1,\n",
    "    #                                                                     num_bands = bands,\n",
    "    #                                                                     frame_size = frames,\n",
    "    #                                                                     hop_size = hops)\n",
    "    \n",
    "    #superflux1 = mad.features.onsets.superflux(log_filt_spec); print(superflux.shape)\n",
    "    \n",
    "    return(superflux, wav_dur, superflux.shape[0])\n",
    "    #return(log_spec, max_spec, superflux, wav_dur, superflux.shape[0])\n",
    "\n",
    "#log_spec1, max_spec1, superflux1, wav_dur1, wav_frames1 = wav_pitch_to_x('data/maps/AkPnCGdD/MUS/MAPS_MUS-chp_op18_AkPnCGdD.wav')\n",
    "#print(\"wave duration:\", wav_dur1)\n",
    "#print(\"wave frames:\", wav_frames1)\n",
    "#plt.figure()\n",
    "#plt.imshow(log_spec1.T, origin='lower', aspect='auto')\n",
    "#plt.figure()\n",
    "#plt.imshow(max_spec1.T, origin='lower', aspect='auto')\n",
    "#plt.figure()\n",
    "#plt.plot(superflux1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''for manual chord recognition development i.e., using neural net. best to start w/ madmon as \n",
    "opposed to essentia below as madmom you can get their alogs online'''\n",
    "\n",
    "from scipy.ndimage.filters import maximum_filter\n",
    "from essentia.standard import *\n",
    "\n",
    "def wav_to_chord_x_3(wav_file):\n",
    "    \n",
    "    # instantiate the audio loader:\n",
    "    loader = essentia.standard.MonoLoader(filename = wav_file)\n",
    "\n",
    "    # and then we actually perform the loading:\n",
    "    audio = loader()\n",
    "\n",
    "    # instantiate windowing \n",
    "    w = Windowing(type = 'blackmanharris74')\n",
    "    spectrum = Spectrum()\n",
    "    mfcc = MFCC()\n",
    "    speaks = SpectralPeaks(sampleRate = NUM_SAMPLES)\n",
    "    hpcp = HPCP(sampleRate = NUM_SAMPLES)\n",
    "\n",
    "    # process frames\n",
    "    mfccs = []; spec_f = []; spec_m = []; chroma = []\n",
    "    for frame in FrameGenerator(audio, frameSize = NUM_FRAMES, hopSize = NUM_HOPS):\n",
    "        # calculate Mel-frequency cepstrum coefficients\n",
    "        mfcc_bands, mfcc_coeffs = mfcc(spectrum(w(frame))) \n",
    "        mfccs.append(mfcc_coeffs)\n",
    "        \n",
    "        # calculate spectral peak frequencies, magnitudes\n",
    "        sp_f, sp_m = speaks(spectrum(w(frame)))\n",
    "        spec_f.append(sp_f); spec_m.append(sp_m)\n",
    "        \n",
    "        chroma.append(hpcp(sp_f, sp_m))\n",
    "    \n",
    "    mfccs = essentia.array(mfccs).T\n",
    "    print(mfccs.shape)\n",
    "    #plt.figure(); plt.plot(mfccs[1,:], aspect = 'auto')\n",
    "    \n",
    "    #spec_f is a list of lists\n",
    "    print(len(spec_f))\n",
    "    \n",
    "    #spec_m is a list of lists\n",
    "    print(len(spec_m))\n",
    "    \n",
    "    chroma = essentia.array(chroma).T\n",
    "    print(chroma.shape)\n",
    "    #plt.figure(); plt.plot(chroma[1:], aspect = 'auto')\n",
    "    \n",
    "    return(mfccs chroma, wav_dur, chorma.shape[1])\n",
    "\n",
    "mfccs, chroma, wav_dur, wav_frames = wav_to_x('data/maps/AkPnCGdD/MUS/MAPS_MUS-chp_op18_AkPnCGdD.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p 173 IO: Input/Output Processor which processes the input data with the input processor and pipes everything\n",
    "# into the given output processor. All Processors defined in the input chain are sequentially called with the \n",
    "# ‘data’ argument only. The output Processor is the only one ever called with two arguments (‘data’, ‘output’).\n",
    "class madmom.processors.IOProcessor(in_processor, out_processor=None)\n",
    "\n",
    "# p174 batch: Process a list of files with the given Processor in batch mode.\n",
    "madmom.processors.process_batch(processor, files, output_dir=None, output_suffix=None,\n",
    "strip_ext=True, num_workers=4, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdowns/.virtualenvs/audio_2.7_env/lib/python2.7/site-packages/ipykernel/__main__.py:9: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    }
   ],
   "source": [
    "def txt_to_y(inFile, mode=\"savedFile\", out_fmt='runTime', wav_dur=0, wav_frames=0):\n",
    "    # valid y_modes: \"pitch_only\", \"chord_only\", \"music\"\n",
    "    \n",
    "    # translates text files w/ onset, offset and pitch into two Y matricies:\n",
    "    #  1. y_notes: [n, 89] vect of binaries representing:\n",
    "    #     a. notes C3 on piano keyboard A1->C#9 (midi 21-109)\n",
    "    #     b. \"not target class\" including notes/pitches outside range AND in-range chords formed by 2+ keys\n",
    "    #  2. y_chords: [n, 120] vect of the \"major\" and \"minor\" triads (some of the most common chords in western music):\n",
    "    #     a. 12 pitch classes i.e., C, C#, D, D#, E, F, F#, G, G#, A, A#, B\n",
    "    #     b. 5 (of total 7.5) octaves i.e., C in 4th octave: C4\n",
    "    #     c. 1 (of total 6 in db) key combinations i.e., triad (not 2, 4, 5, 6 or 7 key combinations)\n",
    "    #     d. major (root pitch, +4 pitches, +3 pitches) or  minor (root pitch, +3, +4) quality \n",
    "    #        i.e., C major triad 4th octave: C4, E4, G4. C minor triad 4th octave: C4, D#4, G4\n",
    "    #     e. NOT IMPLEMENTED: 2 inversions that are defined by the lowest note in the chord\n",
    "    #        i.e., C4, E4, G4 becomes E4, G4, C5 in the first inversion  \n",
    "    #     f. \"not target class\" including single notes and the multitude of less common 2+ note chords\n",
    "    #     other options: http://www.daigleharp.com/Images/Help%20Files/commonchordsforautoharp.pdf, http://www.hooktheory.com/blog/i-analyzed-the-chords-of-1300-popular-songs-for-patterns-this-is-what-i-found/\n",
    "    \n",
    "    if mode == \"savedFile\":\n",
    "        # reads text file into dataframe, sort and round\n",
    "        lines = read_maps_note_file(inFile)\n",
    "    else:\n",
    "        lines = inFile\n",
    "    \n",
    "    chord_rcds_fl = 0; in_chord_fl = 0; pitch_rcds_fl = 0; rval = 0\n",
    "    \n",
    "    # if single line file, save pitch\n",
    "    if lines.shape[0] == 1:\n",
    "        active_pitches = lines.iloc[[0]]\n",
    "        pitch_rcds_fl = 1\n",
    "    \n",
    "    # otherwise, step thru lines assigning notes to pitches or chords\n",
    "    else:\n",
    "        for i in range(1,lines.shape[0]):\n",
    "            \n",
    "            # process pitches: madmom RNN good at extracting notes even from chords. \n",
    "            # so, process all rcds as note records\n",
    "            if pitch_rcds_fl == 0:\n",
    "                # ...then instantiate note array using prior line\n",
    "                active_pitches = lines.iloc[[i-1]]\n",
    "                pitch_rcds_fl = 1\n",
    "                    \n",
    "            # ... and note array already started, then append line to note array\n",
    "            else:\n",
    "                active_pitches = active_pitches.append(lines.iloc[[i-1]])\n",
    "                \n",
    "            # if i is the last line in input array, move it to pitch array also\n",
    "            if i == (lines.shape[0]-1):\n",
    "                if pitch_rcds_fl == 0:\n",
    "                    active_pitches = lines.iloc[[i]]\n",
    "                else:\n",
    "                    active_pitches = active_pitches.append(lines.iloc[[i]])\n",
    "                        \n",
    "            # process chords: if note record has same onset (w/in rounding tolerance of 0.01) as prior...\n",
    "            if lines.iloc[i,0] == lines.iloc[i-1,0]: #and lines.iloc[i,1] == lines.iloc[i-1,1]:\n",
    "                # ... and it's the first chord in piece,...\n",
    "                if chord_rcds_fl == 0:\n",
    "                    # ...then instantiate chord array using the prior pitch (line)\n",
    "                    active_chords = lines.iloc[[i-1]]\n",
    "                    chord_rcds_fl = 1; in_chord_fl = 1\n",
    "                \n",
    "                # otherwise, append the prior pitch (line) to chord array\n",
    "                else:\n",
    "                    active_chords = active_chords.append(lines.iloc[[i-1]])\n",
    "                    in_chord_fl = 1\n",
    "                \n",
    "                # if last line in input array (and it's same as prior), move it to chord array\n",
    "                if i == (lines.shape[0]-1):\n",
    "                    active_chords = active_chords.append(lines.iloc[[i]])\n",
    "                    \n",
    "            # so, current line doesn't have same onset...\n",
    "            else:\n",
    "                #...but you were in a chord... \n",
    "                if in_chord_fl == 1:\n",
    "                    #...append prior pitch (line) to the chord array.\n",
    "                    active_chords = active_chords.append(lines.iloc[[i-1]])\n",
    "                    in_chord_fl = 0\n",
    "                \n",
    "                    # if last line in input array (and you're in a chord), move it to chord array\n",
    "                    if i == (lines.shape[0]-1):\n",
    "                        active_chords = active_chords.append(lines.iloc[[i]])\n",
    "    \n",
    "    if out_fmt == \"runTime\":\n",
    "        if(pitch_rcds_fl == 0):\n",
    "            active_pitches = None\n",
    "            \n",
    "        if(chord_rcds_fl == 0):\n",
    "            active_chords = None\n",
    "        \n",
    "        else:\n",
    "            uniq_onset = pd.DataFrame(active_chords.drop_duplicates(subset=\"OnsetTime\", keep='first'))\n",
    "            uniq_onset = uniq_onset.iloc[:,0:2]\n",
    "            \n",
    "            #print(\"4b. after uniq onset:\", time.time())            \n",
    "            \n",
    "            newLines = []\n",
    "            for i in range(uniq_onset.shape[0]):\n",
    "                newLine = []\n",
    "                cur_pitches = active_chords.MidiPitch[active_chords.OnsetTime == uniq_onset.OnsetTime.iloc[i]]\n",
    "                crd = m21.chord.Chord(np.array(cur_pitches))\n",
    "                root = crd.root()\n",
    "                newLine.append(root.pitchClass)\n",
    "                newLine.append(root.accidental)\n",
    "                newLine.append(root.octave)\n",
    "                newLine.append(crd.containsTriad())\n",
    "                newLine.append(crd.quality)\n",
    "                newLine.append(crd.pitchedCommonName)\n",
    "                \n",
    "                if crd.containsTriad() == True:\n",
    "                    pcStr = NOTE_CLASSES[root.pitchClass]\n",
    "                    \n",
    "                    if crd.quality == \"major\": madChordLabel = pcStr + ':' + \"maj\"\n",
    "                    elif crd.quality == \"minor\": madChordLabel = pcStr + ':' + \"min\"\n",
    "                    else: madChordLabel = 'N'\n",
    "                        \n",
    "                else:\n",
    "                    madChordLabel = 'N'\n",
    "                    \n",
    "                newLine.append(madChordLabel)\n",
    "                newLines.append(newLine)\n",
    "            \n",
    "            newLines = pd.DataFrame(newLines, index=uniq_onset.index,\n",
    "                                    columns=[\"m21RootPitchClass\", \"m21RootPitchAccidental\", \"m21RootOctave\",\n",
    "                                             \"m21ContainsTriad\", \"m21ChordQuality\", \"m21PitchedCommonName\",\n",
    "                                            \"madChordLabel\"])\n",
    "            \n",
    "            uniq_onset = uniq_onset.join(newLines)\n",
    "            #print(\"4c. after\", i, \"iterations:\", time.time())\n",
    "            \n",
    "        return(active_pitches, uniq_onset)\n",
    "    \n",
    "    elif out_fmt == \"oneHot\":\n",
    "        \n",
    "        # format time index w/ slices = wave frame sample rate        \n",
    "        time_ctr = 0\n",
    "        time_incr = float(wav_dur) / wav_frames\n",
    "        time_idx = []\n",
    "    \n",
    "        for k in range(wav_frames): \n",
    "            time_idx.append(np.round(time_ctr,2))\n",
    "            time_ctr = time_ctr + time_incr\n",
    "    \n",
    "        # initialize y matrices    \n",
    "        Y_pitch = pd.DataFrame(np.zeros((len(time_idx), NUM_PITCH_CLASSES), dtype=int),\n",
    "                               index = time_idx, columns = PITCH_CLASSES)\n",
    "        Y_pitch.iloc[:,0] = 1 # set NOT_TGT_CLASS on as default\n",
    "        \n",
    "        Y_chord = pd.DataFrame(np.zeros((len(time_idx), NUM_CHORD_CLASSES), dtype=int),\n",
    "                               index = time_idx, columns = CHORD_CLASSES)\n",
    "        Y_chord.iloc[:,0] = 1 # set NOT_TGT_CLASS on as default\n",
    "    \n",
    "        time_idx = pd.DataFrame(time_idx)\n",
    "\n",
    "        # step thru active, single pitch records\n",
    "        if(pitch_rcds_fl > 0):\n",
    "        \n",
    "            for i in range(active_pitches.shape[0]):\n",
    "               \n",
    "                # ...find the ids of all time indexes that fall after the onset...\n",
    "                more = time_idx[time_idx[0] >= active_pitches.iloc[i,0]].index.tolist()\n",
    "        \n",
    "                #...and the ids of all time indexes that fall before the offset\n",
    "                less = time_idx[time_idx[0] < active_pitches.iloc[i,1]].index.tolist()\n",
    "        \n",
    "                # the intersection are the id's of time indexes where a pitch was active\n",
    "                net = np.intersect1d(more, less, assume_unique=False)\n",
    "        \n",
    "                # flip the class variable for each time index\n",
    "                for j in range(len(net)):\n",
    "                    # if it's a valid pitch...\n",
    "                    if active_pitches.iloc[i,2] in PITCH_CLASSES:\n",
    "                        # ...flip the corresponding pitch column\n",
    "                        Y_pitch.loc[time_idx.iloc[net[j],0], int(active_pitches.iloc[i,2])] = 1\n",
    "                        Y_pitch.loc[time_idx.iloc[net[j],0], NOT_TGT_LABEL] = 0\n",
    "            \n",
    "        # step thru active chord records\n",
    "        if(chord_rcds_fl > 0):\n",
    "            uniq_onset = active_chords.OnsetTime.unique()\n",
    "          \n",
    "            for i in range(uniq_onset.shape[0]):\n",
    "                cur_pitches = active_chords[active_chords.iloc[:,0] == uniq_onset[i]]\n",
    "                cur_chord = m21.chord.Chord(np.array(cur_pitches.iloc[:,2])).pitchedCommonName\n",
    "            \n",
    "                # ...find the ids of all time indexes that fall after the onset...\n",
    "                more = time_idx[time_idx[0] >= uniq_onset[i]].index.tolist()\n",
    "                            \n",
    "                #...and the ids of all time indexes that fall before the offset\n",
    "                less = time_idx[time_idx[0] < cur_pitches.iloc[0,1]].index.tolist()\n",
    "            \n",
    "                # the intersection are the id's of time indexes where a pitch was active\n",
    "                net = np.intersect1d(more, less, assume_unique=False)\n",
    "        \n",
    "                # if it's a valid chord,...\n",
    "                if cur_chord in CHORD_CLASSES:\n",
    "                    # ...cycle thru time indexes...\n",
    "                    for j in range(len(net)):\n",
    "                        # ...flipping the corresponding chord column\n",
    "                        Y_chord.loc[time_idx.iloc[net[j],0], cur_chord] = 1\n",
    "                        Y_chord.loc[time_idx.iloc[net[j],0], NOT_TGT_LABEL] = 0\n",
    "                    \n",
    "                    rval = 1\n",
    "                \n",
    "        #return([rval])\n",
    "        return(Y_pitch, Y_chord)\n",
    "        \n",
    "# test music\n",
    "ptch, crd = txt_to_y('data/maps/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OnsetTime</th>\n",
       "      <th>OffsetTime</th>\n",
       "      <th>m21RootPitchClass</th>\n",
       "      <th>m21RootPitchAccidental</th>\n",
       "      <th>m21RootOctave</th>\n",
       "      <th>m21ContainsTriad</th>\n",
       "      <th>m21ChordQuality</th>\n",
       "      <th>m21PitchedCommonName</th>\n",
       "      <th>madChordLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>110.14</td>\n",
       "      <td>117.07</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>major</td>\n",
       "      <td>C2-major triad</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>123.96</td>\n",
       "      <td>124.20</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>E4-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>124.43</td>\n",
       "      <td>124.67</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>A4-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>124.90</td>\n",
       "      <td>125.15</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>C4-interval class 1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>125.39</td>\n",
       "      <td>125.86</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>A3-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>126.32</td>\n",
       "      <td>127.75</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>G4-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>127.75</td>\n",
       "      <td>127.99</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>D5-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>128.69</td>\n",
       "      <td>129.16</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>D4-unison</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>129.16</td>\n",
       "      <td>129.40</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>G4-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>129.64</td>\n",
       "      <td>130.11</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>F4-whole-tone trichord</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>130.11</td>\n",
       "      <td>130.58</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>minor</td>\n",
       "      <td>A3-minor triad</td>\n",
       "      <td>A:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>130.58</td>\n",
       "      <td>131.05</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>B3-phrygian trichord</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>131.05</td>\n",
       "      <td>131.76</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>C4-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>131.76</td>\n",
       "      <td>131.87</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>D4-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>131.98</td>\n",
       "      <td>132.41</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>G5-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>132.45</td>\n",
       "      <td>132.92</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>minor</td>\n",
       "      <td>E4-minor triad</td>\n",
       "      <td>E:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>132.92</td>\n",
       "      <td>133.37</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>A3-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>133.41</td>\n",
       "      <td>134.11</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>minor</td>\n",
       "      <td>D4-minor triad</td>\n",
       "      <td>D:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>133.88</td>\n",
       "      <td>135.05</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>D5-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>134.11</td>\n",
       "      <td>134.35</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>C5-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>134.35</td>\n",
       "      <td>134.58</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>B4-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>134.58</td>\n",
       "      <td>134.82</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>A4-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>134.82</td>\n",
       "      <td>135.24</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>G4-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>135.28</td>\n",
       "      <td>135.77</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>C3-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>135.77</td>\n",
       "      <td>136.24</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>D3-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>136.24</td>\n",
       "      <td>136.71</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>E3-tritone-fourth</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>136.71</td>\n",
       "      <td>137.41</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>major</td>\n",
       "      <td>F3-major triad</td>\n",
       "      <td>F:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>137.17</td>\n",
       "      <td>137.63</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>minor</td>\n",
       "      <td>D4-minor triad</td>\n",
       "      <td>D:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>137.41</td>\n",
       "      <td>137.52</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>G3-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>137.63</td>\n",
       "      <td>138.06</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>major</td>\n",
       "      <td>C5-major triad</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>206.21</td>\n",
       "      <td>206.74</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>A4-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>206.74</td>\n",
       "      <td>207.27</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>B4-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>207.27</td>\n",
       "      <td>207.79</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>diminished</td>\n",
       "      <td>E3-diminished triad</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>207.79</td>\n",
       "      <td>208.57</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>F3-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>208.32</td>\n",
       "      <td>208.85</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>D5-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>208.85</td>\n",
       "      <td>209.33</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>E3-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>209.38</td>\n",
       "      <td>209.90</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>A3-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>209.90</td>\n",
       "      <td>210.38</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;accidental flat&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>B-4-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>210.43</td>\n",
       "      <td>211.22</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>E5-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>210.96</td>\n",
       "      <td>211.44</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>F5-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>211.48</td>\n",
       "      <td>211.75</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>G3-interval class 5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>212.01</td>\n",
       "      <td>212.21</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>E3-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>212.28</td>\n",
       "      <td>212.55</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>D3-interval class 5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>212.55</td>\n",
       "      <td>212.82</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>major</td>\n",
       "      <td>C5-major triad</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>212.82</td>\n",
       "      <td>213.09</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>F3-unison</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>213.09</td>\n",
       "      <td>213.36</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>E5-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>213.36</td>\n",
       "      <td>213.64</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>minor</td>\n",
       "      <td>D5-minor triad</td>\n",
       "      <td>D:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>213.64</td>\n",
       "      <td>213.84</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>C5-incomplete dominant-seventh chord</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>213.91</td>\n",
       "      <td>214.18</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>minor</td>\n",
       "      <td>G3-minor triad</td>\n",
       "      <td>G:min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>214.18</td>\n",
       "      <td>214.39</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>A3-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>214.46</td>\n",
       "      <td>214.73</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>E3-tritone</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>214.73</td>\n",
       "      <td>215.00</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>F3-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>215.00</td>\n",
       "      <td>215.28</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>G3-unison</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>215.28</td>\n",
       "      <td>215.56</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>F4-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>215.56</td>\n",
       "      <td>215.84</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>major</td>\n",
       "      <td>G4-interval class 4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>215.84</td>\n",
       "      <td>216.06</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>F4-interval class 5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>216.13</td>\n",
       "      <td>216.41</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>A3-interval class 5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>216.41</td>\n",
       "      <td>218.69</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>B3-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>218.69</td>\n",
       "      <td>226.06</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>minor</td>\n",
       "      <td>A4-interval class 3</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>221.53</td>\n",
       "      <td>226.06</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;accidental natural&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>major</td>\n",
       "      <td>C6-major triad</td>\n",
       "      <td>C:maj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OnsetTime  OffsetTime  m21RootPitchClass m21RootPitchAccidental  \\\n",
       "547      110.14      117.07                  0   <accidental natural>   \n",
       "565      123.96      124.20                  4   <accidental natural>   \n",
       "568      124.43      124.67                  9   <accidental natural>   \n",
       "571      124.90      125.15                  0   <accidental natural>   \n",
       "574      125.39      125.86                  9   <accidental natural>   \n",
       "579      126.32      127.75                  7   <accidental natural>   \n",
       "583      127.75      127.99                  2   <accidental natural>   \n",
       "588      128.69      129.16                  2   <accidental natural>   \n",
       "591      129.16      129.40                  7   <accidental natural>   \n",
       "595      129.64      130.11                  5   <accidental natural>   \n",
       "599      130.11      130.58                  9   <accidental natural>   \n",
       "603      130.58      131.05                 11   <accidental natural>   \n",
       "606      131.05      131.76                  0   <accidental natural>   \n",
       "610      131.76      131.87                  2   <accidental natural>   \n",
       "613      131.98      132.41                  7   <accidental natural>   \n",
       "616      132.45      132.92                  4   <accidental natural>   \n",
       "618      132.92      133.37                  9   <accidental natural>   \n",
       "621      133.41      134.11                  2   <accidental natural>   \n",
       "624      133.88      135.05                  2   <accidental natural>   \n",
       "626      134.11      134.35                  0   <accidental natural>   \n",
       "628      134.35      134.58                 11   <accidental natural>   \n",
       "630      134.58      134.82                  9   <accidental natural>   \n",
       "632      134.82      135.24                  7   <accidental natural>   \n",
       "636      135.28      135.77                  0   <accidental natural>   \n",
       "640      135.77      136.24                  2   <accidental natural>   \n",
       "644      136.24      136.71                  4   <accidental natural>   \n",
       "649      136.71      137.41                  5   <accidental natural>   \n",
       "653      137.17      137.63                  2   <accidental natural>   \n",
       "655      137.41      137.52                  7   <accidental natural>   \n",
       "659      137.63      138.06                  0   <accidental natural>   \n",
       "...         ...         ...                ...                    ...   \n",
       "1191     206.21      206.74                  9   <accidental natural>   \n",
       "1193     206.74      207.27                 11   <accidental natural>   \n",
       "1196     207.27      207.79                  4   <accidental natural>   \n",
       "1198     207.79      208.57                  5   <accidental natural>   \n",
       "1200     208.32      208.85                  2   <accidental natural>   \n",
       "1204     208.85      209.33                  4   <accidental natural>   \n",
       "1207     209.38      209.90                  9   <accidental natural>   \n",
       "1210     209.90      210.38                 10      <accidental flat>   \n",
       "1212     210.43      211.22                  4   <accidental natural>   \n",
       "1216     210.96      211.44                  5   <accidental natural>   \n",
       "1219     211.48      211.75                  7   <accidental natural>   \n",
       "1222     212.01      212.21                  4   <accidental natural>   \n",
       "1224     212.28      212.55                  2   <accidental natural>   \n",
       "1227     212.55      212.82                  0   <accidental natural>   \n",
       "1229     212.82      213.09                  5   <accidental natural>   \n",
       "1231     213.09      213.36                  4   <accidental natural>   \n",
       "1234     213.36      213.64                  2   <accidental natural>   \n",
       "1237     213.64      213.84                  0   <accidental natural>   \n",
       "1240     213.91      214.18                  7   <accidental natural>   \n",
       "1243     214.18      214.39                  9   <accidental natural>   \n",
       "1245     214.46      214.73                  4   <accidental natural>   \n",
       "1247     214.73      215.00                  5   <accidental natural>   \n",
       "1249     215.00      215.28                  7   <accidental natural>   \n",
       "1251     215.28      215.56                  5   <accidental natural>   \n",
       "1253     215.56      215.84                  7   <accidental natural>   \n",
       "1255     215.84      216.06                  5   <accidental natural>   \n",
       "1257     216.13      216.41                  9   <accidental natural>   \n",
       "1259     216.41      218.69                 11   <accidental natural>   \n",
       "1269     218.69      226.06                  9   <accidental natural>   \n",
       "1281     221.53      226.06                  0   <accidental natural>   \n",
       "\n",
       "      m21RootOctave m21ContainsTriad m21ChordQuality  \\\n",
       "547               2             True           major   \n",
       "565               4            False           minor   \n",
       "568               4            False           minor   \n",
       "571               4            False           other   \n",
       "574               3            False           minor   \n",
       "579               4            False           major   \n",
       "583               5            False           major   \n",
       "588               4            False           other   \n",
       "591               4            False           major   \n",
       "595               4            False           major   \n",
       "599               3             True           minor   \n",
       "603               3            False           minor   \n",
       "606               4            False           major   \n",
       "610               4            False           major   \n",
       "613               5            False           major   \n",
       "616               4             True           minor   \n",
       "618               3            False           minor   \n",
       "621               4             True           minor   \n",
       "624               5            False           minor   \n",
       "626               5            False           major   \n",
       "628               4            False           minor   \n",
       "630               4            False           minor   \n",
       "632               4            False           major   \n",
       "636               3            False           major   \n",
       "640               3            False           minor   \n",
       "644               3            False           other   \n",
       "649               3             True           major   \n",
       "653               4             True           minor   \n",
       "655               3            False           major   \n",
       "659               5             True           major   \n",
       "...             ...              ...             ...   \n",
       "1191              4            False           minor   \n",
       "1193              4            False           minor   \n",
       "1196              3             True      diminished   \n",
       "1198              3            False           major   \n",
       "1200              5            False           minor   \n",
       "1204              3            False           minor   \n",
       "1207              3            False           minor   \n",
       "1210              4            False           major   \n",
       "1212              5            False           minor   \n",
       "1216              5            False           major   \n",
       "1219              3            False           other   \n",
       "1222              3            False           minor   \n",
       "1224              3            False           other   \n",
       "1227              5             True           major   \n",
       "1229              3            False           other   \n",
       "1231              5            False           minor   \n",
       "1234              5             True           minor   \n",
       "1237              5            False           major   \n",
       "1240              3             True           minor   \n",
       "1243              3            False           minor   \n",
       "1245              3            False           other   \n",
       "1247              3            False           major   \n",
       "1249              3            False           other   \n",
       "1251              4            False           major   \n",
       "1253              4            False           major   \n",
       "1255              4            False           other   \n",
       "1257              3            False           other   \n",
       "1259              3            False           minor   \n",
       "1269              4            False           minor   \n",
       "1281              6             True           major   \n",
       "\n",
       "                      m21PitchedCommonName madChordLabel  \n",
       "547                         C2-major triad         C:maj  \n",
       "565                    E4-interval class 3             N  \n",
       "568                    A4-interval class 3             N  \n",
       "571                    C4-interval class 1             N  \n",
       "574                    A3-interval class 3             N  \n",
       "579                    G4-interval class 4             N  \n",
       "583                    D5-interval class 4             N  \n",
       "588                              D4-unison             N  \n",
       "591                    G4-interval class 4             N  \n",
       "595                 F4-whole-tone trichord             N  \n",
       "599                         A3-minor triad         A:min  \n",
       "603                   B3-phrygian trichord             N  \n",
       "606                    C4-interval class 4             N  \n",
       "610                    D4-interval class 4             N  \n",
       "613                    G5-interval class 4             N  \n",
       "616                         E4-minor triad         E:min  \n",
       "618                    A3-interval class 3             N  \n",
       "621                         D4-minor triad         D:min  \n",
       "624                    D5-interval class 3             N  \n",
       "626                    C5-interval class 4             N  \n",
       "628                    B4-interval class 3             N  \n",
       "630                    A4-interval class 3             N  \n",
       "632                    G4-interval class 4             N  \n",
       "636                    C3-interval class 4             N  \n",
       "640                    D3-interval class 3             N  \n",
       "644                      E3-tritone-fourth             N  \n",
       "649                         F3-major triad         F:maj  \n",
       "653                         D4-minor triad         D:min  \n",
       "655                    G3-interval class 4             N  \n",
       "659                         C5-major triad         C:maj  \n",
       "...                                    ...           ...  \n",
       "1191                   A4-interval class 3             N  \n",
       "1193                   B4-interval class 3             N  \n",
       "1196                   E3-diminished triad             N  \n",
       "1198                   F3-interval class 4             N  \n",
       "1200                   D5-interval class 3             N  \n",
       "1204                   E3-interval class 3             N  \n",
       "1207                   A3-interval class 3             N  \n",
       "1210                  B-4-interval class 4             N  \n",
       "1212                   E5-interval class 3             N  \n",
       "1216                   F5-interval class 4             N  \n",
       "1219                   G3-interval class 5             N  \n",
       "1222                   E3-interval class 3             N  \n",
       "1224                   D3-interval class 5             N  \n",
       "1227                        C5-major triad         C:maj  \n",
       "1229                             F3-unison             N  \n",
       "1231                   E5-interval class 3             N  \n",
       "1234                        D5-minor triad         D:min  \n",
       "1237  C5-incomplete dominant-seventh chord             N  \n",
       "1240                        G3-minor triad         G:min  \n",
       "1243                   A3-interval class 3             N  \n",
       "1245                            E3-tritone             N  \n",
       "1247                   F3-interval class 4             N  \n",
       "1249                             G3-unison             N  \n",
       "1251                   F4-interval class 4             N  \n",
       "1253                   G4-interval class 4             N  \n",
       "1255                   F4-interval class 5             N  \n",
       "1257                   A3-interval class 5             N  \n",
       "1259                   B3-interval class 3             N  \n",
       "1269                   A4-interval class 3             N  \n",
       "1281                        C6-major triad         C:maj  \n",
       "\n",
       "[214 rows x 9 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_maps_note_file(txt_file):\n",
    "    # reads text file into dataframe, sort and round\n",
    "    \n",
    "    lines = [line.rstrip('\\n').split('\\t') for line in open(txt_file, 'U')]\n",
    "    \n",
    "    headers = lines[0]; lines = lines[1:len(lines)]\n",
    "    \n",
    "    lines = pd.DataFrame(lines, columns=[headers[0], headers[1], \n",
    "                                         headers[2]]).convert_objects(convert_numeric=True)\n",
    "    \n",
    "    lines = lines.round({'OnsetTime': 2, 'OffsetTime': 2, 'MidiPitch': 0})\n",
    "    \n",
    "    lines = lines.sort_values(['OnsetTime', 'MidiPitch'], # 'OffsetTime', \n",
    "                              axis=0, ascending=True, inplace=False, \n",
    "                              kind='quicksort', na_position='last')\n",
    "    \n",
    "    '''sort is tricky. some chord notes have same onset, different offsets. technically, the shorter note\n",
    "    should probably appear first (i.e., ascending sort: onset, offset, pitch). problem is you get an \n",
    "    expanding set of chord classes most of which don't sound musically different (i.e., imperceptible \n",
    "    differences in offset). so, i'm sorting above using ascending: onset, midi (after pitch, offset does not matter)''' \n",
    "    \n",
    "    \n",
    "    return(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m21.chord.Chord([50,55,59]).pitchedCommonName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_list(toSave, toDir, toName, toType):\n",
    "    fullName = ''.join([toDir, toName, toType])\n",
    "    with open(fullName, 'w') as f:\n",
    "        for item in toSave:\n",
    "            f.write(item + '\\n')\n",
    "\n",
    "def pickle_it(toPick, toDir, toName):\n",
    "    with open(toDir + toName, 'wb') as f:\n",
    "        pickle.dump(toPick, f)\n",
    "        print(\"pickled:\", toName)\n",
    "\n",
    "def unpickle_it(frmDir, frmName):\n",
    "    with open(frmDir + frmName, 'rb') as f:\n",
    "        return(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put in \"format\" or \"transform\" function\n",
    "\n",
    "# txt_to_y format: [OnsetTime, OffsetTime, Midipitch]\n",
    "\n",
    "def flip_formats(inFile, frmFrmt, toFrmt):\n",
    "    \n",
    "    # valid to/froms: madNote, mapNote, madChord, m21Chord\n",
    "    \n",
    "    inHt, inWd = inFile.shape\n",
    "    \n",
    "    # data types\n",
    "    MAD_NOTE_HEADER_1 = ['note_time', 'MIDI_note']\n",
    "    MAD_NOTE_HEADER_2 = ['note_time', 'MIDI_note', 'duration']\n",
    "    MAD_NOTE_HEADER_3 = ['note_time', 'MIDI_note', 'duration', 'MIDI_note']\n",
    "        \n",
    "    MAD_NOTE_DTYPE_1 = [('note_time', np.float), ('MIDI_note', np.int)]\n",
    "    MAD_NOTE_DTYPE_2 = [('note_time', np.float), ('MIDI_note', np.int), ('duration', np.float)]\n",
    "    MAD_NOTE_DTYPE_3 = [('note_time', np.float), ('MIDI_note', np.int), ('duration', np.float), ('MIDI_note', np.int)]\n",
    "    \n",
    "    MAD_CHORD_DTYPE = [('start', np.float), ('end', np.float), ('label', 'U32')]\n",
    "\n",
    "    MAP_NOTE_HEADER = ['OnsetTime', 'OffsetTime', 'MidiPitch']\n",
    "    MAP_CHORD_HEADER = ['OnsetTime', 'OffsetTime', 'ChordLabel']\n",
    "    \n",
    "    #M21_NOTE_DTYPE = 'f4,f4,int'\n",
    "    #M21_CHORD_DTYPE = 'f4,f4,S10'\n",
    "    \n",
    "    if frmFrmt == \"madNote\":\n",
    "        \n",
    "        if toFrmt == \"mapNote\" or toFrmt == \"m21Chord\" or toFrmt == \"madChord\":\n",
    "            \n",
    "            notes = np.zeros(shape=(inHt, 3), dtype=float) # dtype defaults to float64\n",
    "            notes[:,0] = inFile[:,0]\n",
    "            notes[:,2] = inFile[:,1].astype(int)\n",
    "            \n",
    "            # if no duration\n",
    "            if inFile.shape[1] == 2:\n",
    "                notes[:,1] = 0\n",
    "                \n",
    "            # if duration\n",
    "            else:\n",
    "                notes[:,1] = inFile[:,0] + inFile[:,2]\n",
    "                \n",
    "            if toFrmt == \"mapNote\":\n",
    "                return(notes)\n",
    "        \n",
    "            elif toFrmt == \"m21Chord\" or toFrmt == \"madChord\":\n",
    "                \n",
    "                notes = pd.DataFrame(notes, columns=['OnsetTime', 'OffsetTime', 'MidiPitch'])\n",
    "                notes = tmp.round({'OnsetTime': 2, 'OffsetTime': 2, 'MidiPitch': 0})\n",
    "                notes = tmp.sort_values(['OnsetTime', 'MidiPitch'],\n",
    "                                      axis=0, ascending=True, inplace=False,\n",
    "                                      kind='quicksort', na_position='last')\n",
    "                \n",
    "                notes[\"MidiPitch\"] = tmp['MidiPitch'].astype(int)\n",
    "                \n",
    "                notes, chords = txt_to_y(tmp, mode=\"thisFile\")\n",
    "                \n",
    "                if toFrmt == \"m21Chord\":\n",
    "                    return(chords)\n",
    "            \n",
    "                elif toFrmt == \"madChord\":\n",
    "                    # NOTE: UNTIL YOU FIND OUT HOW THEY'RE CLASSING THEIR CHORDS, DON'T SPEND TIME HERE.\n",
    "                    lines = [line.rstrip('\\n').split('\\t') for line in open('data/mad2m21map.txt', 'U')]\n",
    "                    headers = lines[0]; lines = lines[1:len(lines)]\n",
    "                    lines = pd.DataFrame(lines, columns= headers)\n",
    "                    \n",
    "                    d = dict(zip(lines.m21_chord_pitchedCommonName, lines.mad_chord))\n",
    "                    \n",
    "                    for i in range(chords.shape[0]):\n",
    "                        try:\n",
    "                            chords.ChordLabel.iloc[i] = d[chords.ChordLabel.iloc[i]]\n",
    "                        except:\n",
    "                            chords.ChordLabel.iloc[i] = 'N'\n",
    "                            \n",
    "                    return(chords)\n",
    "        # NOTE: DON'T SPEND TIME GOING FROM MAD.CHORD LABELS TO M21. JUST USE MODEL OUTPUTS TO PREDICT. \n",
    "                \n",
    "                \n",
    "\n",
    "#print(flip_formats(rnn_note_detect, \"madNote\", \"mapNote\"))\n",
    "print(flip_formats(rnn_note_detect, \"madNote\", \"m21Chord\"))\n",
    "print(flip_formats(rnn_note_detect, \"madNote\", \"madChord\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. process batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp = 'data/maps/AkPnCGdD/MUS/MAPS_MUS-chp_op18_AkPnCGdD.mid'\n",
    "mf = m21.midi.MidiFile()\n",
    "mf.open(fp)\n",
    "mf.read()\n",
    "mf.close()\n",
    "len(mf.tracks)\n",
    "\n",
    "s = m21.midi.translate.midiFileToStream(mf)\n",
    "s\n",
    "len(s.flat.notesAndRests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Notes X (for CNN)\n",
    "\n",
    "An End-to-End Neural Network for Polyphonic Piano Music Transcription:\n",
    "\n",
    "    Acoustic model inputs: \n",
    "We transform the input audio to a time-frequency rep- resentation which is then input to the acoustic models. In [20], we used the magnitude short-time Fourier transform (STFT) as input to the acoustic models. However, here we experiment with the constant Q transform (CQT) as the input representation. There are two motivations for this. Firstly, the CQT is fundamentally better suited as a time-frequency representation for music signals, since the frequency axis is linear in pitch [46]. Another advantage of using the CQT is that the resulting representation is much lower dimensional than the STFT. Having a lower dimensional representation is useful when using neural network acoustic models as it reduces the number of parameters in the model.\n",
    "\n",
    "We downsample the audio to 16 kHz from 44.1 kHz. We then compute CQTs over 7 octaves with 36 bins per octave and a hop size of 512 samples, resulting in a 252 dimensional input vector of real values, with a frame rate of 31.25 frames per second. Additionally, we compute the mean and standard deviation of each dimension over the training set and transform the data by subtracting the mean and diving by the standard deviation. \n",
    "\n",
    "    Language model:\n",
    "For the language model training, we sample the MIDI ground truth transcriptions of the training data at the same rate as the audio (32 ms). We obtain sequences of 88 dimensional binary vectors for training the RNN-NADE language models. The 88 outputs correspond to notes A0-C8 on a piano.\n",
    "\n",
    "The test audio is sampled at a frame rate of 100 Hz yielding 100 ⇤ 30 = 3000 frames per test file. For 54 test files over 4 splits, we obtain a total of 648,000 frames at test time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sf)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mfccs[0,:])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(chroma[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Chords X (for CNN)\n",
    "\n",
    "\"Audio chord recognition with a hybrid neural network\"\n",
    "\n",
    "We use DNNs to learn discriminative features from a time- frequency representation of the audio. This is contrary to the common approach of extracting chroma features (and their many variants) as a preprocessing step. \n",
    "\n",
    "We transform the raw audio signal into a time-frequency representation with the constant-Q transform [6]. We first down-sample the audio to 11.025 kHz and compute the CQT with a hop-size of 1024 sam- ples. The CQT is computed over 7 octaves with 24 bins per octave yielding a 168 dimensional vector of real values. One of the advantages of using the CQT is that the rep- resentation is low dimensional and linear in pitch. Com- puting the short-time Fourier transform over long analysis windows would lead to a much higher dimensional rep- resentation. \n",
    "\n",
    "After extracting CQT frames for each track, we use a DNN to classify each frame to its corresponding chord label... We used DNNs with 3 hidden layers. We constrained all the layers to have the same number of hidden units to simplify the task of searching for good DNN architectures. The DNNs have a softmax output layer and the model parameters are obtained using maximum likelihood estimation.\n",
    "\n",
    "Once the DNNs are trained, we use the activations of the final hidden layer of the DNN as features. In our experi- ments we observed that the acoustic model performance was improved (⇠ 3% absolute improvement in frame-level accuracy) if we provided each frame of features with context information. Context information was provided by performing mean and variance pooling over a context window around the central frame of interest [3]. A context window of length 2k + 1 is comprised of the central frame of interest, along with k frames before and after the central frame. In our experiments we found that a context window of 7 frames provided the best results.\n",
    "\n",
    "We trained the network with mini-batch stochastic gradient descent. Instead of using learning rate update schedules, we use ADADELTA which adapts the learning rate over iterations [18]. In our experiments we found Dropout was essential to improve generalisation [16]. We found a Dropout rate of 0.3 applied to all layers of the DNN to be optimal for controlling overfitting. Once the models are trained, we use the model that performs best on the vali- dation set to extract features. In our experiments, the best performing model had 100 hidden units in each layer. Figure 1 is a graphical representation of the feature extraction pipeline. In section 6, we compare DNN acoustic models with different feature inputs.\n",
    "\n",
    "We used a mini-batch size of 100 and early stopping for training. Training was stopped if the log-likelihood of the validation set did not increase for 20 iterations over the entire training set. Unlike the feature extraction stage, we do not discard any of the trained models. Instead of using only the best performing model on the validation set, we average the predictions of all the trained models to form an ensemble of DNNs [8] as the acoustic model. We found that simply averaging the predictions of the acoustic classifiers led to an absolute improvement of up to 3% on frame classification accuracies.\n",
    "\n",
    "The results show that the performance of the acoustic model is greatly improved when the input features to the model are learnt by a DNN as opposed to CQT inputs. The performance of the acoustic model is further improved (3% absolute improvement) when mean and variance pooling is performed over a context window of DNN features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# opening wave files\n",
    "import wave\n",
    "\n",
    "fl_lst = ['data/maps/AkPnCGdD/ISOL/NO/MAPS_ISOL_NO_F_S0_M21_AkPnCGdD',\n",
    "          'data/maps/AkPnCGdD/ISOL/NO/MAPS_ISOL_NO_F_S0_M23_AkPnCGdD',\n",
    "          'data/maps/AkPnCGdD/ISOL/ST/MAPS_ISOL_ST_F_S0_M22_AkPnCGdD']\n",
    "\n",
    "mid_strm = m21.stream.Stream()\n",
    "wav_strm = []\n",
    "\n",
    "for i in fl_lst:\n",
    "    \n",
    "    mid_strm.append(m21.converter.parse(i + '.mid'))\n",
    "    # cut out dead time? if you do, you have to do in both midi and wav\n",
    "    \n",
    "    # load wav file into a list\n",
    "    wav_strm.append(mad.audio.signal.Signal(i + \".wav\", sample_rate=None, num_channels=None, \n",
    "                                            start=None, stop=None, norm=False, gain=0.0, dtype=None))\n",
    "    #wav_strm.append(m21.converter.parse(i + '.wav'))\n",
    "\n",
    "mid_strm.show('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Language Model: X (for RNN/LSTM)\n",
    "\n",
    "Music, like language, exhibits temporal structure. Language model used to provide prior probability distribution for notes and chords. \n",
    "\n",
    "HMM: \"A Supervised Approach To Musical Chord Recognition\" is good example of using HMM on chord sequence in western music that greatly increases chord prediction accuracy. \n",
    "\n",
    "RNN: \"An End-to-End Neural Network for Polyphonic Piano Music Transcription\"\n",
    "\n",
    "Two approaches:\n",
    "\n",
    "First, as outlined in \"An End-to-End Neural Network for Polyphonic Piano Music Transcription\" (where they sampled the midi file for ground truth. Wow, seems that could be an easy way of fiding what freq's are turned off, on) and \"Audio chord recognition with a hybrid neural network\", you can use an RNN (LSTM) to create lanaguge model building note/chord prediction based on priors using all prior history (i.e., unlike HMM which only considers current state). \n",
    "\n",
    "However, you can't just do this in a gready fashion. That is, the correct prediction at time t is not known. Therefore, you can't just predict t+1 based on t (the HMM approach) or even t, t-1, t-2,... t-n (the RNN/LSTM) approach. So, you have to have a way of entertaining a number of possible options, settling on one or the other as evidence of its correct-ness accumulates. So, this requires a method of entertaining, sequentially, a range of solutions winnowing as you go, that's beam search. \n",
    "\n",
    "Beam search 1. generates a range of t+1 predictions sorted by probability estimate, 2. re-estimates the predictions as new X's arrive adding some and dropping others. Beam search has its own problems, some of which are addressed by hashing the beams, but that's beyond the scope. \n",
    "\n",
    "Second, if you assume knowledge of the song (either b/c user entered, or based on note search, prediction becomes a binary classification task i.e., does the current sound match the \"predicted\" sound. And, if not, provide the estimated sound. This model starts w/ knowledge of the song being played. Later, I'll add beam search soley for the purpose of identifying the song. \n",
    "\n",
    "*** \n",
    "\n",
    "As outlined in Section 3, we use RNNs with LSTM units for language modelling. The training data for the language models is obtained by sampling the ground truth chord transcriptions at the same frame-rate at which CQTs are extracted from the audio waveforms. We use RNNs with 2 layers of hidden recurrent units (100 LSTM units each) and an output softmax layer. Each training sequence was further divided into sub-sequences of length 100. \n",
    "\n",
    "The RNNs were trained with stochastic gradient descent on individual sub-sequences, without any mini-batching. Unlike the acoustic models, we observed that ADADELTA did not perform very well for RNN training. Instead, we used an initial learning rate of 0.001 that was linearly decreased to 0 over 1000 training iterations. We also found that a constant momentum rate of 0.9 helped training converge faster and yielded better results on the test set. We used early stopping and training was stopped if validation log- likelihood did not increase after 20 epochs. We used gra- dient clipping when the norm of the gradients was greater than 50 to avoid gradient explosion in the early stages of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. midi-note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt = m21.midi.MidiTrack(1)\n",
    "dt1 = m21.midi.DeltaTime(mt)\n",
    "dt1.time = 1024\n",
    "\n",
    "me1 = m21.midi.MidiEvent(mt)\n",
    "me1.type = \"NOTE_ON\"\n",
    "me1.pitch = 45\n",
    "me1.velocity = 94\n",
    "\n",
    "dt2 = m21.midi.DeltaTime(mt)\n",
    "dt2.time = 2048\n",
    "\n",
    "me2 = m21.midi.MidiEvent(mt)\n",
    "me2.type = \"NOTE_ON\"\n",
    "me2.pitch = 45\n",
    "me2.velocity = 0\n",
    "\n",
    "n = m21.midi.translate.midiEventsToNote([dt1, me1, dt2, me2])\n",
    "n.pitch\n",
    "#n.duration.quarterLength\n",
    "#n.volume.velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. midi-chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def midi_chord(in_vals, out_fmt):\n",
    "    # takes up to 4 midi values (as tuple) occurring concurrently and returns chord or vice versa\n",
    "    \n",
    "    if out_fmt == \"chord\":\n",
    "        \n",
    "        mt = m21.midi.MidiTrack(1)\n",
    "        track = []\n",
    "        \n",
    "        me1 = m21.midi.MidiEvent(mt)\n",
    "        me2 = m21.midi.MidiEvent(mt)\n",
    "        me3 = m21.midi.MidiEvent(mt)\n",
    "        me4 = m21.midi.MidiEvent(mt)\n",
    "        me5 = m21.midi.MidiEvent(mt)\n",
    "        me6 = m21.midi.MidiEvent(mt)\n",
    "        me7 = m21.midi.MidiEvent(mt)\n",
    "        me8 = m21.midi.MidiEvent(mt)\n",
    "        \n",
    "        events = [me1, me2, me3, me4, me5, me6, me7, me8]\n",
    "        \n",
    "        dt0 = m21.midi.DeltaTime(mt)\n",
    "        dt0.time = 0\n",
    "        \n",
    "        for i in range(len(in_vals)):\n",
    "            \n",
    "            track.append(dt0)\n",
    "            \n",
    "            events[i].type = \"NOTE_ON\"\n",
    "            events[i].pitch = in_vals[i]\n",
    "            events[i].velocity = 94\n",
    "            \n",
    "            track.append(events[i])\n",
    "        \n",
    "        dt1 = m21.midi.DeltaTime(mt)\n",
    "        dt1.time = 2048\n",
    "        track.append(dt1)\n",
    "        \n",
    "        incr = len(in_vals)\n",
    "        \n",
    "        for j in range(len(in_vals)):\n",
    "\n",
    "            events[j+incr].type = \"NOTE_OFF\"\n",
    "            events[j+incr].pitch = in_vals[j]\n",
    "            events[j+incr].velocity = 0\n",
    "            \n",
    "            track.append(events[j+incr])\n",
    "            \n",
    "            track.append(dt0)\n",
    "        \n",
    "        return(m21.midi.translate.midiEventsToChord(track))\n",
    "    \n",
    "    elif out_fmt == \"midi\":\n",
    "        print(\"poop\")\n",
    "\n",
    "c = midi_chord((45,46,47), \"chord\")\n",
    "print(c.pitches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compose midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import music21 as m21\n",
    "\n",
    "mt = m21.midi.MidiTrack(1)\n",
    "dt1 = m21.midi.DeltaTime(mt)\n",
    "dt1.time = 0\n",
    "\n",
    "me1 = m21.midi.MidiEvent(mt)\n",
    "me1.type = \"NOTE_ON\"\n",
    "me1.pitch = 45\n",
    "me1.velocity = 94\n",
    "me1.t = 0\n",
    "\n",
    "dt2 = m21.midi.DeltaTime(mt)\n",
    "dt2.time = 0\n",
    "\n",
    "me2 = m21.midi.MidiEvent(mt)\n",
    "me2.type = \"NOTE_ON\"\n",
    "me2.pitch = 46\n",
    "me2.velocity = 94\n",
    "me2.t = 0\n",
    "\n",
    "dt3 = m21.midi.DeltaTime(mt)\n",
    "dt3.time = 2048\n",
    "\n",
    "me3 = m21.midi.MidiEvent(mt)\n",
    "me3.type = \"NOTE_OFF\"\n",
    "me3.pitch = 45\n",
    "me3.velocity = 0\n",
    "me3.t = 2048\n",
    "\n",
    "dt4 = m21.midi.DeltaTime(mt)\n",
    "dt4.time = 0\n",
    "\n",
    "me4 = m21.midi.MidiEvent(mt)\n",
    "me4.type = \"NOTE_OFF\"\n",
    "me4.pitch = 46\n",
    "me4.velocity = 0\n",
    "me4.t = 2048\n",
    "\n",
    "m21.midi.translate.midiEventsToChord([dt1, me1, dt1, me2, dt3, me3, dt1, me4])\n",
    "\n",
    "print([dt1, me1, dt1, me2, dt3, me3, dt1, me4, dt1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do you spot notes vs. echos vs. chords?\n",
    "if you feed frames at 44100 hz, does it learn to recognize c# by its peak spectrum profile? or can it spot that as it develops frame by frame?\n",
    "\n",
    "is it more efficient to run an onset / offset identifier (network or other) first to pull out the sounds and feed them to the network? \n",
    "\n",
    "how do you encode y? they've put down the onset and offset times. \n",
    "1. if you encode y=1 for all frames during that period, you're hoping it can spot the tone on the way up and down. that suggests a multi-frame CNN or a LSTM. theoretically, it could try to predict for every frame. but on way down lots of notes look the same. so, confidence s/b low and you'd not predict (default 0). \n",
    "\n",
    "2. if you think it's just going to find it based on relative peaks, take start time + 100ms\n",
    "\n",
    "3. at 47kHz you're getting 800 frames / second. that's a lot. maybe sample less to get rougher image. train acustic model less and use song model to bring up accuracy i.e., taking output from last hidden nodes for acustic and song models \n",
    "\n",
    "READ HER ORIGINAL PIECE, THE OTHER ONE THAT USED HERS AND/OR THE ONE THAT MENTIONED \n",
    "\n",
    "1. if you have onset identifier,it needs to be an array of 0's as long as x. then at the time stamp, it needs to be incoded w/ the midi pitch? then you need to translate to the note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ds in datasets:\n",
    "    \n",
    "    # LogarithmicFilteredSpectrogram class accepts STFT, FramedSignal, Signal object or a file name as input.\n",
    "    # It loads, filters and scales.\n",
    "\n",
    "    log_filt_spec2 = mad.audio.spectrogram.LogarithmicFilteredSpectrogram('data/maps/MAPS_AkPnBcht_1/AkPnBcht/ISOL/CH/MAPS_ISOL_CH0.1_F_AkPnBcht.wav',\n",
    "                                                                      num_channels=1,\n",
    "                                                                      num_bands=24)\n",
    "    \n",
    "    log_filt_spec = mad.audio.spectrogram.LogarithmicFilteredSpectrogram(ds,\n",
    "                                                                         num_channels=1,\n",
    "                                                                         num_bands=24)\n",
    "\n",
    "    # SpectrogramDifference class calculates first order difference of a spectrogram. diff_frames sets the number\n",
    "    # of frames how far apart the difference should be calculated. diff_max_bins performs maximum filtering. Or, \n",
    "    # positive_diffs returns only the positive differences. Resulting diff spectrogram has the same shape as the \n",
    "    # given one (np.diff() does not), with all frames < diff_frames set to zero.\n",
    "\n",
    "    superflux_diff = mad.audio.spectrogram.SpectrogramDifference(log_filt_spec, positive_diffs=True, diff_max_bins=3)\n",
    "\n",
    "    superflux_2 = np.sum(superflux_diff, axis=1)\n",
    "\n",
    "    superflux_3 = mad.features.onsets.superflux(log_filt_spec)\n",
    "\n",
    "    # we scale them to have the same range\n",
    "    plt.figure()\n",
    "    plt.plot(sf1 / sf1.max(), 'b')  # blue\n",
    "    plt.plot(superflux1 / superflux1.max(), 'g')  # green\n",
    "    plt.plot(superflux_2_1 / superflux_2_1.max(), 'r--')  # dashed red\n",
    "    plt.plot(superflux_3_1 / superflux_3_1.max(), 'k:')  # dotted black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read\n",
    "\n",
    "1. Files from directory, OR\n",
    "2. Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1: Import piano train / test data\n",
    "\n",
    "1. Take their lists of train and test data\n",
    "2. Create list object\n",
    "3. Feed it to a process to either iteratively or bulk load files from directory\n",
    "4. Perform log scale transform of input wav\n",
    "5. Come back to other features, chords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to map sound to notes\n",
    "\n",
    "1. Take professional music sound files\n",
    "2. Play, logging spectrum (frequency / time) and other attributes\n",
    "    a. https://github.com/tyiannak/pyAudioAnalysis/wiki/3.-Feature-Extraction, OR\n",
    "    b. \n",
    "3. Predict notes based on sound\n",
    "    a. input spectrum is the \"X\"\n",
    "    b. sheet music notes are the \"y\" (notes A, B, C, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1: Generate harp note / chord train / test files\n",
    "\n",
    "Garageband or other to generate:\n",
    "1. individual instrument note (pitch?), chord by major/minor, octave, inversion\n",
    "2. sequence files w/ varying amounts of spacing\n",
    "3. mp3's of classical music for which you can easily veryify the notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Standard: Fingerprint music files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n. Fingerprint music\n",
    "\n",
    "https://github.com/dpwe/audfprint\n",
    "\n",
    "Audfprint is a python (and Matlab) script that can take a list of soundfiles and create a database of landmarks, and then subsequently take one or more query audio files and match them against the previously-created database.  The fingerprint is robust to things like time skews, different encoding schemes, and even added noise. It can match small fragments of sound, down to 10 sec or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use FFT, MFCC, etc to create a fignerprint of each of the music files s/t when user starts playing, you can take notes they've played and match "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Streaming: Extract note Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Streaming: Search / match sample to fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish stream\n",
    "\n",
    "Use pyaudio to instantiate stream for practice session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 2\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    return (in_data, pyaudio.paContinue)\n",
    "\n",
    "stream = p.open(format=p.get_format_from_width(WIDTH),\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                output=True,\n",
    "                stream_callback=callback)\n",
    "\n",
    "stream.start_stream()\n",
    "\n",
    "while stream.is_active():\n",
    "    time.sleep(0.1)\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display mode\n",
    "\n",
    "1. Accept song selection\n",
    "2. Load / display sheet music\n",
    "3. Listen for start\n",
    "4. Recieve sounds / translate notes\n",
    "5. Track progress w/ vertical bar\n",
    "6. Spot repeats re-setting tracking bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/tyiannak/pyAudioAnalysis\n",
    "# http://essentia.upf.edu/documentation/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate mode\n",
    "\n",
    "1. display mode functionality including tracking progress\n",
    "2. record playing. overlay repeats. you're tracking stats. so, obj s/b:\n",
    "    a. to get through song re-playing pieces as required, THEN\n",
    "    b. to get through song cleanly\n",
    "3. comparing played to professional\n",
    "    a. option to play:\n",
    "        i. metrinome \n",
    "        ii. professional a low volume\n",
    "4. identify discrpancies (timing after prior note, incorrect note)\n",
    "    a. Gaia, a C++ library with python bindings which implement similarity measures and classification on the results of audio analysis, and generate classification models that Essentia can use to compute high-level description of music.\n",
    "5. show discrepancies\n",
    "    a. accept tolerances (+/- time, other?)\n",
    "    b. show played note in red (i.e., before/after, above/below).\n",
    "6. show / log statistics\n",
    "    a. accuracy\n",
    "    b. similarity\n",
    "    c. error types and frequency distribution\n",
    "        i. early,\n",
    "        ii. late\n",
    "        iii. wrong note\n",
    "    d. problem areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive mode\n",
    "\n",
    "1. evaluate mode functionality\n",
    "2. prompt session info and imprint voice of user for command interface\n",
    "    a. \"this is []. the date is []. i'll be practicing for about [] minutes.\"\n",
    "3. voice commands\n",
    "    a. \"replay [] notes\" - defaults to: 5 notes, played version\n",
    "    b. \"replay base [] notes\"\n",
    "    c. \"loop [] notes\" - \n",
    "        ii. Loop [] notes / Stop Loop\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "This project would not be possible without the invaluable assistance of:\n",
    "\n",
    "## Training data\n",
    "\n",
    "The MAPS piano data set. Roughly 40G of piano notes, chords, music assembled by V. Emiya for her PhD thesis at Telecom ParisTech/ENST in 2008 and in conjunction with R. Badeau, B. David for their paper \"Multipitch estimation of piano sounds using a new probabilistic spectral smoothness principle\"<cite data-cite=\"emiya2010multipitch\"></cite>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for installing latex, bibtex and pdf-ing jupyter notenooks: https://www.youtube.com/watch?v=m3o1KXA1Rjk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
