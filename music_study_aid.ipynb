{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Configure Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML/CSS:\n",
    "http://www.w3schools.com/default.asp\n",
    "\n",
    "Java Script\n",
    "1. The basics: https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/JavaScript_basics\n",
    "\n",
    "2. Grunt, the JavaScript Task Runner. Configure a Gruntfile to perform mundane, repetitive tasks: http://gruntjs.com/getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PYTHON\n",
    "pip install image # install's PIL, the python image library\n",
    "# install portaudio is required by pyaudio (http://portaudio.com/docs/v19-doxydocs/tutorial_start.html)\n",
    "pip install pyaudio # for recording within python. used for audioSearch module. install was a bit funky, but this fixed it: http://stackoverflow.com/questions/33513522/when-installing-pyaudio-pip-cannot-find-portaudio-h-in-usr-local-include\n",
    "pip install pygame # for realtime MIDI performance in midi.realtime module\n",
    "#brew install pygame\n",
    "\n",
    "# Javascript\n",
    "brew install node\n",
    "sudo npm install -g grunt-cli # puts grunt command line interface in system path. The CLI runs the version of Grunt which has been installed next to a Gruntfile, enabling multiple versions of Grunt to be installed on the same machine. If a locally installed Grunt is found, the CLI loads the local installation of the Grunt library, applies the configuration from your Gruntfile, and executes any tasks you've requested for it to run.\n",
    "# https://github.com/cuthbertLab/music21j: install music21j, the java environment for music21.\n",
    "\n",
    "# visualization:\n",
    "# sonic visualizer, for viewing and analyzing contents of music audio files (http://www.sonicvisualiser.org/download.html)\n",
    "# install musescore for viewing and editing music notation (http://www.musescore.org, https://github.com/musescore/MuseScore)\n",
    "# install lilypond for displaying musical scores (http://lilypond.org/)\n",
    "\n",
    "# audio processing: \n",
    "# jack audio for osx. I downloaded latest for Mac from here: https://github.com/jackaudio/jack2/issues/73\n",
    "pip install cffi # C Foreign Function Interface for Python, used to access the C-API of the JACK library from within Python\n",
    "pip install JACK-Client # jack client https://jackclient-python.readthedocs.io/en/0.4.1/\n",
    "pip install SpeechRecognition # https://pypi.python.org/pypi/SpeechRecognition/. Git repository has prerequisites including pyaudio version: https://github.com/Uberi/speech_recognition\n",
    "#pip install pysoundfile \n",
    "pip install python-Levenshtein # Python extension for computing string edit distances and similarities.\n",
    "pip install flask # http://flask.pocoo.org/ flask, like django, is a python web framework.\n",
    "pip install flask-socketio\n",
    "pip install eventlet\n",
    "pip install gevent\n",
    "pip install gunicorn\n",
    "pip install nginx\n",
    "pip install redis # redis-2.10.5\n",
    "\n",
    "# JAVA SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## B. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# directory, files management, etc.\n",
    "import os as os\n",
    "#from os.path import isfile, join\n",
    "import glob\n",
    "from StringIO import StringIO\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# arrays, dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# utilities\n",
    "import random as rand\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# audio\n",
    "import sys\n",
    "import wave\n",
    "import pyaudio as pa\n",
    "import jack as jk\n",
    "import soundfile as sf\n",
    "\n",
    "# MIR\n",
    "import essentia as es\n",
    "import madmom as mad\n",
    "import music21 as m21\n",
    "#import yaafelib as yf\n",
    "\n",
    "# neural nets\n",
    "\n",
    "# visualization\n",
    "import bokeh as bk\n",
    "\n",
    "import sys\n",
    "import flask\n",
    "import gevent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Envirnonment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sys.executable)\n",
    "print(os.environ)\n",
    "print(! which python)\n",
    "print(! printenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''music21 environment configuration'''\n",
    "\n",
    "us = m21.environment.UserSettings()\n",
    "\n",
    "m21.environment.set('autoDownload', 'allow')\n",
    "\n",
    "for k in us.keys():\n",
    "    try:\n",
    "        print(\"us['{0}'] = {1}\".format(k, us[k]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pitches, notes\n",
    "PITCH_CLASSES = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "NUM_PITCH_CLASS = len(PITCH_CLASSES)\n",
    "NUM_MIDI_PITCH = 127               # range of audible sounds\n",
    "RANGE_PIANO_PITCH = range(21,109)  # 88 notes (12 notes * 7.25 octave scale), midi 21:108\n",
    "NUM_PIANO_PITCH = len(RANGE_PIANO_PITCH)\n",
    "\n",
    "# chords\n",
    "NUM_COM_CHORD_OCTAVE = 5\n",
    "#NUM_ALL_CHORD_PITCH = NUM_PIANO_PITCH # midi 21:108 inclusive. 7.25 octave piano scale\n",
    "#RANGE_COM_CORD_PITCH = range(36,96) # midi 36:95 inclusive, 5 octave piano scale\n",
    "#NUM_COM_CHORD_PITCH = len(RANGE_COM_CHORD_PITCH)\n",
    "NUM_MAJOR_MINOR = 2\n",
    "CHORD_KEY_SETS = 1 # only 3-key chords \n",
    "\n",
    "# loudness\n",
    "RANGE_MEZZO_FORTE = range(60, 69)\n",
    "RANGE_PIANO_FORTE = range(32, 97)\n",
    "\n",
    "# classes\n",
    "NUM_NOT_TGT_CLASS = 1\n",
    "NOT_TGT_LABEL = [\"not target\"]\n",
    "NUM_PITCH_CLASSES = NUM_PIANO_PITCH + NUM_NOT_TGT_CLASS\n",
    "NUM_CHORD_CLASSES = (NUM_PITCH_CLASS * NUM_COM_CHORD_OCTAVE * NUM_MAJOR_MINOR * CHORD_KEY_SETS) + NUM_NOT_TGT_CLASS\n",
    "\n",
    "#PITCH_CLASSES = RANGE_PIANO_PITCH\n",
    "#PITCH_CLASSES = NOT_TGT_LABEL + PITCH_CLASSES\n",
    "\n",
    "CHORD_CLASSES = []\n",
    "for k in range(3,8):\n",
    "    for i in PITCH_CLASSES:\n",
    "        for j in [\"major\", \"minor\"]:\n",
    "            CHORD_CLASSES.append(str(i+str(k)+\"-\"+j+\" triad\"))\n",
    "CHORD_CLASSES = NOT_TGT_LABEL + CHORD_CLASSES\n",
    "\n",
    "# to standardize feature extract algos\n",
    "NUM_SAMPLES = 22050 # 44100\n",
    "NUM_FRAMES = 1024 # 2048\n",
    "NUM_HOPS = 512 # 441\n",
    "NUM_BANDS = 24 # 48\n",
    "\n",
    "# other \n",
    "dat_dir = 'data/maps/'\n",
    "WINDOW = 0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# II. Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Accoustic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Format X & Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Process .wav and .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''grabs performance (.wav) and ground truth (.txt). runs performance thru neural nets. \n",
    "evaluates performance.'''\n",
    "\n",
    "def process_wavs(inWavDir, outTxtDir):\n",
    "    '''\n",
    "    reads wav, writes note, cnn chord and dnn chord files to target dir\n",
    "    if tgtWavDir provided, appends chord, note and wav files incrementing. saves wav to target dir. \n",
    "    '''\n",
    "    \n",
    "    inWavs = glob.glob(inWavDir + '/*/*.wav')\n",
    "    wavs = []; begTm = 0; noteCtr = 0; chordCtr = 0\n",
    "    \n",
    "    for i in range(len(inWavs)):\n",
    "        if i % 5 == 0: print(\"processing input file #\", i)\n",
    "        \n",
    "        # process X: chord and note recognition from wave\n",
    "        dnm = os.path.dirname(inWavs[i])\n",
    "        fnm, _ = os.path.splitext(os.path.basename(inWavs[i]))\n",
    "        inNm = dnm + '/' + fnm + '.txt'\n",
    "        outNm = outTxtDir + '/' + fnm\n",
    "        \n",
    "        #dnn_chord_rec(inWavs[i], savTo = outNm + '.chords.dnn.txt' )\n",
    "        #cnn_chord_rec(inWavs[i], savTo = outNm + '.chords.cnn.txt'  )\n",
    "        rnn_note_and_chord_rec(\"single\", inWavs[i], outNm) # outNm + '.notes.txt'\n",
    "        \n",
    "        # process Y: corresponding y values from text files\n",
    "        N, C = txt_to_y(inNm)\n",
    "        \n",
    "        if N is not None:\n",
    "            N['Duration'] = N.OffsetTime - N.OnsetTime\n",
    "            N = N[['OnsetTime', 'MidiPitch', 'Duration' ]]\n",
    "            mad.features.notes.write_notes(np.array(N),\n",
    "                                           outNm + '.note.y.txt',\n",
    "                                           fmt=['%.3f', '%d', '%.3f'])\n",
    "            \n",
    "        if C is not None:\n",
    "            save_chords(outNm + '.chords.y.txt', C)\n",
    "\n",
    "#process_wavs(inWavDir = 'data/wip/tmp/',\n",
    "#             outTxtDir = 'data/wip/tmp')\n",
    "    \n",
    "process_wavs(inWavDir = 'data/maps/AkPnBcht/UCHO/I60-68/',\n",
    "             outTxtDir = 'data/wip/AkPnBcht/UCHO/I60-68/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Format Y Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def txt_to_y(inFile, mode=\"savedFile\", out_fmt='runTime', wav_dur=0, wav_frames=0):\n",
    "    '''TODO: implement chord recognition window s/t notes w/in window are considered components of chord.\n",
    "    only do this if you see material chord misclassifications in normal application use.'''\n",
    "    # valid y_modes: \"pitch_only\", \"chord_only\", \"music\"\n",
    "    \n",
    "    '''translates text files w/ onset, offset and pitch into two Y matricies:\n",
    "    #  1. y_notes: [n, 89] vect of binaries representing:\n",
    "    #     a. notes C3 on piano keyboard A1->C#9 (midi 21-109)\n",
    "    #     b. \"not target class\" including notes/pitches outside range AND in-range chords formed by 2+ keys\n",
    "    #  2. y_chords: [n, 120] vect of the \"major\" and \"minor\" triads (some of the most common chords in western music):\n",
    "    #     a. 12 pitch classes i.e., C, C#, D, D#, E, F, F#, G, G#, A, A#, B\n",
    "    #     b. 5 (of total 7.5) octaves i.e., C in 4th octave: C4\n",
    "    #     c. 1 (of total 6 in db) key combinations i.e., triad (not 2, 4, 5, 6 or 7 key combinations)\n",
    "    #     d. major (root pitch, +4 pitches, +3 pitches) or  minor (root pitch, +3, +4) quality \n",
    "    #        i.e., C major triad 4th octave: C4, E4, G4. C minor triad 4th octave: C4, D#4, G4\n",
    "    #     e. NOT IMPLEMENTED: 2 inversions that are defined by the lowest note in the chord\n",
    "    #        i.e., C4, E4, G4 becomes E4, G4, C5 in the first inversion  \n",
    "    #     f. \"not target class\" including single notes and the multitude of less common 2+ note chords\n",
    "    #     other options: http://www.daigleharp.com/Images/Help%20Files/commonchordsforautoharp.pdf, \n",
    "    http://www.hooktheory.com/blog/i-analyzed-the-chords-of-1300-popular-songs-for-patterns-this-is-what-i-found/'''\n",
    "    \n",
    "    if mode == \"savedFile\":\n",
    "        # reads text file into dataframe, sort and round\n",
    "        lines = read_maps_note_file(inFile)\n",
    "    else:\n",
    "        lines = inFile\n",
    "    \n",
    "    chord_rcds_fl = 0; in_chord_fl = 0; pitch_rcds_fl = 0; rval = 0\n",
    "    \n",
    "    # if single line file, save pitch\n",
    "    if lines.shape[0] == 1:\n",
    "        active_pitches = lines.iloc[[0]]\n",
    "        pitch_rcds_fl = 1\n",
    "    \n",
    "    # otherwise, step thru lines assigning notes to pitches or chords\n",
    "    else:\n",
    "        for i in range(1,lines.shape[0]):\n",
    "            \n",
    "            # process pitches: madmom RNN good at extracting notes even from chords. \n",
    "            # so, process all rcds as note records\n",
    "            if pitch_rcds_fl == 0:\n",
    "                # ...then instantiate note array using prior line\n",
    "                active_pitches = lines.iloc[[i-1]]\n",
    "                pitch_rcds_fl = 1\n",
    "                    \n",
    "            # ... and note array already started, then append line to note array\n",
    "            else:\n",
    "                active_pitches = active_pitches.append(lines.iloc[[i-1]])\n",
    "                \n",
    "            # if i is the last line in input array, move it to pitch array also\n",
    "            if i == (lines.shape[0]-1):\n",
    "                if pitch_rcds_fl == 0:\n",
    "                    active_pitches = lines.iloc[[i]]\n",
    "                else:\n",
    "                    active_pitches = active_pitches.append(lines.iloc[[i]])\n",
    "                        \n",
    "            # process chords: if note record has same onset (w/in rounding tolerance of 0.01) as prior...\n",
    "            if lines.iloc[i,0] == lines.iloc[i-1,0]: #and lines.iloc[i,1] == lines.iloc[i-1,1]:\n",
    "                # ... and it's the first chord in piece,...\n",
    "                if chord_rcds_fl == 0:\n",
    "                    # ...then instantiate chord array using the prior pitch (line)\n",
    "                    active_chords = lines.iloc[[i-1]]\n",
    "                    chord_rcds_fl = 1; in_chord_fl = 1\n",
    "                \n",
    "                # otherwise, append the prior pitch (line) to chord array\n",
    "                else:\n",
    "                    active_chords = active_chords.append(lines.iloc[[i-1]])\n",
    "                    in_chord_fl = 1\n",
    "                \n",
    "                # if last line in input array (and it's same as prior), move it to chord array\n",
    "                if i == (lines.shape[0]-1):\n",
    "                    active_chords = active_chords.append(lines.iloc[[i]])\n",
    "                    \n",
    "            # so, current line doesn't have same onset...\n",
    "            else:\n",
    "                #...but you were in a chord... \n",
    "                if in_chord_fl == 1:\n",
    "                    #...append prior pitch (line) to the chord array.\n",
    "                    active_chords = active_chords.append(lines.iloc[[i-1]])\n",
    "                    in_chord_fl = 0\n",
    "                \n",
    "                    # if last line in input array (and you're in a chord), move it to chord array\n",
    "                    if i == (lines.shape[0]-1):\n",
    "                        active_chords = active_chords.append(lines.iloc[[i]])\n",
    "    \n",
    "    if out_fmt == \"runTime\":\n",
    "        if(pitch_rcds_fl == 0):\n",
    "            active_pitches = None\n",
    "            \n",
    "        if(chord_rcds_fl == 0):\n",
    "            active_chords = None\n",
    "        \n",
    "        else:\n",
    "            chordsDF = to_chords(active_chords, inFrmt = \"m21noteDF\")\n",
    "            #print(\"4c. after\", i, \"iterations:\", time.time())\n",
    "        \n",
    "        return(active_pitches, chordsDF)\n",
    "    \n",
    "    elif out_fmt == \"oneHot\":\n",
    "        \n",
    "        # format time index w/ slices = wave frame sample rate        \n",
    "        time_ctr = 0\n",
    "        time_incr = float(wav_dur) / wav_frames\n",
    "        time_idx = []\n",
    "    \n",
    "        for k in range(wav_frames): \n",
    "            time_idx.append(np.round(time_ctr,2))\n",
    "            time_ctr = time_ctr + time_incr\n",
    "    \n",
    "        # initialize y matrices    \n",
    "        Y_pitch = pd.DataFrame(np.zeros((len(time_idx), NUM_PITCH_CLASSES), dtype=int),\n",
    "                               index = time_idx, columns = PITCH_CLASSES)\n",
    "        Y_pitch.iloc[:,0] = 1 # set NOT_TGT_CLASS on as default\n",
    "        \n",
    "        Y_chord = pd.DataFrame(np.zeros((len(time_idx), NUM_CHORD_CLASSES), dtype=int),\n",
    "                               index = time_idx, columns = CHORD_CLASSES)\n",
    "        Y_chord.iloc[:,0] = 1 # set NOT_TGT_CLASS on as default\n",
    "    \n",
    "        time_idx = pd.DataFrame(time_idx)\n",
    "\n",
    "        # step thru active, single pitch records\n",
    "        if(pitch_rcds_fl > 0):\n",
    "        \n",
    "            for i in range(active_pitches.shape[0]):\n",
    "               \n",
    "                # ...find the ids of all time indexes that fall after the onset...\n",
    "                more = time_idx[time_idx[0] >= active_pitches.iloc[i,0]].index.tolist()\n",
    "        \n",
    "                #...and the ids of all time indexes that fall before the offset\n",
    "                less = time_idx[time_idx[0] < active_pitches.iloc[i,1]].index.tolist()\n",
    "        \n",
    "                # the intersection are the id's of time indexes where a pitch was active\n",
    "                net = np.intersect1d(more, less, assume_unique=False)\n",
    "        \n",
    "                # flip the class variable for each time index\n",
    "                for j in range(len(net)):\n",
    "                    # if it's a valid pitch...\n",
    "                    if active_pitches.iloc[i,2] in PITCH_CLASSES:\n",
    "                        # ...flip the corresponding pitch column\n",
    "                        Y_pitch.loc[time_idx.iloc[net[j],0], int(active_pitches.iloc[i,2])] = 1\n",
    "                        Y_pitch.loc[time_idx.iloc[net[j],0], NOT_TGT_LABEL] = 0\n",
    "            \n",
    "        # step thru active chord records\n",
    "        if(chord_rcds_fl > 0):\n",
    "            uniq_onset = active_chords.OnsetTime.unique()\n",
    "          \n",
    "            for i in range(uniq_onset.shape[0]):\n",
    "                cur_pitches = active_chords[active_chords.iloc[:,0] == uniq_onset[i]]\n",
    "                cur_chord = m21.chord.Chord(np.array(cur_pitches.iloc[:,2])).pitchedCommonName\n",
    "            \n",
    "                # ...find the ids of all time indexes that fall after the onset...\n",
    "                more = time_idx[time_idx[0] >= uniq_onset[i]].index.tolist()\n",
    "                            \n",
    "                #...and the ids of all time indexes that fall before the offset\n",
    "                less = time_idx[time_idx[0] < cur_pitches.iloc[0,1]].index.tolist()\n",
    "            \n",
    "                # the intersection are the id's of time indexes where a pitch was active\n",
    "                net = np.intersect1d(more, less, assume_unique=False)\n",
    "        \n",
    "                # if it's a valid chord,...\n",
    "                if cur_chord in CHORD_CLASSES:\n",
    "                    # ...cycle thru time indexes...\n",
    "                    for j in range(len(net)):\n",
    "                        # ...flipping the corresponding chord column\n",
    "                        Y_chord.loc[time_idx.iloc[net[j],0], cur_chord] = 1\n",
    "                        Y_chord.loc[time_idx.iloc[net[j],0], NOT_TGT_LABEL] = 0\n",
    "                    \n",
    "                    rval = 1\n",
    "                \n",
    "        #return([rval])\n",
    "        return(Y_pitch, Y_chord)\n",
    "        \n",
    "# test music\n",
    "#ptch, crd = txt_to_y('data/maps/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if type([]) == list:\n",
    "    print 1\n",
    "else:\n",
    "    print 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Clean Chord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_chords(inChords, inFrmt):\n",
    "    \n",
    "    if inFrmt == 'madChordDF':\n",
    "        outChords = inChords.iloc[:,0:2]\n",
    "        notClass = [float('nan'), None, float('nan'), None, None, None, None]\n",
    "        dels = []\n",
    "        pitchClasses = range(NUM_PITCH_CLASSES) # Integer vales 0-11, where C=0, C#=1, D=2...B=11 per M21\n",
    "        newLines = []\n",
    "        \n",
    "        for i in range(inChords.shape[0]):\n",
    "            newLine = []\n",
    "            \n",
    "            # pitch class\n",
    "            if inChords.iloc[i,2][0] == 'N':\n",
    "                newLines.append(notClass)\n",
    "                dels.append(i)\n",
    "                continue\n",
    "                '''for lines identified as 'N', add placeholder which will be subsequently deleted. these are \n",
    "                chords for which the onset is known, but the classification is not known (trained). by deleting\n",
    "                we re losing info. however, evaluation chokes on records w/ nans or nones. further, given how \n",
    "                effective note rnn is at spotting chord onset AND classifying chords, chord CNN/DNN will be \n",
    "                used to augment its predictions'''\n",
    "            elif inChords.iloc[i,2][1] == ':': \n",
    "                newLine.append(pitchClasses[PITCH_CLASSES.index(inChords.iloc[i,2][0])])\n",
    "                # accidental\n",
    "                newLine.append('accidental natural')\n",
    "            elif inChords.iloc[i,2][1] == '#':\n",
    "                '''flats \"-\" madmon trained their network w/ 25 classes representing:\n",
    "                1. 12 pitch classes: 'A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#'\n",
    "                2. 2 qualities 'maj', 'min'\n",
    "                3. N for none of the above\n",
    "                \n",
    "                Problem is that, while they occur often in music, flats aren't included. Looking at akpnbcht,\n",
    "                see that E- and B- chords are classed as D# and A# by the algorithm. Specifically:\n",
    "                1. of 13.5k chords, 1.9k (14%) were flats. 1,112 E-, 820 B-. \n",
    "                2. of 1,112 E-, 208 (100% of those classed by the algo) were classed as D#. \n",
    "                3. of 820 B-, 128 (100% of those classed by the algo) were classed as A#. \n",
    "                4. of mad chords classed D, 376 were classed by m21 as D, 208 as E.\n",
    "                5. of mad chords classed A, 354 were classed by m21 as A, 128 as B.\n",
    "                \n",
    "                so, assuming akpnbcht is a representative dataset:\n",
    "                1. mad classifies flats as next pitch class down\n",
    "                2. this requires changes to:\n",
    "                    a. the mad chord outputs to bring them in inline w/ m21-based classifications (below)\n",
    "                    b. the m21-based y records\n",
    "                '''\n",
    "                if inChords.iloc[i,2][0] == 'D':\n",
    "                    newLine.append(pitchClasses[PITCH_CLASSES.index('E')])\n",
    "                    newLine.append('accidental flat')\n",
    "                elif inChords.iloc[i,2][0] == 'A':\n",
    "                    newLine.append(pitchClasses[PITCH_CLASSES.index('B')])\n",
    "                    newLine.append('accidental flat')\n",
    "                else: \n",
    "                    newLine.append(pitchClasses[PITCH_CLASSES.index(inChords.iloc[i,2][0:1])])\n",
    "                    newLine.append('accidental sharp')\n",
    "            elif inChords.iloc[i,2][1] == '-':\n",
    "                newLine.append(pitchClasses[PITCH_CLASSES.index(inChords.iloc[i,2][0])+1])\n",
    "                newLine.append('accidental flat')\n",
    "            else:\n",
    "                print('unrecognized accidental on chord input file at line', i); end\n",
    "            \n",
    "            # octave\n",
    "            newLine.append(-10)\n",
    "            \n",
    "            # triad\n",
    "            newLine.append(True)\n",
    "            \n",
    "            # quality\n",
    "            s = inChords.iloc[i,2] \n",
    "            try: \n",
    "                start = s.find(':') + 1\n",
    "                end = len(s)\n",
    "                found = s[start:end]\n",
    "            except:\n",
    "                print(\"chord record without maj/min quality:\", i)\n",
    "                end\n",
    "            if found == 'maj': newLine.append(\"major\")\n",
    "            elif found == 'min': newLine.append(\"minor\")\n",
    "            else: print(\"chord record with non maj/min quality:\", i); end\n",
    "            \n",
    "            # m21 pitched common name\n",
    "            newLine.append(None)\n",
    "            \n",
    "            # mad chord label\n",
    "            newLine.append(inChords.iloc[i,2])\n",
    "            newLines.append(newLine)\n",
    "    \n",
    "    elif inFrmt == \"m21noteDF\":\n",
    "        # going from stream of notes to m21 chords\n",
    "        outChords = pd.DataFrame(inChords.drop_duplicates(subset=\"OnsetTime\", keep='first'))\n",
    "        outChords = outChords.iloc[:,0:2]\n",
    "        dels = []; newLines = []\n",
    "        \n",
    "        for i in range(outChords.shape[0]):\n",
    "            newLine = []\n",
    "            curPitches = inChords.MidiPitch[inChords.OnsetTime == outChords.OnsetTime.iloc[i]]\n",
    "            crd = m21.chord.Chord(np.array(curPitches))\n",
    "            root = crd.root()\n",
    "            newLine.append(root.pitchClass)\n",
    "                \n",
    "            s = str(root.accidental)\n",
    "            \n",
    "            try: \n",
    "                start = s.find('<') + 1\n",
    "                end = s.find('>', start)\n",
    "                found = s[start:end]\n",
    "            except:\n",
    "                found = root.accidental\n",
    "                print(\"accidental w/ out < or >:\", i)\n",
    "                print(root.accidental)\n",
    "    \n",
    "            newLine.append(found)\n",
    "            newLine.append(root.octave)\n",
    "            newLine.append(crd.containsTriad())\n",
    "            newLine.append(crd.quality)\n",
    "            newLine.append(crd.pitchedCommonName)\n",
    "                \n",
    "            if crd.containsTriad() == True:\n",
    "                '''based on the assumption that the basic unit of chord construction is the major/minor triad, \n",
    "                I check for triad here, then determine the appropriate (as near as I can tell) madmom root. This\n",
    "                reverses the process used to map from madmom chords to m21 chords above. \n",
    "                '''\n",
    "                # if root is flat,...\n",
    "                if root.accidental == 'accidental flat':\n",
    "                    # ... move down one note\n",
    "                    if root.pitchClass >= 1:\n",
    "                        pcStr = PITCH_CLASSES[root.pitchClass-1]\n",
    "                    else:\n",
    "                        pcStr = PITCH_CLASSES[-1]\n",
    "                else:\n",
    "                    pcStr = PITCH_CLASSES[root.pitchClass]\n",
    "                    \n",
    "                if crd.quality == \"major\": madChordLabel = pcStr + ':maj'\n",
    "                elif crd.quality == \"minor\": madChordLabel = pcStr + ':min'\n",
    "                else: madChordLabel = 'N'\n",
    "                        \n",
    "            else:\n",
    "                madChordLabel = 'N'\n",
    "                    \n",
    "            newLine.append(madChordLabel)\n",
    "            newLines.append(newLine)\n",
    "            \n",
    "    newLines = pd.DataFrame(newLines, index=outChords.index,\n",
    "                            columns=[\"m21RootPitchClass\", \"m21RootPitchAccidental\", \"m21RootOctave\",\n",
    "                                     \"m21ContainsTriad\", \"m21ChordQuality\", \"m21PitchedCommonName\",\n",
    "                                     \"madChordLabel\"])\n",
    "    \n",
    "    outChords = outChords.join(newLines)\n",
    "    outChords = outChords.drop(outChords.index[dels])\n",
    "        \n",
    "    return(outChords)\n",
    "    \n",
    "# test it\n",
    "#dnn_chord_rec(inWav='data/wip/tmp/MAPS_MUS-bach_846_AkPnBcht.wav', \n",
    "#              savTo='data/wip/tmp/MAPS_MUS-bach_846_AkPnBcht.chords.dnn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for cleaning up chords\n",
    "\n",
    "tmp = glob.glob('data/wip/AkPnBcht/MUS/*.chord.y.txt')\n",
    "# 'data/wip/AkPnBcht/MUS/*.chords.dnn.txt'\n",
    "rcdCtr = 0\n",
    "flatCtr = 0\n",
    "m21Ltr = []\n",
    "madLtr = []\n",
    "m21dSharpLtr = []\n",
    "m21aSharpLtr = []\n",
    "\n",
    "for line in tmp:\n",
    "    madChords = load_chords(line)\n",
    "    \n",
    "    for i in range(madChords.shape[0]):\n",
    "        rcdCtr = rcdCtr + 1\n",
    "        if madChords.iloc[i,7][1] == '-':\n",
    "            flatCtr = flatCtr +1\n",
    "            m21Ltr.append(madChords.iloc[i,7][0]) \n",
    "            madLtr.append(madChords.iloc[i,8][0])\n",
    "\n",
    "        if madChords.iloc[i,8][0] == 'D':\n",
    "            m21dSharpLtr.append(madChords.iloc[i,7][0:1]) \n",
    "            \n",
    "        if madChords.iloc[i,8][0] == 'A':\n",
    "            m21aSharpLtr.append(madChords.iloc[i,7][0:1]) \n",
    "            \n",
    "    \n",
    "print(\"rcds:\", rcdCtr, \"flats:\", flatCtr, \"pct:\", flatCtr / rcdCtr)\n",
    "print\n",
    "\n",
    "print(\"when classed '-' by m21, what is the madmom class?\")\n",
    "for i in range(len(PITCH_CLASSES)):\n",
    "    print(PITCH_CLASSES[i], \": m21:\", m21Ltr.count(PITCH_CLASSES[i]), \": mad:\", madLtr.count(PITCH_CLASSES[i]))\n",
    "print\n",
    "    \n",
    "print(\"when classed D# by madmom, what is the m21 class?\")\n",
    "for i in range(len(PITCH_CLASSES)):\n",
    "    print(PITCH_CLASSES[i], \":\" \"m21:\", m21dSharpLtr.count(PITCH_CLASSES[i]))\n",
    "print\n",
    "\n",
    "print(\"when classed A# by madmom, what is the m21 class?\")\n",
    "for i in range(len(PITCH_CLASSES)):\n",
    "    print(PITCH_CLASSES[i], \":\" \"m21:\", m21aSharpLtr.count(PITCH_CLASSES[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = glob.glob('data/wip/AkPnBcht/MUS/*.chords.cnn.txt')\n",
    "pitchClasses = range(12)\n",
    "\n",
    "for line in tmp:\n",
    "    \n",
    "    inChords = load_chords(line)\n",
    "    \n",
    "    for i in range(inChords.shape[0]):\n",
    "        \n",
    "        if inChords.iloc[i,8][1] == '#':\n",
    "            if inChords.iloc[i,8][0] == 'D':\n",
    "                print(\"in D, assigning:\", pitchClasses.index[PITCH_CLASSES == 'E'], \"s/b 5\")\n",
    "                inChords.iloc[i,2] = pitchClasses[PITCH_CLASSES == 'E']\n",
    "                inChords.iloc[i,3] = 'accidental flat'\n",
    "            elif inChords.iloc[i,8][0] == 'A':\n",
    "                print(\"in A, assigning:\", pitchClasses.index[PITCH_CLASSES == 'B'], \"s/b 11\")\n",
    "                inChords.iloc[i,2] = pitchClasses[PITCH_CLASSES == 'B']\n",
    "                inChords.iloc[i,3] = 'accidental flat'\n",
    "                \n",
    "    save_chords(line, inChords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Note Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_note_and_chord_rec(mode, inFile, outFileBase=None):\n",
    "    \n",
    "    if outFileBase is not None:\n",
    "        outNoteFile = outFileBase + '.notes.rnn.txt'\n",
    "        outChordFile = outFileBase + '.chords.rnn.txt'\n",
    "    \n",
    "    # generate and save note transcription from .wav file\n",
    "    if mode == \"single\":\n",
    "        if outFileBase == None: \n",
    "            rtrn = ! PianoTranscriptor single {inFile}\n",
    "            return(rtrn)\n",
    "        else:\n",
    "            ! PianoTranscriptor single {inFile} -o {outNoteFile}\n",
    "    \n",
    "    elif mode == \"batch\":\n",
    "        if outFileBase == None:\n",
    "            print(\"ERROR: need an outFile (really dir) when using using batch mode.\")\n",
    "        else:\n",
    "            # assumes inFile is a list of files. assumes outFile is a directory\n",
    "            for i in range(len(inFile)): \n",
    "                ! PianoTranscriptor batch {inFile[i]} -o {outNoteFile}\n",
    "    \n",
    "    # generate and save chord transcription from note transcription\n",
    "    try:\n",
    "        # MOVE THIS TO RE-FORMATTER\n",
    "            # note save format is onset + midi (as used by eval).\n",
    "            # note -> chord format is onset + offset + midi\n",
    "        rnnNotes = pd.DataFrame(mad.features.notes.load_notes(outNoteFile)) # outNm + '.notes.rnn.txt'\n",
    "        rnnNotes.insert(1, \"Offset\", pd.Series(np.zeros(rnnNotes.shape[0]), index=rnnNotes.index))\n",
    "        rnnNotes.columns = ['OnsetTime', 'OffsetTime', 'MidiPitch']\n",
    "        rnnNotes = rnnNotes.sort_values(['OnsetTime', 'MidiPitch'], # 'OffsetTime', \n",
    "                                        axis=0, ascending=True, inplace=False,\n",
    "                                        kind='quicksort', na_position='last')\n",
    "        rnnNotes[\"MidiPitch\"] = rnnNotes['MidiPitch'].astype(int)\n",
    "    \n",
    "        # get chord preds using RNN notes\n",
    "        N, C = txt_to_y(rnnNotes, \"thisFile\")\n",
    "    \n",
    "        if C is not None: \n",
    "            save_chords(outChordFile, C)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # https://www.safaribooksonline.com/blog/2014/02/12/using-shell-commands-effectively-ipython/\n",
    "\n",
    "#, 'data/maps/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.wav']\n",
    "#rnn_note_and_chord_rec(\"single\", \n",
    "#                       'data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav', \n",
    "#                       'data/wip/tmp/MAPS_MUS-alb_se3_AkPnBcht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inAudio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chord Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''achieves 45% major/minor triad recogniation. significantly slower than dnn model'''\n",
    "\n",
    "def cnn_chord_rec(inWav, savTo=None):\n",
    "    \n",
    "    '''from Filip Korzeniowski and Gerhard Widmer, “A Fully Convolutional Deep Auditory Model for \n",
    "    Musical Chord Recognition”, Proceedings of IEEE International Workshop on Machine Learning for\n",
    "    Signal Processing (MLSP), 2016..'''\n",
    "    \n",
    "    # instantiate madmom CNNChordFeatureProcessor\n",
    "    featproc = mad.features.chords.CNNChordFeatureProcessor()\n",
    "    \n",
    "    # create DeepChromaChordRecognitionProcessor to decode chord sequence from extracted chromas\n",
    "    decode = mad.features.chords.CRFChordRecognitionProcessor()\n",
    "    \n",
    "    chordrec = mad.processors.SequentialProcessor([featproc, decode])\n",
    "    \n",
    "    madChords = chordrec(inWav)\n",
    "    \n",
    "    rtrn = to_chords(pd.DataFrame(madChords), 'madChordDF')\n",
    "    # when you decide to expand the output classes:\n",
    "        # 1. use the code here: https://github.com/CPJKU/madmom/blob/master/madmom/features/chords.py\n",
    "        # 2. this: DeepChromaChordRecognitionProcessor() calls this: majmin_targets_to_chord_labels()\n",
    "        # 3. the latter implements 25 classes (including N for no chord) using pred_to_cl(pred)\n",
    "        # 4. so, the net is outputing preds for these classes. class preds are translated to 0-23 + 24 labels\n",
    "        # 5. to go beyond these classes you're going to need to either:\n",
    "            # a. roll-your-own net w/ more classes\n",
    "            # b. cross vector using output from other algos i.e., RNN note or CNN chord... START HERE.\n",
    "    \n",
    "    if savTo is not None:\n",
    "        save_chords(savTo, rtrn) \n",
    "    else:\n",
    "        return(rtrn)\n",
    "\n",
    "#cnn_chord_rec('data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav',\n",
    "#              'data/wip/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.madChordCnn.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''achieves:\n",
    "    1. 22% precision and 6% fscore on collection of map music\n",
    "    2. 35% major/minor triad recognition, \n",
    "    so, clearly the 70% fscore numbers they're giving are on major/minor triads w/in a target range'''\n",
    "\n",
    "def dnn_chord_rec(inWav, savTo=None):\n",
    "    \n",
    "    '''from Filip Korzeniowski and Gerhard Widmer, “Feature Learning for Chord Recognition: The Deep\n",
    "    Chroma Extractor”, Proceedings of the 17th International Society for Music Information Retrieval\n",
    "    Conference (ISMIR), 2016.'''\n",
    "    \n",
    "    # instantiate madmom deep chroma processor to extract chroma vectors\n",
    "    dcp = mad.audio.chroma.DeepChromaProcessor()\n",
    "    \n",
    "    # create DeepChromaChordRecognitionProcessor to decode chord sequence from extracted chromas\n",
    "    decode = mad.features.chords.DeepChromaChordRecognitionProcessor()\n",
    "    \n",
    "    # SequentialProcessor links dcp and decode steps to transcribe chord(s)\n",
    "    chordrec = mad.processors.SequentialProcessor([dcp, decode])\n",
    "    \n",
    "    madChords = chordrec(inWav)\n",
    "    \n",
    "    rtrn = to_chords(pd.DataFrame(madChords), 'madChordDF')\n",
    "    \n",
    "    if savTo is not None:\n",
    "        save_chords(savTo, rtrn)  \n",
    "    else:\n",
    "        return(rtrn)\n",
    "\n",
    "#dnn_rslts = dnn_chord_rec('data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav')\n",
    "\n",
    "#dcc_rslts = dcc_chord_rec('data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav',\n",
    "#                          'data/wip/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.madChordDcc.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Find Common Chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get most commonly occuring chords in a dataset\n",
    "\n",
    "chordFiles = glob.glob('data/wip/AkPnBcht/MUS/*.chords.y.txt')\n",
    "\n",
    "for i in range(len(chordFiles)):\n",
    "    chords = load_chords(chordFiles[i])\n",
    "    #print(chords)\n",
    "    if i == 0:\n",
    "        allChords = chords.m21PitchedCommonName\n",
    "    else:\n",
    "        allChords = allChords.append(chords.m21PitchedCommonName)\n",
    "\n",
    "allChords = allChords.tolist(); print(len(allChords))\n",
    "allChords.sort()\n",
    "uniqueChords = list(set(allChords))\n",
    "uniqueChords.sort()\n",
    "\n",
    "counts = []\n",
    "for i in range(len(uniqueChords)):\n",
    "    counts.append(allChords.count(uniqueChords[i]))\n",
    "\n",
    "print(sum(counts))\n",
    "\n",
    "outP = zip(uniqueChords, counts)\n",
    "outP.sort(key=lambda tup: tup[1], reverse=True)\n",
    "print(pd.DataFrame(outP[0:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chord rec notes:\n",
    "'''http://www.audiocommons.org/assets/files/AC-WP4-UPF-D4.1%20Report%20on%20the%20analysis%20and%20compilation%20of%20state-of-the-art%20methods%20for%20the%20automatic%20annotation%20of%20music%20pieces%20and%20music%20samples.pdf\n",
    "Audio Commons, deliverable d4: Report on the analysis and compilation of state-of-the-art methods \n",
    "for the automatic annotation of music pieces and music samples:\n",
    "\n",
    "Recent work utilize deep learning to learn alternative features for replacing chroma features [Zhou15]. In\n",
    "their work, authors investigate two types of architectures for the neural net, a common one in which the\n",
    "amount of neurons is the same in every layer, and a bottleneck-shaped architecture in which the middle\n",
    "layer has fewer neurons. Grézl et al. claim [Grézl07] that Bottleneck architecture is more suitable to\n",
    "learn high-level features than common one, and that it reduces overfitting. Moreover, following Zhou and\n",
    "Lerch results [Zhou15], it leads to better results for chord recognition than a common architecture.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Evaluate Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTES\n",
    "# 'data/wip/AkPnBcht/MUS/*.note.y.txt'\n",
    "# 'data/wip/AkPnBcht/MUS/*.notes.rnn.txt'\n",
    "noteAnnots = glob.glob('data/wip/AkPnBcht/UCHO/I60-68/*.note.y.txt')\n",
    "noteDetects = glob.glob('data/wip/AkPnBcht/UCHO/I60-68/*.notes.rnn.txt')\n",
    "\n",
    "if len(noteAnnots) == len(noteDetects):\n",
    "    eval_objs = []\n",
    "    for i in range(len(noteAnnots)):\n",
    "        annotations = mad.features.notes.load_notes(noteAnnots[i])\n",
    "        detections = mad.features.notes.load_notes(noteDetects[i])\n",
    "        \n",
    "        eval_objs.append(mad.evaluation.notes.NoteEvaluation(detections, annotations, window=0.025, delay=0))\n",
    "    \n",
    "    print('***** Note performance:')\n",
    "    print(mad.evaluation.notes.NoteSumEvaluation(eval_objs, name=None).tostring())\n",
    "    \n",
    "else:\n",
    "    print(\"missing Annots or Detects\")\n",
    "    end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Evaluate Chords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Onset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def onset_evaluation(detections, annotations, window=WINDOW):\n",
    "    \"\"\"\n",
    "    Determine the true/false positive/negative detections.\n",
    "    Parameters\n",
    "    ----------\n",
    "    detections : numpy array\n",
    "        Detected notes.\n",
    "    annotations : numpy array\n",
    "        Annotated ground truth notes.\n",
    "    window : float, optional\n",
    "        Evaluation window [seconds].\n",
    "    Returns\n",
    "    -------\n",
    "    tp : numpy array, shape (num_tp,)\n",
    "        True positive detections.\n",
    "    fp : numpy array, shape (num_fp,)\n",
    "        False positive detections.\n",
    "    tn : numpy array, shape (0,)\n",
    "        True negative detections (empty, see notes).\n",
    "    fn : numpy array, shape (num_fn,)\n",
    "        False negative detections.\n",
    "    errors : numpy array, shape (num_tp,)\n",
    "        Errors of the true positive detections wrt. the annotations.\n",
    "    Notes\n",
    "    -----\n",
    "    The returned true negative array is empty, because we are not interested\n",
    "    in this class, since it is magnitudes bigger than true positives array.\n",
    "    \"\"\"\n",
    "    # make sure the arrays have the correct types and dimensions\n",
    "    detections = np.asarray(detections) # , dtype=np.float\n",
    "    annotations = np.asarray(annotations) # , dtype=np.float\n",
    "    # TODO: right now, it only works with 1D arrays\n",
    "    if detections.ndim > 1 or annotations.ndim > 1:\n",
    "        raise NotImplementedError('please implement multi-dim support')\n",
    "\n",
    "    # init TP, FP, FN and errors\n",
    "    tp = np.zeros(0)\n",
    "    fp = np.zeros(0)\n",
    "    tn = np.zeros(0)  # we will not alter this array\n",
    "    fn = np.zeros(0)\n",
    "    errors = np.zeros(0)\n",
    "\n",
    "    # if neither detections nor annotations are given\n",
    "    if len(detections) == 0 and len(annotations) == 0:\n",
    "        # return the arrays as is\n",
    "        return tp, fp, tn, fn, errors\n",
    "    # if only detections are given\n",
    "    elif len(annotations) == 0:\n",
    "        # all detections are FP\n",
    "        return tp, detections, tn, fn, errors\n",
    "    # if only annotations are given\n",
    "    elif len(detections) == 0:\n",
    "        # all annotations are FN\n",
    "        return tp, fp, tn, annotations, errors\n",
    "\n",
    "    # window must be greater than 0\n",
    "    if float(window) <= 0:\n",
    "        raise ValueError('window must be greater than 0')\n",
    "\n",
    "    # sort the detections and annotations\n",
    "    det = np.sort(detections)\n",
    "    ann = np.sort(annotations)\n",
    "    # cache variables\n",
    "    det_length = len(detections)\n",
    "    ann_length = len(annotations)\n",
    "    det_index = 0\n",
    "    ann_index = 0\n",
    "    # iterate over all detections and annotations\n",
    "    while det_index < det_length and ann_index < ann_length:\n",
    "        # fetch the first detection\n",
    "        d = det[det_index]\n",
    "        # fetch the first annotation\n",
    "        a = ann[ann_index]\n",
    "        # compare them\n",
    "        if abs(d - a) <= window:\n",
    "            # TP detection\n",
    "            tp = np.append(tp, d)\n",
    "            # append the error to the array\n",
    "            errors = np.append(errors, d - a)\n",
    "            # increase the detection and annotation index\n",
    "            det_index += 1\n",
    "            ann_index += 1\n",
    "        elif d < a:\n",
    "            # FP detection\n",
    "            fp = np.append(fp, d)\n",
    "            # increase the detection index\n",
    "            det_index += 1\n",
    "            # do not increase the annotation index\n",
    "        elif d > a:\n",
    "            # we missed a annotation: FN\n",
    "            fn = np.append(fn, a)\n",
    "            # do not increase the detection index\n",
    "            # increase the annotation index\n",
    "            ann_index += 1\n",
    "        else:\n",
    "            # can't match detected with annotated onset\n",
    "            raise AssertionError('can not match % with %', d, a)\n",
    "    # the remaining detections are FP\n",
    "    fp = np.append(fp, det[det_index:])\n",
    "    # the remaining annotations are FN\n",
    "    fn = np.append(fn, ann[ann_index:])\n",
    "    # check calculations\n",
    "    if len(tp) + len(fp) != len(detections):\n",
    "        raise AssertionError('bad TP / FP calculation')\n",
    "    if len(tp) + len(fn) != len(annotations):\n",
    "        raise AssertionError('bad FN calculation')\n",
    "    if len(tp) != len(errors):\n",
    "        raise AssertionError('bad errors calculation')\n",
    "    # convert to numpy arrays and return them\n",
    "    return np.array(tp), np.array(fp), tn, np.array(fn), np.array(errors)    \n",
    "    \n",
    "def ChordOnsetEvaluation(detections, annotations, window):\n",
    "    \n",
    "    # init TP, FP, TN and FN lists\n",
    "    tp = np.zeros((0, 2))\n",
    "    fp = np.zeros((0, 2))\n",
    "    tn = np.zeros((0, 2))  # this will not be altered\n",
    "    fn = np.zeros((0, 2))\n",
    "    errors = np.zeros((0, 2))\n",
    "    \n",
    "    # get a list of all chords detected / annotated\n",
    "    chords = np.unique(np.concatenate((detections[:,1],\n",
    "                                       annotations[:,1]))).tolist()\n",
    "    #print(\"chords:\")\n",
    "    #print(chords)\n",
    "    # iterate over all chords\n",
    "    for chord in chords:\n",
    "        # perform normal onset detection on each chord\n",
    "        det = detections[detections[:, 1] == chord]\n",
    "        ann = annotations[annotations[:, 1] == chord]\n",
    "        tp_, fp_, _, fn_, err_ = onset_evaluation(det[:, 0], ann[:, 0], window)\n",
    "        \n",
    "        # convert returned arrays to lists and append the detections and\n",
    "        # annotations to the correct lists\n",
    "        tp = np.vstack((tp, det[np.in1d(det[:, 0], tp_)]))\n",
    "        fp = np.vstack((fp, det[np.in1d(det[:, 0], fp_)]))\n",
    "        fn = np.vstack((fn, ann[np.in1d(ann[:, 0], fn_)]))\n",
    "        \n",
    "        # append the chord number to the errors\n",
    "        err_ = np.vstack((np.array(err_),\n",
    "                          np.repeat(np.asarray([chord]), len(err_)))).T\n",
    "        errors = np.vstack((errors, err_))\n",
    "        \n",
    "    # check calculations\n",
    "    #print(\"len(tp):\", len(tp))\n",
    "    #print(\"len(fp):\", len(fp))\n",
    "    #print(\"len(detections:)\", len(detections))\n",
    "    if len(tp) + len(fp) != len(detections):\n",
    "        raise AssertionError('bad TP / FP calculation')\n",
    "    if len(tp) + len(fn) != len(annotations):\n",
    "        raise AssertionError('bad FN calculation')\n",
    "    if len(tp) != len(errors):\n",
    "        raise AssertionError('bad errors calculation')\n",
    "        \n",
    "    # sort the arrays\n",
    "    # Note: The errors must have the same sorting order as the TPs, so they\n",
    "    #       must be done first (before the TPs get sorted)\n",
    "    errors = errors[tp[:, 0].argsort()]\n",
    "    tp = tp[tp[:, 0].argsort()]\n",
    "    fp = fp[fp[:, 0].argsort()]\n",
    "    fn = fn[fn[:, 0].argsort()]\n",
    "    \n",
    "    # return the arrays\n",
    "    return tp, fp, tn, fn, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Chord Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassEvaluation(mad.evaluation.Evaluation):\n",
    "    \"\"\"\n",
    "    Evaluation class for measuring Precision, Recall and F-measure based on\n",
    "    2D numpy arrays with true/false positive/negative detections.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tp : list of tuples or numpy array, shape (num_tp, 2)\n",
    "        True positive detections.\n",
    "    fp : list of tuples or numpy array, shape (num_fp, 2)\n",
    "        False positive detections.\n",
    "    tn : list of tuples or numpy array, shape (num_tn, 2)\n",
    "        True negative detections.\n",
    "    fn : list of tuples or numpy array, shape (num_fn, 2)\n",
    "        False negative detections.\n",
    "    name : str\n",
    "        Name to be displayed.\n",
    "    Notes\n",
    "    -----\n",
    "    The second item of the tuples or the second column of the arrays denote\n",
    "    the class the detection belongs to.\n",
    "    \"\"\"\n",
    "    def __init__(self, tp=None, fp=None, tn=None, fn=None, **kwargs):\n",
    "        # set default values\n",
    "        if tp is None:\n",
    "            tp = np.zeros((0, 2))\n",
    "        if fp is None:\n",
    "            fp = np.zeros((0, 2))\n",
    "        if tn is None:\n",
    "            tn = np.zeros((0, 2))\n",
    "        if fn is None:\n",
    "            fn = np.zeros((0, 2))\n",
    "        super(MultiClassEvaluation, self).__init__(**kwargs)\n",
    "        self.tp = np.asarray(tp) # , dtype=np.float\n",
    "        self.fp = np.asarray(fp) # , dtype=np.float\n",
    "        self.tn = np.asarray(tn) # , dtype=np.float\n",
    "        self.fn = np.asarray(fn) # , dtype=np.float\n",
    "\n",
    "    def tostring(self, verbose=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Format the evaluation metrics as a human readable string.\n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : bool\n",
    "            Add evaluation for individual classes.\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Evaluation metrics formatted as a human readable string.\n",
    "        \"\"\"\n",
    "        ret = ''\n",
    "\n",
    "        if verbose:\n",
    "            # extract all classes\n",
    "            classes = []\n",
    "            if self.tp.any():\n",
    "                classes = np.append(classes, np.unique(self.tp[:, 1]))\n",
    "            if self.fp.any():\n",
    "                classes = np.append(classes, np.unique(self.fp[:, 1]))\n",
    "            if self.tn.any():\n",
    "                classes = np.append(classes, np.unique(self.tn[:, 1]))\n",
    "            if self.fn.any():\n",
    "                classes = np.append(classes, np.unique(self.fn[:, 1]))\n",
    "            for cls in sorted(np.unique(classes)):\n",
    "                # extract the TP, FP, TN and FN of this class\n",
    "                tp = self.tp[self.tp[:, 1] == cls]\n",
    "                fp = self.fp[self.fp[:, 1] == cls]\n",
    "                tn = self.tn[self.tn[:, 1] == cls]\n",
    "                fn = self.fn[self.fn[:, 1] == cls]\n",
    "                # evaluate them\n",
    "                e = Evaluation(tp, fp, tn, fn, name='Class %s' % cls)\n",
    "                # append to the output string\n",
    "                ret += '  %s\\n' % e.tostring(verbose=False)\n",
    "        # normal formatting\n",
    "        ret += 'Annotations: %5d TP: %5d FP: %4d FN: %4d ' \\\n",
    "               'Precision: %.3f Recall: %.3f F-measure: %.3f Acc: %.3f' % \\\n",
    "               (self.num_annotations, self.num_tp, self.num_fp, self.num_fn,\n",
    "                self.precision, self.recall, self.fmeasure, self.accuracy)\n",
    "        # return\n",
    "        return ret\n",
    "\n",
    "    # for chord evaluation with Precision, Recall, F-measure use the Evaluation\n",
    "# class and just define the evaluation function\n",
    "# TODO: extend to also report the measures without octave errors\n",
    "\n",
    "class ChordEvaluation(MultiClassEvaluation):\n",
    "    \"\"\"\n",
    "        Evaluation class for measuring Precision, Recall and F-measure of chords.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        detections : str, list or numpy array\n",
    "            Detected chords.\n",
    "            \n",
    "        annotations : str, list or numpy array\n",
    "            Annotated ground truth chords.\n",
    "            \n",
    "        window : float, optional\n",
    "            F-measure evaluation window [seconds]\n",
    "            \n",
    "        delay : float, optional\n",
    "            Delay the detections `delay` seconds for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, detections, annotations, evalCols, window=WINDOW, delay=0,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # load the chord detections and annotations\n",
    "        detections = pd.read_table(detections)\n",
    "        annotations = pd.read_table(annotations)\n",
    "        \n",
    "        # shift the detections if needed\n",
    "        if delay != 0:\n",
    "            detections.iloc[:, 0] += delay\n",
    "        \n",
    "        # build input array        \n",
    "        if len(evalCols) == 2:\n",
    "            outDet = np.array(detections.iloc[:,evalCols])\n",
    "            outAnn = np.array(annotations.iloc[:,evalCols])\n",
    "            \n",
    "        else:\n",
    "            outDet = detections.iloc[:,evalCols[0]]\n",
    "            outAnn = annotations.iloc[:,evalCols[0]]\n",
    "            \n",
    "            classDet = detections.iloc[:,evalCols[1]].tolist()\n",
    "            classAnn = annotations.iloc[:,evalCols[1]].tolist()\n",
    "\n",
    "            for i in range(2,len(evalCols)):\n",
    "                newDet = detections.iloc[:,evalCols[i]]\n",
    "                newAnn = annotations.iloc[:,evalCols[i]]\n",
    "                \n",
    "                for j in range(len(classDet)):\n",
    "                    classDet[j] = str(classDet[j]) + str(newDet[j])\n",
    "                \n",
    "                for j in range(len(classAnn)):\n",
    "                    classAnn[j] = str(classAnn[j]) + str(newAnn[j])\n",
    "            \n",
    "            outDet = np.array(pd.DataFrame({'a': outDet, 'b':classDet}))\n",
    "            outAnn = np.array(pd.DataFrame({'a': outAnn, 'b':classAnn}))\n",
    "        \n",
    "        # evaluate onsets (passing only onset and classifier)\n",
    "        perf = ChordOnsetEvaluation(outDet, outAnn, window)\n",
    "        tp, fp, tn, fn, errors = perf\n",
    "\n",
    "        super(ChordEvaluation, self).__init__(tp, fp, tn, fn, **kwargs)\n",
    "        self.errors = errors\n",
    "        \n",
    "        # save them for the individual chord evaluation\n",
    "        self.detections = detections\n",
    "        self.annotations = annotations\n",
    "        self.window = window\n",
    "    \n",
    "    @property\n",
    "    def mean_error(self):\n",
    "        \"\"\"Mean of the errors.\"\"\"\n",
    "        warnings.warn('mean_error is given for all chords, this will change!')\n",
    "        if len(self.errors) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return np.nanmean(self.errors[:,0].astype(np.float))\n",
    "    \n",
    "    @property\n",
    "    def std_error(self):\n",
    "        \"\"\"Standard deviation of the errors.\"\"\"\n",
    "        warnings.warn('std_error is given for all chords, this will change!')\n",
    "        if len(self.errors) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return np.nanstd(self.errors[:,0].astype(np.float))\n",
    "    \n",
    "    def tostring(self, chords=False, **kwargs):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ----------\n",
    "            chords : bool, optional\n",
    "                Display detailed output for all individual chords.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            str\n",
    "                Evaluation metrics formatted as a human readable string.\n",
    "        \"\"\"\n",
    "        \n",
    "        ret = ''\n",
    "        if self.name is not None:\n",
    "            ret += '%s\\n  ' % self.name\n",
    "        \n",
    "        # add statistics for the individual chord\n",
    "        if chords:\n",
    "            \n",
    "            # determine which chords are present\n",
    "            chords = []\n",
    "            if self.tp.any():\n",
    "                chords = np.append(chords, np.unique(self.tp[:, 1]))\n",
    "            if self.fp.any():\n",
    "                chords = np.append(chords, np.unique(self.fp[:, 1]))\n",
    "            if self.tn.any():\n",
    "                chords = np.append(chords, np.unique(self.tn[:, 1]))\n",
    "            if self.fn.any():\n",
    "                chords = np.append(chords, np.unique(self.fn[:, 1]))\n",
    "\n",
    "            # evaluate them individually\n",
    "            for chord in sorted(np.unique(chords)):\n",
    "                \n",
    "                # detections and annotations for this chord (only onset times)\n",
    "                det = self.detections[self.detections[:, 1] == chord][:, 0]\n",
    "                ann = self.annotations[self.annotations[:, 1] == chord][:, 0]\n",
    "                name = 'chord %s' % chord\n",
    "                e = mad.evaluation.onsets.OnsetEvaluation(det, ann, self.window, name=name)\n",
    "                \n",
    "                # append to the output string\n",
    "                ret += '  %s\\n' % e.tostring(chords=False)\n",
    "                    \n",
    "        # normal formatting\n",
    "        ret += 'chords: %5d TP: %5d FP: %4d FN: %4d ' \\\n",
    "            'Precision: %.3f Recall: %.3f F-measure: %.3f ' \\\n",
    "                'Acc: %.3f mean: %5.1f ms std: %5.1f ms' % \\\n",
    "                    (self.num_annotations, self.num_tp, self.num_fp, self.num_fn,\n",
    "                     self.precision, self.recall, self.fmeasure, self.accuracy,\n",
    "                     self.mean_error * 1000., self.std_error * 1000.)\n",
    "        # return\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii. Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChordSumEvaluation(mad.evaluation.SumEvaluation, ChordEvaluation):\n",
    "    \"\"\"\n",
    "        Class for summing chord evaluations.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def errors(self):\n",
    "        \"\"\"Errors of the true positive detections wrt. the ground truth.\"\"\"\n",
    "        if not self.eval_objects:\n",
    "            # return empty array\n",
    "            return np.zeros((0, 2))\n",
    "        return np.concatenate([e.errors for e in self.eval_objects])\n",
    "\n",
    "\n",
    "class ChordMeanEvaluation(mad.evaluation.MeanEvaluation, ChordSumEvaluation):\n",
    "    \"\"\"\n",
    "        Class for averaging chord evaluations.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def mean_error(self):\n",
    "        \"\"\"Mean of the errors.\"\"\"\n",
    "        warnings.warn('mean_error is given for all chords, this will change!')\n",
    "        return np.nanmean([e.mean_error for e in self.eval_objects])\n",
    "    \n",
    "    @property\n",
    "    def std_error(self):\n",
    "        \"\"\"Standard deviation of the errors.\"\"\"\n",
    "        warnings.warn('std_error is given for all chords, this will change!')\n",
    "        return np.nanmean([e.std_error for e in self.eval_objects])\n",
    "    \n",
    "    def tostring(self, **kwargs):\n",
    "        \"\"\"\n",
    "            Format the evaluation metrics as a human readable string.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            str\n",
    "                Evaluation metrics formatted as a human readable string.\n",
    "        \"\"\"\n",
    "        \n",
    "        # format with floats instead of integers\n",
    "        ret = ''\n",
    "        if self.name is not None:\n",
    "            ret += '%s\\n  ' % self.name\n",
    "        ret += 'Chords: %5.2f TP: %5.2f FP: %5.2f FN: %5.2f ' \\\n",
    "            'Precision: %.3f Recall: %.3f F-measure: %.3f ' \\\n",
    "                'Acc: %.3f mean: %5.1f ms std: %5.1f ms' % \\\n",
    "                    (self.num_annotations, self.num_tp, self.num_fp, self.num_fn,\n",
    "                     self.precision, self.recall, self.fmeasure, self.accuracy,\n",
    "                     self.mean_error * 1000., self.std_error * 1000.)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHORDS\n",
    "\n",
    "# Evaluate chords\n",
    "'''In music theory, the concept of root denotes the idea that a chord can be represented and named by one of its notes. It is linked to harmonic thinking, that is, to the idea that vertical aggregates of notes can form a single unit, a chord. It is in this sense that one can speak of a \"C chord\", or a \"chord on C\", a chord built from \"C\" and of which the note (or pitch) \"C\" is the root. When a C chord is referred to in Classical music or popular music without a reference to what type of chord it is (either Major or minor, in most cases), this chord is assumed to be a C major triad, which contains the notes C, E and G. The root needs not be the bass note, the lowest note of the chord: the concept of root is linked to that of the inversion of chords, which is derived from the notion of invertible counterpoint. In this concept, chords can be inverted while still retaining their root.\n",
    "\n",
    "'''\n",
    "\n",
    "chdRnnDet = glob.glob('data/wip/AkPnBcht/MUS/*.chords.rnn.txt')\n",
    "chdCnnDet = glob.glob('data/wip/AkPnBcht/MUS/*.chords.cnn.txt')\n",
    "chdDnnDet = glob.glob('data/wip/AkPnBcht/MUS/*.chords.dnn.txt')\n",
    "chdAnnot = glob.glob('data/wip/AkPnBcht/MUS/*.chords.y.txt')\n",
    "\n",
    "fSets = [chdRnnDet, chdCnnDet, chdDnnDet]\n",
    "fSetTtl = ['Note RNN Chord Pred', 'CNN Chord Preds', 'DNN Chord Preds']\n",
    "evalCols = [(0,2),(0,2,3),(0,4),(0,2,3,4),(0,6),(0,8),(0,7)]\n",
    "\n",
    "for h in range(len(fSets)):\n",
    "    pcObjs = []; pObjs = []; oObjs = []; nObjs = []; qObjs = []; madObjs = []; m21Objs = []\n",
    "    \n",
    "    if h == 0: \n",
    "        delay = 0.01 # RNN delay\n",
    "        window = 0.05\n",
    "    elif h == 1:\n",
    "        delay = 0.1 # CNN delay\n",
    "        window = 0.5\n",
    "    elif h == 2:\n",
    "        delay = 0.01 # DNN delay\n",
    "        window = 0.5\n",
    "    \n",
    "    for i in range(len(fSets[h])): \n",
    "        # pitch class performance\n",
    "        pcObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[0], window=window, delay=delay))\n",
    "        \n",
    "        # pitch (class + accidental) performance\n",
    "        pObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[1], window=window, delay=delay))\n",
    "        \n",
    "        # octave performance\n",
    "        oObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[2], window=window, delay=delay))\n",
    "        \n",
    "        # note (pitch + octave) performance\n",
    "        nObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[3], window=window, delay=delay))\n",
    "        \n",
    "        # quality performance\n",
    "        qObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[4], window=window, delay=delay))\n",
    "        \n",
    "        # mad performance\n",
    "        madObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[5], window=window, delay=delay))\n",
    "        \n",
    "        # m21 performance\n",
    "        m21Objs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[6], window=window, delay=delay))\n",
    "        \n",
    "        \n",
    "    print('**********', fSetTtl[h])\n",
    "    print('*** Pitch Class:')\n",
    "    print(ChordSumEvaluation(pcObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Pitch:')\n",
    "    print(ChordSumEvaluation(pObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Octave:')\n",
    "    print(ChordSumEvaluation(oObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Note:')\n",
    "    print(ChordSumEvaluation(nObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Quality:')\n",
    "    print(ChordSumEvaluation(qObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Mad Class:')\n",
    "    print(ChordSumEvaluation(madObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** M21 Class:')\n",
    "    print(ChordSumEvaluation(m21Objs, name=None).tostring())\n",
    "    print\n",
    "    print\n",
    "    \n",
    "    '''note: for cnn / dnn chord rec, precision is the key metric i.e., when I predict, do I predict correctly.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_offset_window(inDetLsts, inDetNms, inAnnotLst):\n",
    "    \n",
    "    # technically should take 5% of rcds to determine best offest, window. not going to do that.    \n",
    "    dly = np.round(np.arange(start=0.01, stop=0.11, step=0.01),3).tolist() + \\\n",
    "    np.round(np.arange(start=0.12, stop=0.22, step=0.02),3).tolist()\n",
    "    # + np.round(np.arange(start=0.23, stop=0.5, step=0.03),3).tolist()\n",
    "    wndw = dly\n",
    "    \n",
    "    evalCols = (0,8)\n",
    "    rslts = []\n",
    "    \n",
    "    # for each prediction algorithm,...\n",
    "    for h in range(len(inDetLsts)):\n",
    "        \n",
    "        lnCtr = 0\n",
    "        rsltMtrx = pd.DataFrame(np.ndarray((len(dly)*len(wndw),7)),\n",
    "                                columns=[\"predictor\", \"dly\", \"wndw\", \"prec\", \"recall\", \"fScr\", \"acc\"])\n",
    "        evalObjs = []\n",
    "        \n",
    "        # ...try different onset delays and...\n",
    "        for i in range(len(dly)):\n",
    "            \n",
    "            # ...different window durations...\n",
    "            for j in range(len(wndw)):\n",
    "                \n",
    "                # ...across the sample of prediction files...\n",
    "                for k in range(len(inDetLsts[h])):\n",
    "                    \n",
    "                    evalObjs.append(ChordEvaluation(inDetLsts[h][k], inAnnotLst[k],\n",
    "                                                    evalCols, window=wndw[j], delay=dly[i]))\n",
    "                \n",
    "                # summarize results across files by window and delay\n",
    "                evalSum = ChordSumEvaluation(evalObjs, name=None)\n",
    "                \n",
    "                rsltMtrx.iloc[lnCtr,0] = inDetNms[h]\n",
    "                rsltMtrx.iloc[lnCtr,1] = dly[i]\n",
    "                rsltMtrx.iloc[lnCtr,2] = wndw[j]\n",
    "                rsltMtrx.iloc[lnCtr,3] = evalSum.precision\n",
    "                rsltMtrx.iloc[lnCtr,4] = evalSum.recall\n",
    "                rsltMtrx.iloc[lnCtr,5] = evalSum.fmeasure\n",
    "                rsltMtrx.iloc[lnCtr,6] = evalSum.accuracy\n",
    "                lnCtr = lnCtr + 1\n",
    "        \n",
    "        rsltMtrx = rsltMtrx.sort_values(by='fScr', axis=0, ascending=False, na_position='last').iloc[0:10,:]\n",
    "        rslts.append(rsltMtrx)\n",
    "    \n",
    "    return(rslts)\n",
    "                \n",
    "rnnDet = ['data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.chords.rnn.txt', \n",
    "          'data/wip/AkPnBcht/MUS/MAPS_MUS-schumm-6_AkPnBcht.chords.rnn.txt'] #glob.glob('data/wip/AkPnBcht/MUS/*.chords.rnn.txt')\n",
    "\n",
    "cnnDet = ['data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.chords.cnn.txt',\n",
    "         'data/wip/AkPnBcht/MUS/MAPS_MUS-schumm-6_AkPnBcht.chords.cnn.txt'] #glob.glob('data/wip/AkPnBcht/MUS/*.chords.cnn.txt')\n",
    "\n",
    "dnnDet = ['data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.chords.dnn.txt',\n",
    "         'data/wip/AkPnBcht/MUS/MAPS_MUS-schumm-6_AkPnBcht.chords.dnn.txt'] #glob.glob('data/wip/AkPnBcht/MUS/*.chords.dnn.txt')\n",
    "\n",
    "fSet = [rnnDet, cnnDet, dnnDet]\n",
    "\n",
    "fSetNm = ['RNN Chord Preds', 'CNN Chord Preds', 'DNN Chord Preds']\n",
    "\n",
    "annot = ['data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.chords.y.txt',\n",
    "        'data/wip/AkPnBcht/MUS/MAPS_MUS-schumm-6_AkPnBcht.chords.y.txt']    #glob.glob('data/wip/AkPnBcht/MUS/*.chords.y.txt')\n",
    "\n",
    "rslt = find_offset_window(fSet, fSetNm, annot)\n",
    "\n",
    "'''Looking for best delay and window, I find that by max'ing window up to 0.5 seconds, I still only achieve\n",
    "0.044 and 0.033 F Scores for CNN and DNN chord recognition algos vs. 0.9 for RNN note -> chord rec. The basic\n",
    "problem is: \n",
    "1. that the CNN and DNN chord recognition algos work only in a narrow range of:\n",
    "    a. major/minor triads, \n",
    "    b. in mid-range keys e.g., midi 60-70. even then they only achieve 0.4-ish and 0.3-ish F Scores.\n",
    "2. while the most popular, they represent a very small portion of total chords played.\n",
    "3. the more accurate CNN algo is prohibitively slow, inappropriate for online processing.\n",
    "\n",
    "It's unlikely that they will contribute meaningfully to an improvement to the RNN Chord Rec algo. So, rather\n",
    "than attempt to combine records then use a GBM, time is better focused on:\n",
    "1. operationalizing the RNN, then\n",
    "2. introducing the language model... using index search to find song so that next note/chord expectation is known, then\n",
    "3. swinging back at some point to train the algos on a broader set of keys, speed processing, etc. when you do this, \n",
    "here are the top 30 chords w/ window = 0.01 rounding:\n",
    "0    C4-interval class 4  136\n",
    "1         G2-major triad  135\n",
    "2              A2-unison  120\n",
    "3         F4-major triad  119\n",
    "4   B-4-interval class 4  116\n",
    "5    D4-interval class 3  115\n",
    "6   E-4-interval class 4  110\n",
    "7    E4-interval class 3  108\n",
    "8              B2-unison  107\n",
    "9    G4-interval class 4  106\n",
    "10   C5-interval class 4  105\n",
    "11  G#4-interval class 3  104\n",
    "12   C4-interval class 3  103\n",
    "13  G#3-interval class 3  100\n",
    "14   B3-interval class 3   97\n",
    "15             F3-unison   97\n",
    "16  F#4-interval class 3   96\n",
    "17  E-5-interval class 4   94\n",
    "18            G#2-unison   92\n",
    "19   G3-interval class 4   92\n",
    "20        G4-major triad   92\n",
    "21            C#3-unison   91\n",
    "22             E2-unison   91\n",
    "23             A3-unison   89\n",
    "24  C#4-interval class 3   89\n",
    "25             D2-unison   88\n",
    "26   A4-interval class 3   87\n",
    "27       E-4-major triad   87\n",
    "28  B-3-interval class 4   85\n",
    "29       E-3-major triad   84\n",
    "#def combined_chord_pred():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Read / Write Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_maps_note_file(txt_file):\n",
    "    # reads text file into dataframe, sort and round\n",
    "    \n",
    "    lines = [line.rstrip('\\n').split('\\t') for line in open(txt_file, 'U')]\n",
    "    \n",
    "    headers = lines[0]; lines = lines[1:len(lines)]\n",
    "    \n",
    "    lines = pd.DataFrame(lines, columns=[headers[0], headers[1], \n",
    "                                         headers[2]]).convert_objects(convert_numeric=True)\n",
    "    \n",
    "    lines = lines.round({'OnsetTime': 2, 'OffsetTime': 2, 'MidiPitch': 0})\n",
    "    \n",
    "    lines = lines.sort_values(['OnsetTime', 'MidiPitch'], # 'OffsetTime', \n",
    "                              axis=0, ascending=True, inplace=False, \n",
    "                              kind='quicksort', na_position='last')\n",
    "    \n",
    "    '''sort is tricky. some chord notes have same onset, different offsets. technically, the shorter note\n",
    "    should probably appear first (i.e., ascending sort: onset, offset, pitch). problem is you get an \n",
    "    expanding set of chord classes most of which don't sound musically different (i.e., imperceptible \n",
    "    differences in offset). so, i'm sorting above using ascending: onset, midi (after pitch, offset does not matter)''' \n",
    "    \n",
    "    \n",
    "    return(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Configure Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. In From Microphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Establish Microphone Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"Library for performing speech recognition, with support for several engines and APIs, online and offline.\"\"\"\n",
    "\n",
    "__author__ = \"Anthony Zhang (Uberi)\"\n",
    "__version__ = \"3.4.6\"\n",
    "__license__ = \"BSD\"\n",
    "\n",
    "import io, os, subprocess, wave, aifc, math, audioop\n",
    "import collections, threading\n",
    "import platform, stat\n",
    "import json, hashlib, hmac, time, base64, random, uuid\n",
    "import tempfile, shutil\n",
    "\n",
    "try: # attempt to use the Python 2 modules\n",
    "    from urllib import urlencode\n",
    "    from urllib2 import Request, urlopen, URLError, HTTPError\n",
    "except ImportError: # use the Python 3 modules\n",
    "    from urllib.parse import urlencode\n",
    "    from urllib.request import Request, urlopen\n",
    "    from urllib.error import URLError, HTTPError\n",
    "\n",
    "# define exceptions\n",
    "class WaitTimeoutError(Exception): pass\n",
    "class RequestError(Exception): pass\n",
    "class UnknownValueError(Exception): pass\n",
    "\n",
    "class AudioSource(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\"this is an abstract class\")\n",
    "\n",
    "    def __enter__(self):\n",
    "        raise NotImplementedError(\"this is an abstract class\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        raise NotImplementedError(\"this is an abstract class\")\n",
    "\n",
    "class Microphone(AudioSource):\n",
    "    \"\"\"\n",
    "    Creates a new ``Microphone`` instance, which represents a physical microphone on the computer. Subclass of\n",
    "    ``AudioSource``. This will throw an ``AttributeError`` if you don't have PyAudio 0.2.9 or later installed.\n",
    "    If ``device_index`` is unspecified or ``None``, the default microphone is used as the audio source. Otherwise,\n",
    "    ``device_index`` should be the index of the device to use for audio input. A device index is an integer between\n",
    "    0 and ``pyaudio.get_device_count() - 1`` (assume we have used ``import pyaudio`` beforehand) inclusive. It \n",
    "    represents an audio device such as a microphone or speaker. See the `PyAudio documentation <http://people.csail.mit.edu/hubert/pyaudio/docs/>`__ .\n",
    "    \n",
    "    The microphone audio is recorded in chunks of ``chunk_size`` and samples, at a rate of ``sample_rate`` samples\n",
    "    per second (Hertz). Higher ``sample_rate`` values result in better audio quality, but also more bandwidth \n",
    "    (and therefore, slower recognition). Additionally, some machines, such as some Raspberry Pi models, can't \n",
    "    keep up if this value is too high. Higher ``chunk_size`` values help avoid triggering on rapidly changing \n",
    "    ambient noise, but also makes detection less sensitive. This value, generally, should be left at its default.\n",
    "    \"\"\"\n",
    "    def __init__(self, device_index = None, sample_rate = 22050, chunk_size = 1024): #was sample_rate = 16000\n",
    "        # CONFIGURE MICROPHONE TO JACK. THEN PASS JACK INDEX AS DEVICE_INDEX\n",
    "        # DEFAULT SAMPLE S/B 22,050 OR 44,100\n",
    "        # set up PyAudio\n",
    "        self.pyaudio_module = self.get_pyaudio()\n",
    "\n",
    "        assert device_index is None or isinstance(device_index, int), \"Device index must be None or an integer\"\n",
    "        if device_index is not None: # ensure device index is in range\n",
    "            audio = self.pyaudio_module.PyAudio()\n",
    "            try:\n",
    "                count = audio.get_device_count() # obtain device count\n",
    "            except:\n",
    "                audio.terminate()\n",
    "                raise\n",
    "            assert 0 <= device_index < count, \"Device index out of range ({0} devices available; device index should be between 0 and {1} inclusive)\".format(count, count - 1)\n",
    "        assert isinstance(sample_rate, int) and sample_rate > 0, \"Sample rate must be a positive integer\"\n",
    "        assert isinstance(chunk_size, int) and chunk_size > 0, \"Chunk size must be a positive integer\"\n",
    "        self.device_index = device_index\n",
    "        self.format = self.pyaudio_module.paInt16 # 16-bit int sampling THIS SHOULD PROBABLY CHANGE TO PA.JACK\n",
    "        self.SAMPLE_WIDTH = self.pyaudio_module.get_sample_size(self.format) # size of each sample\n",
    "        print(\"microphone:init:SAMPLE_WIDTH:\",self.SAMPLE_WIDTH)\n",
    "        self.SAMPLE_RATE = sample_rate # sampling rate in Hertz\n",
    "        print(\"microphone:init:sample_rate:\",self.SAMPLE_RATE)\n",
    "        self.CHUNK = chunk_size # number of frames stored in each buffer\n",
    "        print(\"microphone:init:chunk_size:\",self.CHUNK)\n",
    "        self.audio = None\n",
    "        self.stream = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pyaudio():\n",
    "        \"\"\"\n",
    "        Imports the pyaudio module and checks its version. Throws exceptions if pyaudio can't be found or a wrong\n",
    "        version is installed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import pyaudio\n",
    "        except ImportError:\n",
    "            raise AttributeError(\"Could not find PyAudio; check installation\")\n",
    "        from distutils.version import LooseVersion\n",
    "        if LooseVersion(pyaudio.__version__) < LooseVersion(\"0.2.9\"):\n",
    "            raise AttributeError(\"PyAudio 0.2.9 or later is required (found version {0})\".format(pyaudio.__version__))\n",
    "        return pyaudio\n",
    "\n",
    "    @staticmethod\n",
    "    def list_microphone_names():\n",
    "        \"\"\"\n",
    "        Returns a list of the names of all available microphones. For microphones where the name can't be retrieved,\n",
    "        the list entry contains ``None`` instead. The index of each microphone's name is the same as its device index\n",
    "        when creating a ``Microphone`` instance - indices in this list can be used as values of ``device_index``.\n",
    "        \"\"\"\n",
    "        audio = Microphone.get_pyaudio().PyAudio()\n",
    "        try:\n",
    "            result = []\n",
    "            for i in range(audio.get_device_count()):\n",
    "                device_info = audio.get_device_info_by_index(i)\n",
    "                result.append(device_info.get(\"name\"))\n",
    "        finally:\n",
    "            audio.terminate()\n",
    "        return result\n",
    "\n",
    "    def __enter__(self):\n",
    "        assert self.stream is None, \"This audio source is already inside a context manager\"\n",
    "        self.audio = self.pyaudio_module.PyAudio()\n",
    "        try:\n",
    "            self.stream = Microphone.MicrophoneStream(\n",
    "                self.audio.open(\n",
    "                    input_device_index = self.device_index, channels = 1,\n",
    "                    format = self.format, rate = self.SAMPLE_RATE, frames_per_buffer = self.CHUNK,\n",
    "                    input = True, # stream is an input stream\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            self.audio.terminate()\n",
    "            raise\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        try:\n",
    "            self.stream.close()\n",
    "        finally:\n",
    "            self.stream = None\n",
    "            self.audio.terminate()\n",
    "\n",
    "    class MicrophoneStream(object):\n",
    "        def __init__(self, pyaudio_stream):\n",
    "            self.pyaudio_stream = pyaudio_stream\n",
    "\n",
    "        def read(self, size):\n",
    "            return self.pyaudio_stream.read(size, exception_on_overflow = False)\n",
    "\n",
    "        def close(self):\n",
    "            try:\n",
    "                # sometimes, if the stream isn't stopped, closing the stream throws an exception\n",
    "                if not self.pyaudio_stream.is_stopped():\n",
    "                    self.pyaudio_stream.stop_stream()\n",
    "            finally:\n",
    "                self.pyaudio_stream.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Process Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioFile(AudioSource):\n",
    "    \"\"\"\n",
    "    Creates a new ``AudioFile`` instance given a WAV/AIFF/FLAC audio file `filename_or_fileobject`. Subclass of\n",
    "    ``AudioSource``. If ``filename_or_fileobject`` is a string, then it is interpreted as a path to an audio file\n",
    "    on the filesystem. Otherwise, ``filename_or_fileobject`` should be a file-like object such as ``io.BytesIO`` \n",
    "    or similar.\n",
    "    \n",
    "    Note: io. is the core python toolset for working with streams https://docs.python.org/2/library/io.html.\n",
    "    \n",
    "    Note that functions that read from the audio (such as ``recognizer_instance.record`` or \n",
    "    ``recognizer_instance.listen``) will move ahead in the stream. For example, if you execute \n",
    "    ``recognizer_instance.record(audiofile_instance, duration=10)`` twice, the first time it will return the first\n",
    "    10 seconds of audio, and the second time it will return the 10 seconds of audio right after that. This is always\n",
    "    reset to the beginning when entering an ``AudioFile`` context.\n",
    "    \n",
    "    WAV files must be in PCM/LPCM format; WAVE_FORMAT_EXTENSIBLE and compressed WAV are not supported and may \n",
    "    result in undefined behaviour. Both AIFF and AIFF-C (compressed AIFF) formats are supported.\n",
    "    FLAC files must be in native FLAC format; OGG-FLAC is not supported and may result in undefined behaviour.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename_or_fileobject):\n",
    "        if str is bytes: # Python 2 - if a file path is specified, it must either be a `str` instance or a `unicode` instance\n",
    "            assert isinstance(filename_or_fileobject, (str, unicode)) or hasattr(filename_or_fileobject, \"read\"), \"Given audio file must be a filename string or a file-like object\"\n",
    "        else: # Python 3 - if a file path is specified, it must be a `str` instance\n",
    "            assert isinstance(filename_or_fileobject, str) or hasattr(filename_or_fileobject, \"read\"), \"Given audio file must be a filename string or a file-like object\"\n",
    "        self.filename_or_fileobject = filename_or_fileobject\n",
    "        self.stream = None\n",
    "        self.DURATION = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        assert self.stream is None, \"This audio source is already inside a context manager\"\n",
    "        try:\n",
    "            # attempt to read the file as WAV\n",
    "            self.audio_reader = wave.open(self.filename_or_fileobject, \"rb\")\n",
    "            self.little_endian = True \n",
    "            # RIFF WAV is a little-endian format (most ``audioop`` operations assume frames are stored in little-endian form)\n",
    "        except wave.Error:\n",
    "            try:\n",
    "                # attempt to read the file as AIFF\n",
    "                self.audio_reader = aifc.open(self.filename_or_fileobject, \"rb\")\n",
    "                self.little_endian = False # AIFF is a big-endian format\n",
    "            except aifc.Error:\n",
    "                # attempt to read the file as FLAC\n",
    "                if hasattr(self.filename_or_fileobject, \"read\"):\n",
    "                    flac_data = self.filename_or_fileobject.read()\n",
    "                else:\n",
    "                    with open(self.filename_or_fileobject, \"rb\") as f: flac_data = f.read()\n",
    "\n",
    "                # run the FLAC converter with the FLAC data to get the AIFF data\n",
    "                flac_converter = get_flac_converter()\n",
    "                process = subprocess.Popen([\n",
    "                    flac_converter,\n",
    "                    \"--stdout\", \"--totally-silent\", # put the resulting AIFF file in stdout, and make sure it's not mixed with any program output\n",
    "                    \"--decode\", \"--force-aiff-format\", # decode the FLAC file into an AIFF file\n",
    "                    \"-\", # the input FLAC file contents will be given in stdin\n",
    "                ], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "                aiff_data, stderr = process.communicate(flac_data)\n",
    "                aiff_file = io.BytesIO(aiff_data)\n",
    "                try:\n",
    "                    self.audio_reader = aifc.open(aiff_file, \"rb\")\n",
    "                except aifc.Error:\n",
    "                    assert False, \"Audio file could not be read as WAV, AIFF, or FLAC; check if file is corrupted\"\n",
    "                self.little_endian = False # AIFF is a big-endian format\n",
    "        assert 1 <= self.audio_reader.getnchannels() <= 2, \"Audio must be mono or stereo\"\n",
    "        self.SAMPLE_WIDTH = self.audio_reader.getsampwidth()\n",
    "\n",
    "        # 24-bit audio needs some special handling for old Python versions (workaround for https://bugs.python.org/issue12866)\n",
    "        samples_24_bit_pretending_to_be_32_bit = False\n",
    "        if self.SAMPLE_WIDTH == 3: # 24-bit audio\n",
    "            try: audioop.bias(b\"\", self.SAMPLE_WIDTH, 0) # test whether this sample width is supported (for example, ``audioop`` in Python 3.3 and below don't support sample width 3, while Python 3.4+ do)\n",
    "            except audioop.error: # this version of audioop doesn't support 24-bit audio (probably Python 3.3 or less)\n",
    "                samples_24_bit_pretending_to_be_32_bit = True # while the ``AudioFile`` instance will outwardly appear to be 32-bit, it will actually internally be 24-bit\n",
    "                self.SAMPLE_WIDTH = 4 # the ``AudioFile`` instance should present itself as a 32-bit stream now, since we'll be converting into 32-bit on the fly when reading\n",
    "\n",
    "        self.SAMPLE_RATE = self.audio_reader.getframerate()\n",
    "        self.CHUNK = 4096\n",
    "        self.FRAME_COUNT = self.audio_reader.getnframes()\n",
    "        self.DURATION = self.FRAME_COUNT / float(self.SAMPLE_RATE)\n",
    "        self.stream = AudioFile.AudioFileStream(self.audio_reader, self.little_endian, samples_24_bit_pretending_to_be_32_bit)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if not hasattr(self.filename_or_fileobject, \"read\"): # only close the file if it was opened by this class in the first place (if the file was originally given as a path)\n",
    "            self.audio_reader.close()\n",
    "        self.stream = None\n",
    "        self.DURATION = None\n",
    "\n",
    "    class AudioFileStream(object):\n",
    "        def __init__(self, audio_reader, little_endian, samples_24_bit_pretending_to_be_32_bit):\n",
    "            self.audio_reader = audio_reader # an audio file object (e.g., a `wave.Wave_read` instance)\n",
    "            self.little_endian = little_endian # whether the audio data is little-endian (when working with big-endian things, we'll have to convert it to little-endian before we process it)\n",
    "            self.samples_24_bit_pretending_to_be_32_bit = samples_24_bit_pretending_to_be_32_bit # this is true if the audio is 24-bit audio, but 24-bit audio isn't supported, so we have to pretend that this is 32-bit audio and convert it on the fly\n",
    "\n",
    "        def read(self, size = -1):\n",
    "            buffer = self.audio_reader.readframes(self.audio_reader.getnframes() if size == -1 else size)\n",
    "            if not isinstance(buffer, bytes): buffer = b\"\" # workaround for https://bugs.python.org/issue24608\n",
    "\n",
    "            sample_width = self.audio_reader.getsampwidth()\n",
    "            if not self.little_endian: # big endian format, convert to little endian on the fly\n",
    "                if hasattr(audioop, \"byteswap\"): # ``audioop.byteswap`` was only added in Python 3.4 (incidentally, that also means that we don't need to worry about 24-bit audio being unsupported, since Python 3.4+ always has that functionality)\n",
    "                    buffer = audioop.byteswap(buffer, sample_width)\n",
    "                else: # manually reverse the bytes of each sample, which is slower but works well enough as a fallback\n",
    "                    buffer = buffer[sample_width - 1::-1] + b\"\".join(buffer[i + sample_width:i:-1] for i in range(sample_width - 1, len(buffer), sample_width))\n",
    "\n",
    "            # workaround for https://bugs.python.org/issue12866\n",
    "            if self.samples_24_bit_pretending_to_be_32_bit: # we need to convert samples from 24-bit to 32-bit before we can process them with ``audioop`` functions\n",
    "                buffer = b\"\".join(\"\\x00\" + buffer[i:i + sample_width] for i in range(0, len(buffer), sample_width)) # since we're in little endian, we prepend a zero byte to each 24-bit sample to get a 32-bit sample\n",
    "            if self.audio_reader.getnchannels() != 1: # stereo audio\n",
    "                buffer = audioop.tomono(buffer, sample_width, 1, 1) # convert stereo audio data to mono\n",
    "            return buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Extract Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioData(object):\n",
    "    def __init__(self, frame_data, sample_rate, sample_width):\n",
    "        assert sample_rate > 0, \"Sample rate must be a positive integer\"\n",
    "        assert sample_width % 1 == 0 and 1 <= sample_width <= 4, \"Sample width must be between 1 and 4 inclusive\"\n",
    "        self.frame_data = frame_data\n",
    "        self.sample_rate = sample_rate\n",
    "        self.sample_width = int(sample_width)\n",
    "\n",
    "    def get_raw_data(self, convert_rate = None, convert_width = None):\n",
    "        \"\"\"\n",
    "        Returns a byte string representing the raw frame data for the audio represented by the ``AudioData`` instance.\n",
    "        \n",
    "        If ``convert_rate`` is specified and the audio sample rate is not ``convert_rate`` Hz, the resulting audio \n",
    "        is resampled to match.\n",
    "        \n",
    "        If ``convert_width`` is specified and the audio samples are not ``convert_width`` bytes each, the resulting \n",
    "        audio is converted to match. Writing these bytes directly to a file results in a valid `RAW/PCM audio file\n",
    "        <https://en.wikipedia.org/wiki/Raw_audio_format>`__.\n",
    "        \"\"\"\n",
    "        assert convert_rate is None or convert_rate > 0, \"Sample rate to convert to must be a positive integer\"\n",
    "        assert convert_width is None or (convert_width % 1 == 0 and 1 <= convert_width <= 4), \"Sample width to convert to must be between 1 and 4 inclusive\"\n",
    "\n",
    "        raw_data = self.frame_data\n",
    "\n",
    "        # make sure unsigned 8-bit audio (which uses unsigned samples) is handled like higher sample width audio (which uses signed samples)\n",
    "        if self.sample_width == 1:\n",
    "            raw_data = audioop.bias(raw_data, 1, -128) # subtract 128 from every sample to make them act like signed samples\n",
    "\n",
    "        # resample audio at the desired rate if specified\n",
    "        if convert_rate is not None and self.sample_rate != convert_rate:\n",
    "            raw_data, _ = audioop.ratecv(raw_data, self.sample_width, 1, self.sample_rate, convert_rate, None)\n",
    "\n",
    "        # convert samples to desired sample width if specified\n",
    "        if convert_width is not None and self.sample_width != convert_width:\n",
    "            if convert_width == 3: # we're converting the audio into 24-bit (workaround for https://bugs.python.org/issue12866)\n",
    "                raw_data = audioop.lin2lin(raw_data, self.sample_width, 4) # convert audio into 32-bit first, which is always supported\n",
    "                try: audioop.bias(b\"\", 3, 0) # test whether 24-bit audio is supported (for example, ``audioop`` in Python 3.3 and below don't support sample width 3, while Python 3.4+ do)\n",
    "                except audioop.error: # this version of audioop doesn't support 24-bit audio (probably Python 3.3 or less)\n",
    "                    raw_data = b\"\".join(raw_data[i + 1:i + 4] for i in range(0, len(raw_data), 4)) # since we're in little endian, we discard the first byte from each 32-bit sample to get a 24-bit sample\n",
    "                else: # 24-bit audio fully supported, we don't need to shim anything\n",
    "                    raw_data = audioop.lin2lin(raw_data, self.sample_width, convert_width)\n",
    "            else:\n",
    "                raw_data = audioop.lin2lin(raw_data, self.sample_width, convert_width)\n",
    "\n",
    "        # if the output is 8-bit audio with unsigned samples, convert the samples we've been treating as signed to unsigned again\n",
    "        if convert_width == 1:\n",
    "            raw_data = audioop.bias(raw_data, 1, 128) # add 128 to every sample to make them act like unsigned samples again\n",
    "\n",
    "        return raw_data\n",
    "\n",
    "    def get_wav_data(self, convert_rate = None, convert_width = None):\n",
    "        \"\"\"\n",
    "        Returns a byte string representing the contents of a WAV file containing the audio represented by the \n",
    "        ``AudioData`` instance. If ``convert_width`` is specified and the audio samples are not ``convert_width`` \n",
    "        bytes each, the resulting audio is converted to match. If ``convert_rate`` is specified and the audio sample\n",
    "        rate is not ``convert_rate`` Hz, the resulting audio is resampled to match. Writing these bytes directly to \n",
    "        a file results in a valid `WAV file <https://en.wikipedia.org/wiki/WAV>`__.\n",
    "        \"\"\"\n",
    "        raw_data = self.get_raw_data(convert_rate, convert_width)\n",
    "        sample_rate = self.sample_rate if convert_rate is None else convert_rate\n",
    "        sample_width = self.sample_width if convert_width is None else convert_width\n",
    "                \n",
    "        # generate the WAV file contents\n",
    "        with io.BytesIO() as wav_file:\n",
    "            wav_writer = wave.open(wav_file, \"wb\")\n",
    "            try: # note that we can't use context manager, since that was only added in Python 3.4\n",
    "                wav_writer.setframerate(sample_rate)\n",
    "                wav_writer.setsampwidth(sample_width)\n",
    "                wav_writer.setnchannels(1)\n",
    "                wav_writer.writeframes(raw_data)       \n",
    "                wav_data = wav_file.getvalue()\n",
    "            finally:  # make sure resources are cleaned up\n",
    "                wav_writer.close()\n",
    "                \n",
    "            \n",
    "                        \n",
    "        return wav_data\n",
    "    \n",
    "    def get_aiff_data(self, convert_rate = None, convert_width = None):\n",
    "        \"\"\"\n",
    "        Returns a byte string representing the contents of an AIFF-C file containing the audio represented by \n",
    "        the ``AudioData`` instance. If ``convert_width`` is specified and the audio samples are not ``convert_width``\n",
    "        bytes each, the resulting audio is converted to match. If ``convert_rate`` is specified and the audio sample\n",
    "        rate is not ``convert_rate`` Hz, the resulting audio is resampled to match. Writing these bytes directly to\n",
    "        a file results in a valid `AIFF-C file <https://en.wikipedia.org/wiki/Audio_Interchange_File_Format>`__.\n",
    "        \"\"\"\n",
    "        raw_data = self.get_raw_data(convert_rate, convert_width)\n",
    "        sample_rate = self.sample_rate if convert_rate is None else convert_rate\n",
    "        sample_width = self.sample_width if convert_width is None else convert_width\n",
    "\n",
    "        # the AIFF format is big-endian, so we need to covnert the little-endian raw data to big-endian\n",
    "        if hasattr(audioop, \"byteswap\"): # ``audioop.byteswap`` was only added in Python 3.4\n",
    "            raw_data = audioop.byteswap(raw_data, sample_width)\n",
    "        else: # manually reverse the bytes of each sample, which is slower but works well enough as a fallback\n",
    "            raw_data = raw_data[sample_width - 1::-1] + b\"\".join(raw_data[i + sample_width:i:-1] for i in range(sample_width - 1, len(raw_data), sample_width))\n",
    "\n",
    "        # generate the AIFF-C file contents\n",
    "        with io.BytesIO() as aiff_file:\n",
    "            aiff_writer = aifc.open(aiff_file, \"wb\")\n",
    "            try: # note that we can't use context manager, since that was only added in Python 3.4\n",
    "                aiff_writer.setframerate(sample_rate)\n",
    "                aiff_writer.setsampwidth(sample_width)\n",
    "                aiff_writer.setnchannels(1)\n",
    "                aiff_writer.writeframes(raw_data)\n",
    "                aiff_data = aiff_file.getvalue()\n",
    "            finally:  # make sure resources are cleaned up\n",
    "                aiff_writer.close()\n",
    "        return aiff_data\n",
    "\n",
    "    def get_flac_data(self, convert_rate = None, convert_width = None):\n",
    "        \"\"\"\n",
    "        Returns a byte string representing the contents of a FLAC file containing the audio represented by the \n",
    "        ``AudioData`` instance. Note that 32-bit FLAC is not supported. If the audio data is 32-bit and \n",
    "        ``convert_width`` is not specified, then the resulting FLAC will be a 24-bit FLAC. If ``convert_rate`` \n",
    "        is specified and the audio sample rate is not ``convert_rate`` Hz, the resulting audio is resampled to match.\n",
    "        If ``convert_width`` is specified and the audio samples are not ``convert_width`` bytes each, the resulting \n",
    "        audio is converted to match. Writing these bytes directly to a file results in a valid `FLAC file \n",
    "        <https://en.wikipedia.org/wiki/FLAC>`__.\n",
    "        \"\"\"\n",
    "        assert convert_width is None or (convert_width % 1 == 0 and 1 <= convert_width <= 3), \"Sample width to convert to must be between 1 and 3 inclusive\"\n",
    "\n",
    "        if self.sample_width > 3 and convert_width is None: # resulting WAV data would be 32-bit, which is not convertable to FLAC using our encoder\n",
    "            convert_width = 3 # the largest supported sample width is 24-bit, so we'll limit the sample width to that\n",
    "\n",
    "        # run the FLAC converter with the WAV data to get the FLAC data\n",
    "        wav_data = self.get_wav_data(convert_rate, convert_width)\n",
    "        flac_converter = get_flac_converter()\n",
    "        process = subprocess.Popen([\n",
    "            flac_converter,\n",
    "            \"--stdout\", \"--totally-silent\", # put the resulting FLAC file in stdout, and make sure it's not mixed with any program output\n",
    "            \"--best\", # highest level of compression available\n",
    "            \"-\", # the input FLAC file contents will be given in stdin\n",
    "        ], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        flac_data, stderr = process.communicate(wav_data)\n",
    "        return flac_data       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Out To MuseScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Register Jack Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jack\n",
    "import struct\n",
    "import sys\n",
    "import signal\n",
    "import os\n",
    "import threading\n",
    "import binascii\n",
    "import time\n",
    "\n",
    "'''Client object is a context manager. Can be used in a with statement to activate() in the beginning and \n",
    "deactivate() and close() at end. parms: name (str, desired client name), use_exact_name (bool, raise error if name\n",
    "is not unique), no_start_server (bool, don't start JACK server when it is not already running), servername (str, \n",
    "server names are unique to each user, if unspecified, use \"default\" unless JACK_DEFAULT_SERVER is defined in the \n",
    "process environment), session_id (str, pass SessionID Token allowing sessionmanager to identify client again. 4 \n",
    "types of client ports: Client.inports, Client.outports, Client.midi_inports and Client.midi_outports. \n",
    "https://jackclient-python.readthedocs.io/en/0.4.1/#jack.Port'''\n",
    "\n",
    "@jack.set_error_function\n",
    "def error(msg):\n",
    "    print(\"Error:\", msg)\n",
    "\n",
    "#@jack.set_info_function\n",
    "#def info(msg):\n",
    "#    print(\"Info:\", msg)\n",
    "\n",
    "print(\"starting client\")\n",
    "client = jack.Client(\"music_client\") # client is object name used in code. 'music_client' is port name in Jack server\n",
    "\n",
    "if client.status.server_started:\n",
    "    print(\"JACK server was started\")\n",
    "else:\n",
    "    print(\"JACK server was already running\")\n",
    "if client.status.name_not_unique:\n",
    "    print(\"unique client name generated:\", client.name)\n",
    "\n",
    "'''register ports'''\n",
    "# create two audio in and audio out ports\n",
    "#for number in 1, 2: # Jack Audio recognizes input and output placing them in server as appropriate\n",
    "#    client.inports.register(\"input_{0}\".format(number))\n",
    "#    client.outports.register(\"output_{0}\".format(number))\n",
    "\n",
    "# create two midi ports\n",
    "inPort = client.midi_inports.register(\"midi_in\")\n",
    "outPort = client.midi_outports.register(\"midi_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Register Ports on Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# listing ports by kind\n",
    "#print(client.inports); print(client.outports)\n",
    "#print(client.midi_inports); print(client.midi_outports)\n",
    "# list ports by characteristic\n",
    "#print(client.get_port_by_name('music_client:input_1')) # port is referred to as client:port_name\n",
    "#print(client.get_ports(is_audio=True, is_output=True, is_physical=True))\n",
    "#print(client.get_ports(\"mus.*1$\")) # You can even use regular expressions to search for ports:\n",
    "# list connections for a port\n",
    "#print(client.get_all_connections('music_client:input_1')) # connections not for a client, but port on client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Connect Ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# client management\n",
    "\n",
    "# Tell the JACK server that the program is ready to start processing audio.\n",
    "client.activate() \n",
    "\n",
    "# Establish a connection between two ports. When connection exists, data written to source port is available\n",
    "# to destination port. Audio ports can not be connected with MIDI ports.\n",
    "#client.connect(\"system:capture_1\", \"music_client:input_1\")\n",
    "#client.connect(\"music_client:output_1\", \"system:playback_1\")\n",
    "#client.connect(\"system:capture_2\", \"music_client:input_2\")\n",
    "#client.connect(\"music_client:output_2\", \"system:playback_2\") \n",
    "\n",
    "#client.connect(outPort, 'mscore:mscore-midiin-1')\n",
    "client.connect('mscore:mscore-midi-1',inPort)\n",
    "#client.transport_start()# Start JACK transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.deactivate(ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Client Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_client_status(inClient):\n",
    "    print(\"Settings:\")\n",
    "    # client settings\n",
    "    print(\"name:\", inClient.name)\n",
    "    print(\"port list:\") # gets all ports for client (instance of class)\n",
    "    print(inClient.get_ports())\n",
    "    print(\"sample rate:\", inClient.samplerate) # sample rate of the JACK system (read-only).\n",
    "    print(\"block size:\", inClient.blocksize) \n",
    "    ''' Must be a power of 2. Max size ever passed. Query before activate(). Size may change. Clients that depend\n",
    "    on it must register a callback with set_blocksize_callback() so they will be notified if it does. Changing \n",
    "    blocksize stops JACK engine process, then calls all registered callback functions before restarting.'''\n",
    "    print\n",
    "    print(\"Client Status:\")\n",
    "    print(\"status:\", inClient.status)\n",
    "    print(\"realtime:\", inClient.realtime)\n",
    "    print(\"frames since:\", inClient.frames_since_cycle_start)\n",
    "    print(\"frame time:\", inClient.frame_time) \n",
    "    '''Estimated current time in frames. Used in other threads (not the process callback). Return value can be\n",
    "    compared with the value of last_frame_time to relate time in other threads to JACK time.'''\n",
    "    print(\"last frame time:\", inClient.last_frame_time) \n",
    "    print(\"cpu load:\", inClient.cpu_load()) \n",
    "    print\n",
    "    print(\"Transport Status:\")\n",
    "    print(\"trans state:\", inClient.transport_state)\n",
    "    print(\"trans frame:\", inClient.transport_frame) # Returns estimate of the current transport frame. Assign frame number repositions the JACK transport.\n",
    "    print(\"trans query:\", inClient.transport_query()) # Query the current transport state and position. \n",
    "    print(\"trans struct:\", inClient.transport_query_struct()) # Query the current transport state and position. Realtime-safe.\n",
    "    #client.transport_reposition_struct(position)\n",
    "    \n",
    "get_client_status(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NOTEON = 0x90\n",
    "#NOTEOFF = 0x80\n",
    "#def make_midi_note(onOff, pitch, vel):\n",
    "#    outNote = struct.pack('3B', onOff, pitch, vel)\n",
    "#    return(outNote)\n",
    "#toMS = make_midi_note(NOTEON,60,80)\n",
    "\n",
    "event = threading.Event()\n",
    "\n",
    "offSet=[]; midDat=[]\n",
    "\n",
    "@client.set_process_callback\n",
    "def process(frames):\n",
    "    for offset, datum in inPort.incoming_midi_events():\n",
    "        offSet.append(offset)\n",
    "        midDat.append(datum)\n",
    "        print(\"{0}: 0x{1}\".format(client.last_frame_time + offset, #\n",
    "                                  binascii.hexlify(datum).decode()))\n",
    "                    \n",
    "@client.set_shutdown_callback\n",
    "def shutdown(status, reason):\n",
    "    print(\"JACK shutdown!\")\n",
    "    print(\"status:\", status)\n",
    "    print(\"reason:\", reason)\n",
    "    event.set()\n",
    "\n",
    "    #assert len(client.inports) == len(client.outports)\n",
    "    #assert frames == client.blocksize\n",
    "    '''direct transfer one buffer to the other'''\n",
    "    #for i, o in zip(client.inports, client.outports):\n",
    "    #    o.get_buffer()[:] = i.get_buffer()\n",
    "    #outPort.clear_buffer()\n",
    "    #for offset, indata in inport.incoming_midi_events():\n",
    "        # Note: This may raise an exception:\n",
    "        #outport.write_midi_event(offset, indata)  # pass through\n",
    "    #print(inPort.get_buffer()[:])\n",
    "    #print(\"b4 writing event\")\n",
    "    #outPort.write_midi_event(0, toMS)\n",
    "    #print(\"aftr writing event\")\n",
    "    #if len(indata) == 3:\n",
    "    #        status, pitch, vel = struct.unpack('3B', indata)\n",
    "    #        if status >> 4 in (NOTEON, NOTEOFF):\n",
    "    #            for i in INTERVALS:\n",
    "    #                # Note: This may raise an exception:\n",
    "    #                outport.write_midi_event(offset, (status, pitch + i, vel))\n",
    "\n",
    "ctr = 0\n",
    "with client:\n",
    "    client.connect('mscore:mscore-midi-1',inPort)\n",
    "    \n",
    "    print(\"#\" * 80)\n",
    "    print(\"kill kernel to quit\")\n",
    "    print(\"#\" * 80)\n",
    "    input()\n",
    "    \n",
    "    #time.sleep(1)\n",
    "    #ctr = ctr + 1\n",
    "    #if ctr == 5:\n",
    "    #    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import music21 as m21\n",
    "import pygame as pg\n",
    "import cStringIO\n",
    "\n",
    "keyDetune = []\n",
    "for i in range(0, 127):\n",
    "    keyDetune.append(i) # random.randint(-30, 30)\n",
    "\n",
    "b = m21.corpus.parse('bwv66.6')\n",
    "\n",
    "for n in b.flat.notes:\n",
    "    n.microtone = keyDetune[n.midi]\n",
    "    \n",
    "sp = m21.midi.realtime.StreamPlayer(b)\n",
    "\n",
    "@client.set_process_callback\n",
    "def process(frames):\n",
    "    print('in process callback')\n",
    "    outPort.clear_buffer()\n",
    "    outPort.write_midi_event(sp.play(busyFunction=None, busyArgs=None, \n",
    "                                     endFunction=None, endArgs=None, \n",
    "                                     busyWaitMilliseconds=100))\n",
    "        \n",
    "        #outPort.write_midi_event(offSet[15+i], midDat[15+i]) #, \n",
    "        #print(\"{0}: 0x{1}\".format(offSet[15+i], # client.last_frame_time + \n",
    "        #                          binascii.hexlify(midDat[15+i]).decode()))\n",
    "    time.sleep(3)\n",
    "        \n",
    "    #print(\"sleep1\")\n",
    "    #time.sleep(5)\n",
    "    #outPort.write_midi_event(offSet[16], midDat[16])\n",
    "    #print(\"sleep2\")\n",
    "    #time.sleep(5)\n",
    "    #outPort.write_midi_event(offSet[17], midDat[17])\n",
    "                    \n",
    "@client.set_shutdown_callback\n",
    "def shutdown(status, reason):\n",
    "    print(\"JACK shutdown!\")\n",
    "    print(\"status:\", status)\n",
    "    print(\"reason:\", reason)\n",
    "    event.set()\n",
    "\n",
    "with client:\n",
    "    print('connecting port')\n",
    "    client.connect(outPort, 'mscore:mscore-midiin-1')\n",
    "    \n",
    "    print(\"#\" * 80)\n",
    "    print(\"kill kernel to quit\")\n",
    "    print(\"#\" * 80)\n",
    "    input()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(offSet)\n",
    "print(len(offSet))\n",
    "print(len(midDat))\n",
    "#for i in range(len(midDat)):\n",
    "#    print(\"0x{0}\".format(binascii.hexlify(midDat[i]).decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Deactivate Ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client.inports.transport_stop() # Stop JACK transport.\n",
    "client.outports.transport_stop()\n",
    "\n",
    "client.outports.clear()  # unregister all audio output ports\n",
    "# or client.disconnect(source, destination) to remove individual connections between two ports.\n",
    "\n",
    "client.deactivate(ignore_errors=True) # Tell the JACK server to remove self from the process graph. \n",
    "# Also, disconnect all ports belonging to it, since inactive clients have no port connections.\n",
    "\n",
    "client.close(ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m21 to midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''this one plays chords, but for multi-instrument sounds, you'll need to import \n",
    "MuseScore logic, use MusicPlayer in headless mode, etc.\n",
    "\n",
    "note: jack router hijacks system sound when enabled. to play output in presence of jack, \n",
    "you'll need to create client and route output to speakers/headphone'''\n",
    "\n",
    "def play_m21(inM21Stream):\n",
    "    \n",
    "    import random\n",
    "    import music21 as m21\n",
    "    import pygame as pg\n",
    "    import cStringIO\n",
    "\n",
    "    sCadence_mf = m21.midi.translate.streamToMidiFile(inM21Stream.flat)\n",
    "    sCadence_mStr = sCadence_mf.writestr()\n",
    "    sCadence_mStrFile = cStringIO.StringIO(sCadence_mStr)\n",
    "\n",
    "    freq = 44100    # audio CD quality\n",
    "    bitsize = -16   # unsigned 16 bit\n",
    "    channels = 2    # 1 is mono, 2 is stereo\n",
    "    buffer = 1024    # number of samples\n",
    "    pg.mixer.init(freq, bitsize, channels, buffer)\n",
    "\n",
    "    # optional volume 0 to 1.0\n",
    "    pg.mixer.music.set_volume(0.8)\n",
    "\n",
    "    def play_music(music_file):\n",
    "        \"\"\"\n",
    "        stream music with mixer.music module in blocking manner\n",
    "        this will stream the sound from disk while playing\n",
    "        \"\"\"\n",
    "        clock = pg.time.Clock()\n",
    "    \n",
    "        try:\n",
    "            pg.mixer.music.load(music_file)\n",
    "            #print \"Music file %s loaded!\" % music_file\n",
    "        except pg.error:\n",
    "            print \"File %s not found! (%s)\" % (music_file, pg.get_error())\n",
    "            return\n",
    "        pg.mixer.music.play()\n",
    "        while pg.mixer.music.get_busy():\n",
    "            # check if playback has finished\n",
    "            clock.tick(30)\n",
    "\n",
    "    # play the midi file we just saved\n",
    "    play_music(sCadence_mStrFile)\n",
    "\n",
    "\n",
    "#fname = 'data/maps/AkPnBcht/UCHO/I60-68/C0-3-7-8/MAPS_UCHO_C0-3-7-8_I60-68_S0_n4_AkPnBcht.mid'\n",
    "#fname='data/wip/tmp/canon.mid'\n",
    "#mf = m21.midi.MidiFile()\n",
    "#mf.open(fname)\n",
    "#mf.read()\n",
    "#mf.close()\n",
    "    \n",
    "#strm = m21.midi.translate.midiFileToStream(mf)\n",
    "#play_m21(strm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment\n",
    "\n",
    "Dividing up a score into small, possibly overlapping sections. \n",
    "Used for controlling flow and for searching across pieces for similarity.\n",
    "http://web.mit.edu/music21/doc/moduleReference/moduleSearchSegment.html\n",
    "\n",
    "lower level note to string translations are handled here: http://web.mit.edu/music21/doc/moduleReference/moduleSearchBase.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check to see what is installled\n",
    "m21.search.segment.getDifflibOrPyLev(seq2=None, junk=None, forceDifflib=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''indexScoreFilePaths(): Returns a dictionary of the lists from indexScoreParts for \n",
    "each score in scoreFilePaths'''\n",
    "\n",
    "searchResults = m21.corpus.search('bwv19')\n",
    "fpsNamesOnly = sorted([searchResult.sourcePath for searchResult in searchResults])\n",
    "print(fpsNamesOnly)\n",
    "print\n",
    "\n",
    "scoreDict = m21.search.segment.indexScoreFilePaths(fpsNamesOnly[2:5])\n",
    "print(len(scoreDict['bwv190.7.mxl']))\n",
    "print\n",
    "\n",
    "print(scoreDict['bwv190.7.mxl'][0]['measureList'])\n",
    "print \n",
    "\n",
    "print(scoreDict['bwv190.7.mxl'][0]['segmentList'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoreDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search\n",
    "\n",
    "two objectives:\n",
    "1. subset score into bars and measures for user training, output w/out losing header info\n",
    "2. create lookup (dict) w/ cached corpus fingerprints for quick song recognition (by notes or names). return a list of matches w/ probs. accept verbal selection of 1, 2, 3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Levenshtein # hmmm... doesn't seem to recognize it\n",
    "tmp = m21.search.segment.getDifflibOrPyLev()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "luca = m21.corpus.parse('luca/gloria')\n",
    "#luca.show('text')\n",
    "#print\n",
    "lucaCantus = luca.parts[0]\n",
    "\n",
    "segments, measureLists = m21.search.segment.translateMonophonicPartToSegments(lucaCantus)\n",
    "print(segments[0:2])\n",
    "print(measureLists[0:2])\n",
    "print\n",
    "\n",
    "segments, measureLists = m21.search.segment.translateMonophonicPartToSegments(lucaCantus, algorithm=m21.search.translateDiatonicStreamToString)\n",
    "print(segments[0:2])\n",
    "print(measureLists[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similarity\n",
    "\n",
    "for faster search pip install Levenshtein Python C extension https://pypi.python.org/pypi/python-Levenshtein\n",
    "\n",
    "lower level functions from (http://web.mit.edu/music21/doc/moduleReference/moduleSearchBase.html), used in search below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''receives m21 stream objects for the segment played and the segment standard. compares two and returns\n",
    "similarity score. sourced from m21: http://web.mit.edu/music21/doc/moduleReference/moduleSearchBase.html'''\n",
    "\n",
    "import music21 as m21\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# note: include in global options: 1. level of similarity, 2. show comparisons, 3. play comparisons\n",
    "# enh: play played and standard wavs vs. these midi interpretations\n",
    "\n",
    "#class score_similarity():\n",
    "#    def __init__(self, played, standard, level, playFl=False, showFl=False):\n",
    "#        #assert len(played) = len(standard), \"Played and standard must be same length\"\n",
    "#        self.played = played\n",
    "#        self.standard = standard\n",
    "#        self.level = level\n",
    "#        self.playFl = playFl\n",
    "#        self.showFl = showFl\n",
    "\n",
    "def score_similarity(standard, played, level=1, playFl=False, showFl=False):\n",
    "    \n",
    "    # need to ask what's easiest rythm or notes\n",
    "    levels = []; scores = []; text = []\n",
    "    \n",
    "    if ((level == 5) or (level == 1)): # did you get the notes right?\n",
    "        levels.append(1)\n",
    "        scores.append(m21.search.approximateNoteSearchNoRhythm(standard, played)[0].matchProbability)\n",
    "        text.append(\"notes no rythm:\")\n",
    "        \n",
    "    if ((level == 5) or (level == 2)): # did you get the notes and / or rythm?\n",
    "        levels.append(2)\n",
    "        scores.append(m21.search.approximateNoteSearchWeighted(standard, played)[0].matchProbability)\n",
    "        text.append(\"weighted notes and rythm:\")\n",
    "        \n",
    "    if ((level == 5) or (level == 3)): # trading off note and rythm accuracy\n",
    "        levels.append(3)\n",
    "        scores.append(m21.search.approximateNoteSearch(standard, played)[0].matchProbability)\n",
    "        text.append(\"absolute notes and rythm:\")\n",
    "            \n",
    "    if ((level == 5) or (level == 4)): # focusing just on rythm? \n",
    "        levels.append(4)\n",
    "        scores.append(m21.search.approximateNoteSearchOnlyRhythm(standard, played)[0].matchProbability)\n",
    "        text.append(\"rythm only:\")\n",
    "        \n",
    "    else:\n",
    "        return(\"invalid level. levels are 1-5 inclusive where 5 is all\")\n",
    "        \n",
    "    if playFl == True:\n",
    "        pp = m21.midi.realtime.StreamPlayer(played)\n",
    "        sp = m21.midi.realtime.StreamPlayer(standard)\n",
    "        pp.play()\n",
    "        time.sleep(1)\n",
    "        sp.play()\n",
    "            \n",
    "    if showFl == True:\n",
    "        standard.show() # need to figure out how to get these to print. perhaps one over the other.\n",
    "        played.show()\n",
    "\n",
    "    return(pd.DataFrame(zip(levels,text,scores),columns=['level', 'score basis', 'score']))\n",
    "  \n",
    "#rslt = score_similarity(standard=s, played=o4, level=5, playFl=False, showFl=False)\n",
    "#print(rslt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display\n",
    "\n",
    "in hibernation pending fix to jsonpickleJS such that vexflow works in music21:\n",
    "https://github.com/cuthbertLab/music21j/issues/13\n",
    "\n",
    "http://web.mit.edu/music21/doc/moduleReference/moduleVexflowToMusic21j.html\n",
    "\n",
    "https://github.com/cuthbertLab/music21j\n",
    "\n",
    "https://github.com/cuthbertLab/music21/blob/master/music21/vexflow/toMusic21j.py\n",
    "\n",
    "https://github.com/0xfe/vexflow/tree/master/src\n",
    "\n",
    "http://web.mit.edu/music21/doc/moduleReference/moduleConverterSubConverters.html#music21.converter.subConverters.ConverterIPython\n",
    "\n",
    "https://iacchus.github.io/2016/08/03/More-Stuff-Implementing-html-ipython-notebook-in-Hub-Press.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''m21 had commented out vfshow def in ConverterIPython below. Comment indicates alpha \n",
    "\"doesn't work too well. to get that code included here, I created a local file \n",
    "m21SubConverterClass.py which defines the SubConverter. Characteristics of that SubConverter\n",
    "are inherited when creating an instance of class ConverterIPython (below). now that version\n",
    "has to import from the music21 vexflow directory the \"toMusic21j\" files (init.py, \n",
    "toMusic21j.py). Because that's looking in a directory, it has the full music21 prefix.\n",
    "\n",
    "Then in the <script> section we're looking for a music21.jsonPickle.Converter(). That appears\n",
    "to be, as of oct '15 the source of a bug (https://github.com/cuthbertLab/music21j/issues/13)\n",
    "which they're working as of 3 Jul '16. However, it looks like there's a dependency on \n",
    "jsonpickleJS, which is the source of the problem. So, keep this code, but punt for now.'''\n",
    "\n",
    "import m21SubConverterClass\n",
    "import music21 as m21\n",
    "\n",
    "class ConverterIPython(m21SubConverterClass.SubConverter):\n",
    "    '''\n",
    "    Meta-subconverter for displaying image data etc. in iPython\n",
    "    using either png (via MuseScore or LilyPond) or directly via\n",
    "    Vexflow/music21j\n",
    "    '''\n",
    "    \n",
    "    registerFormats = ('ipython',)\n",
    "    registerOutputExtensions = ()\n",
    "    registerOutputSubformatExtensions = {'lilypond': 'ly'}\n",
    "    \n",
    "    def vfshow(self, s):\n",
    "        '''pickle this object and send it to Vexflow\n",
    "        Alpha -- does not work too well.'''\n",
    "        import random\n",
    "        from music21.vexflow import toMusic21j\n",
    "        from IPython.display import HTML # @UnresolvedImport\n",
    "        vfp = toMusic21j.VexflowPickler()\n",
    "        \n",
    "        vfp.mode = 'jsonSplit'\n",
    "        outputCode = vfp.fromObject(s)\n",
    "        idName = 'canvasDiv' + str(random.randint(0, 10000))\n",
    "        htmlBlock = '<div id=\"' + idName + '\"><canvas/></div>'\n",
    "        js = '''\n",
    "        <script>\n",
    "            data = ''' + outputCode + ''';\n",
    "            var jpc = new m21.jsonPickle.Converter();\n",
    "            var streamObj = jpc.run(data);\n",
    "            streamObj.replaceCanvas(\"#''' + idName + '''\");\n",
    "        </script>\n",
    "        '''\n",
    "        return HTML(htmlBlock + js)\n",
    "    \n",
    "    def show(self, obj, fmt, app=None, subformats=None, **keywords):\n",
    "        '''\n",
    "        show using the appropriate subformat.\n",
    "        '''\n",
    "        if subformats is None:\n",
    "            subformats = ['vexflow']\n",
    "        \n",
    "        if subformats and subformats[0] == 'vexflow':\n",
    "            return self.vfshow(obj)\n",
    "            #subformats = ['lilypond','png']\n",
    "        helperFormat = subformats[0]\n",
    "        helperSubformats = subformats[1:]\n",
    "        \n",
    "        from music21 import converter\n",
    "        \n",
    "        helperConverter = converter.Converter()\n",
    "        helperConverter.setSubconverterFromFormat(helperFormat)\n",
    "        helperSubConverter = helperConverter.subConverter\n",
    "\n",
    "        if helperFormat == 'musicxml':        \n",
    "            ### hack to make musescore excerpts -- fix with a converter class in MusicXML\n",
    "            savedDefaultTitle = defaults.title\n",
    "            savedDefaultAuthor = defaults.author\n",
    "            defaults.title = ''\n",
    "            defaults.author = ''\n",
    "            \n",
    "            if 'Opus' not in obj.classes:\n",
    "                fp = helperSubConverter.write(obj, helperFormat, subformats=helperSubformats)\n",
    "        \n",
    "                defaults.title = savedDefaultTitle\n",
    "                defaults.author = savedDefaultAuthor\n",
    "                if helperSubformats[0] == 'png':\n",
    "                    from music21.ipython21 import objects as ipythonObjects\n",
    "                    ipo = ipythonObjects.IPythonPNGObject(fp)\n",
    "                    return ipo\n",
    "            else:\n",
    "                from IPython.display import Image, display # @UnresolvedImport\n",
    "                for s in obj.scores:\n",
    "                    fp = helperSubConverter.write(s, helperFormat, subformats=helperSubformats)\n",
    "            \n",
    "                    if helperSubformats[0] == 'png':\n",
    "                        from music21.ipython21 import objects as ipythonObjects # @Reimport\n",
    "                        ipo = ipythonObjects.IPythonPNGObject(fp)\n",
    "                        display(Image(filename=ipo.fp, retina=True))\n",
    "                defaults.title = savedDefaultTitle\n",
    "                defaults.author = savedDefaultAuthor\n",
    "                return None\n",
    "        elif helperFormat == 'midi':\n",
    "            import base64\n",
    "            from IPython.display import HTML, display # @UnresolvedImport @Reimport\n",
    "            fp = helperSubConverter.write(obj, helperFormat, subformats=helperSubformats)\n",
    "            with open(fp, 'rb') as f:\n",
    "                binaryMidiData = f.read()\n",
    "            binaryBase64 = base64.b64encode(binaryMidiData)\n",
    "            s = common.SingletonCounter()\n",
    "            outputId = \"midiPlayerDiv\" + str(s())\n",
    "            display(HTML(\"\"\"\n",
    "                <div id='\"\"\" + outputId + \"\"\"'></div>\n",
    "                <link rel=\"stylesheet\" href=\"http://artusi.xyz/music21j/css/m21.css\" \n",
    "                    type=\"text/css\" />\n",
    "                <script>\n",
    "                require.config({\n",
    "                    paths: {'music21': 'http://artusi.xyz/music21j/src/music21'} \n",
    "                }); \n",
    "                require(['music21'], function() { \n",
    "                               mp = new music21.miditools.MidiPlayer();\n",
    "                               mp.addPlayer('#\"\"\" + outputId + \"\"\"'); \n",
    "                               mp.base64Load('data:audio/midi;base64,\"\"\" + \n",
    "                                   binaryBase64.decode('utf-8') + \"\"\"'); \n",
    "                        });\n",
    "                </script>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from music21 import vexflow as vexflow\n",
    "import music21 as m21\n",
    "\n",
    "tmp = ConverterIPython()\n",
    "tmp.show(s,'vexflow')\n",
    "\n",
    "#n = m21.note.Note('C#4')\n",
    "#vexflow.toMusic21j.fromObject(n, mode='jsbody')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-testing jsonPickle: http://web.mit.edu/music21/doc/moduleReference/moduleVexflowToMusic21j.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''two possible approaches:\n",
    "1. approach 1:\n",
    "    a. python: stream -> vexflow, wrap in json, send via socketio, \n",
    "    b. javascript: receive json, json -> stream, display in browser\n",
    "    \n",
    "2. approach 2:\n",
    "    a. python: stream -> vexflow, pickle, save\n",
    "    b. javascript: load, unfreeze, display\n",
    "\n",
    "Here's approach 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''1. create stream object'''\n",
    "\n",
    "'''aside from creating massively large files (i.e., 6-7k / note), using \n",
    "vexflow.toMusic21j.fromObject() doesn't get the formats right. it can handle notes, \n",
    "and chord then notes, but it can't handle notes then chord. rendering error: \n",
    "Uncaught TypeError: Cannot read property 'octave' of undefined. in stream object.\n",
    "\n",
    "What's odd is that \n",
    "'''\n",
    "import music21 as m21\n",
    "\n",
    "s = m21.stream.Stream()\n",
    "p = m21.stream.Part()\n",
    "n1 = m21.note.Note('C#4')\n",
    "n2 = m21.note.Note('C4')\n",
    "n3 = m21.note.Note('D5')\n",
    "n4 = m21.note.Note('G5')\n",
    "\n",
    "#p.append(n1)\n",
    "p.append(n1)\n",
    "#p.append(n3)\n",
    "#p.append(n4)\n",
    "\n",
    "c = m21.chord.Chord([n2, n3, n4])\n",
    "p.append(c)\n",
    "\n",
    "s.insert(0,p)\n",
    "\n",
    "#s.show()\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''2. create music21j (whole html) object'''\n",
    "\n",
    "v = m21.vexflow.toMusic21j.fromObject(p) # mode='jsbody' # json\n",
    "print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''3. pickle / save the object'''\n",
    "\n",
    "'''so, I:\n",
    "1. cloned jsonpickleJS repository. it's now in frontEnd/src/ext/jsonpickleJS.\n",
    "2. created testPickle.html and testUnpickle.html as outlined in the jsonpickleJS notes: https://github.com/cuthbertLab/jsonpickleJS.\n",
    "3. pointed the html data-main and src:\n",
    "    a. to internet versions of the source and require files\n",
    "    b. to local versions of the source and require file\n",
    "4. both failed giving same error: \"Uncaught SyntaxError: Unexpected token import\" referring\n",
    "to the first import in the jsonpickleJS/main.js module. Specifically: import { unpickler } from './unpickler';\n",
    "5. investigated this fix which seems to be related to what babel is transpiling.\n",
    "    a. babel is a javascript compiler. it compiles the modules provided by webpack, etc.\n",
    "    b. webpack is a module bundler. give it modules w/ dependencies and it output static \"assets\"\n",
    "6. Babel transpiling seems to be driven by a webpack configuration in the current installation.\n",
    "7. so, to progress, you'll need to find the webpack file appropriate to the jsonpickleJS\n",
    "implementation and update it to include the module module its missing(?):\n",
    "http://stackoverflow.com/questions/38743111/uncaught-syntaxerror-unexpected-token-import\n",
    "\n",
    "8. So, when you install from git, if there are package.json and Gruntfile.js\n",
    "files in the dir then you need to:\n",
    "    a. cd to install directory\n",
    "    b. terminal: npm install - this ill generate the node-modules directory w/ dependencies\n",
    "    c. terminal: grunt - this will perform the configuration script\n",
    "\n",
    "9. Problem is this still didn't work. \n",
    "\n",
    "10. So, spent 6 hours trying to figure out how to get Babel ES6 to interpret the import and \n",
    "export commands correctly (i.e., diff .babelrc configs), trying to re-write the code using\n",
    "require (rather than import), and trying to figure out how to get exports and module.exports\n",
    "working. To no avail.\n",
    "\n",
    "SO, THIS IS IMPORTANT, DON'T BURN MORE TIME ON PICKLEJS. BASICALLY, THE FILES ARE WAY TOO BIG\n",
    "TO PASS OVER SOCKETIO AND THE CONFIGURATION ISSUES ARE TOO MANY. TIME IS BETTER SPENT THINKING\n",
    "OF HOW YOU CAN SEND OVER SIMPLE STREAMS OF CHARACTERS (TINY NOTATION) AND HAVE THEM INTERPRETED.\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''4. in javascript, recover the object'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''5. in javascript, render the object'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Here's approach 2: \n",
    "\n",
    "freeze (which wraps stream object in json) doesn't work. if you:\n",
    "(1)freeze it in python \n",
    "(per: http://web.mit.edu/music21/doc/moduleReference/moduleFreezeThaw.html), \n",
    "(2)socketio the resulting json over to the browser, then \n",
    "(3) use:\n",
    "\n",
    "pyConv = new music21.fromPython.Converter();\n",
    "s = pyConv.run(stringRepresentingM21JsonPickle)\n",
    "\n",
    "per: http://web.mit.edu/music21/music21j/doc/music21_fromPython.js.html. you get an \n",
    "unpickler module error:\n",
    "\n",
    "\"Uncaught SyntaxError: Unexpected token o in JSON at position 1\".\n",
    "\n",
    "seems to be the problem referred to https://github.com/cuthbertLab/music21j/issues/13\n",
    "\n",
    "note also: m21.stream is not directly JSON serializable i.e., if you:\n",
    "1. import jsonpickle as jp\n",
    "2. import json \n",
    "2. run: frozen = json.dumps(s). It fails.\n",
    "\n",
    "'''\n",
    "\n",
    "#s = m21.converter.parse('./data/wip/scores/bach minuet in g classic medium.xml')\n",
    "\n",
    "print(len(s))\n",
    "frz = m21.freezeThaw.JSONFreezer(s)\n",
    "print(type(frz))\n",
    "\n",
    "so = frz.storedObject\n",
    "print(type(so))\n",
    "\n",
    "so.show()\n",
    "\n",
    "#thw = m21.freezeThaw.JSONThawer(frz)\n",
    "#print(type(thw))\n",
    "\n",
    "#so = thw.storedObject\n",
    "#print(type(so))\n",
    "\n",
    "#type(fs)\n",
    "\n",
    "#sJSON = s.write('musicxml')\n",
    "#type(sJSON)\n",
    "\n",
    "#print(sJSON)\n",
    "\n",
    "#tnc = m21.tinyNotation.Converter(\"3/4 E4 r f# g=lastG trip{b-8 a g} c4~ c\")\n",
    "#stream1 = tnc.parse().stream\n",
    "#stream1.show()\n",
    "#s.write('TinyNotation')\n",
    "#c = m21.converter.Converter()\n",
    "#print(c.subconvertersList(\"output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from flask import Flask, Response\n",
    "import gevent\n",
    "from gevent.wsgi import WSGIServer\n",
    "from gevent.queue import Queue\n",
    "\n",
    "# SSE \"protocol\" is described here: http://mzl.la/UPFyxY\n",
    "class ServerSentEvent(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.event = None\n",
    "        self.id = None\n",
    "        self.desc_map = {\n",
    "            self.data : \"data\",\n",
    "            self.event : \"event\",\n",
    "            self.id : \"id\"\n",
    "        }\n",
    "\n",
    "    def encode(self):\n",
    "        if not self.data:\n",
    "            return \"\"\n",
    "        lines = [\"%s: %s\" % (v, k) \n",
    "                 for k, v in self.desc_map.iteritems() if k]\n",
    "        \n",
    "        return \"%s\\n\\n\" % \"\\n\".join(lines)\n",
    "\n",
    "app = Flask(__name__)\n",
    "subscriptions = []\n",
    "\n",
    "# Client code consumes like this.\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    debug_template = \"\"\"\n",
    "     <html>\n",
    "       <head>\n",
    "       </head>\n",
    "       <body>\n",
    "         <h1>Server sent events</h1>\n",
    "         <div id=\"event\"></div>\n",
    "         <script type=\"text/javascript\">\n",
    "\n",
    "         var eventOutputContainer = document.getElementById(\"event\");\n",
    "         var evtSrc = new EventSource(\"/subscribe\");\n",
    "\n",
    "         evtSrc.onmessage = function(e) {\n",
    "             console.log(e.data);\n",
    "             eventOutputContainer.innerHTML = e.data;\n",
    "         };\n",
    "\n",
    "         </script>\n",
    "       </body>\n",
    "     </html>\n",
    "    \"\"\"\n",
    "    return(debug_template)\n",
    "\n",
    "@app.route(\"/debug\")\n",
    "def debug():\n",
    "    return \"Currently %d subscriptions\" % len(subscriptions)\n",
    "\n",
    "@app.route(\"/publish\")\n",
    "def publish():\n",
    "    #Dummy data - pick up from request for real data\n",
    "    def notify():\n",
    "        msg = str(time.time())\n",
    "        for sub in subscriptions[:]:\n",
    "            sub.put(msg)\n",
    "    \n",
    "    gevent.spawn(notify)\n",
    "    \n",
    "    return \"OK\"\n",
    "\n",
    "@app.route(\"/subscribe\")\n",
    "def subscribe():\n",
    "    def gen():\n",
    "        q = Queue()\n",
    "        subscriptions.append(q)\n",
    "        try:\n",
    "            while True:\n",
    "                result = q.get()\n",
    "                ev = ServerSentEvent(str(result))\n",
    "                yield ev.encode()\n",
    "        except GeneratorExit: # Or maybe use flask signals\n",
    "            subscriptions.remove(q)\n",
    "\n",
    "    return Response(gen(), mimetype=\"text/event-stream\")\n",
    "\n",
    "#import flask\n",
    "#import time\n",
    "\n",
    "#@route(\"/stream\")\n",
    "#def stream():\n",
    "#    def eventStream():\n",
    "#        while True:\n",
    "#            message = \"Current date & time \" + time.strftime(\"%c\")\n",
    "#            yield \"data:{}\\n\\n\".format(message)\n",
    "#            time.sleep(5)\n",
    "#    \n",
    "#    return Response(eventStream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "# author: oskar.blom@gmail.com\n",
    "#\n",
    "# Make sure your gevent version is >= 1.0\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "app.debug = True\n",
    "server = WSGIServer((\"\", 5000), app)\n",
    "server.serve_forever()\n",
    "    # Then visit http://localhost:5000 to subscribe \n",
    "    # and send messages by visiting http://localhost:5000/publish\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "base requirement:\n",
    "1. select score by voice\n",
    "    a. option: by notes\n",
    "    b. option: maintain playlist get's \"loaded\" for session including score/wav, fingerprint, etc.\n",
    "    c. option: realtime song switching. if several not predicted notes, search other songs for switch\n",
    "    d. option: maintain reprotoire\n",
    "2. show score segment\n",
    "    a. option: note annotations\n",
    "    b. option: show progress\n",
    "3. play standard\n",
    "    a. option: speed up or slow down\n",
    "    b. option: play metronome\n",
    "4. record played\n",
    "    a. option: lay down tracks\n",
    "5. identify differences\n",
    "6. communicate differences\n",
    "    a. option: display played notes\n",
    "    b. option: alongside standard\n",
    "    c. option: w/ errors highlighted\n",
    "7. playback standard or played\n",
    "\n",
    "advanced:\n",
    "gamification\n",
    "\n",
    "mainpage layout:\n",
    "1. top, command window\n",
    "    a. show user commands, system responses\n",
    "    b. scroll down as dialog evolves\n",
    "    c. key modules\n",
    "        i. recordAudio.html\n",
    "        ii. delayMic.html\n",
    "        \n",
    "2. middle, score window\n",
    "    a. display score\n",
    "    b. either scroll right or down keeping score in window\n",
    "    c. show current position\n",
    "    d. key modules\n",
    "        i. renderTinyNotationDivs.html\n",
    "        ii. followScore.html\n",
    "        \n",
    "3. bottom, play window\n",
    "    a. write music as written\n",
    "    b. scroll right or down\n",
    "    c. key modules\n",
    "        i. audioSearchGame.html / audioSearchTest.html\n",
    "        ii. midiInRequire.html / midiInChords.html\n",
    "        iii. pickleM21.html\n",
    "\n",
    "arch:\n",
    "1. python server handling local browser I/O\n",
    "2. browswer starts at main menu with\n",
    "    a. build playlist\n",
    "    b. main page: play songs\n",
    "    c. see scores\n",
    "    \n",
    "3. how much can voice nav handle? \n",
    "\n",
    "vree.org has produced an embeddable player (http://wim.vree.org/js/index.html) that \n",
    "displays the score and synchronizes music (mp3) or video (mp4) with the score. so, \n",
    "c/b an option if you: 1. wanted to show longer pieces, 2. weren't worried about rendering\n",
    "master and played side by side. Though xml option might let you create two instances,\n",
    "and display them side by side.\n",
    "\n",
    "problem is that: 1. need to convert MAPS wavs to mp3, 2. beyond MAPS, you'd have to\n",
    "either find standard mp3 performances or generate midi from score, then generate mp3\n",
    "from midi. result would be standard you can play. to play played, you: \n",
    "1. record in mp3,\n",
    "2. encode via madmom, \n",
    "3. transcribe via m21,\n",
    "4. write xml\n",
    "5. feed both to vree\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "%%HTML\n",
    "! file:///Users/mdowns/Projects/music_study_aid/abcweb_37/abcweb_local.html?preload/ksdata.js\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m21.tinyNotation.TinyNotation(\"4/4 E2 F#4 G8 r trip{G8 F A} G4 E2~ E1 B4 c B A G16 A8. G8 F E4 D C1 BB2 C2 D2 C2 BBB1 CC1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search - possibly valuable, possibly not\n",
    "\n",
    "music21.search.base.mostCommonMeasureRythms(streamIn, transposeDiatonic=False)\n",
    "problem: what if there isn't clear dilineation of measures\n",
    "\n",
    "music21.search.base.rhythmicSearch(thisStream, searchStream)\n",
    "\n",
    "music21.search.base.translateDiatonicStreamToString(inputStreamOrIterator, returnMeasures=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''two issues here: 1. does m21 create a workable midi translation. or are critical elements \n",
    "lost s/t you need to first run it thru mscore (using command line prompts). 2. regardless, \n",
    "how do you break up the file s/t you can read 1, 2, 3 measures (bars) at a time.\n",
    "\n",
    "try:\n",
    "1. reading in smaller maps midi MUS file, 2. reading in musescore midi. 3. compare the two \n",
    "in terms of a: header info, b: segmentation, c: rendering. \n",
    "\n",
    "Then, 2. run thru the m21 \"make\" options to see what diff they make. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''trying w/ music in corpus'''\n",
    "\n",
    "'''list of works in the corpus: \n",
    "http://web.mit.edu/music21/doc/about/referenceCorpus.html#referencecorpus. corpus contains\n",
    "works in xml and humdrum. contains lots of bach, some handel and bethoven. no pachebel.\n",
    "search corpus using: http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html'''\n",
    "\n",
    "# Parse an Opus, a collection of Scores. The most important method call for corpus. \n",
    "# While parse() method in converter takes filepath, this parse searches the corpus.\n",
    "# returns a music21.stream.Stream.\n",
    "o = m21.corpus.parse('josquin/laDeplorationDeLaMorteDeJohannesOckeghem')\n",
    "\n",
    "# Create a Score from a Measure range\n",
    "sExcerpt = o.mergeScores().measures(126, 134)\n",
    "\n",
    "# Create a reduction of Chords\n",
    "reduction = sExcerpt.chordify()\n",
    "\n",
    "# Iterate over the Chords and prepare presentation\n",
    "for c in reduction.flat.getElementsByClass('Chord'):\n",
    "    c.closedPosition(forceOctave=4, inPlace=True)\n",
    "    c.annotateIntervals()\n",
    "\n",
    "# Add the reduction and display the results\n",
    "sExcerpt.insert(0, reduction)\n",
    "#m21.play(sExcerpt)\n",
    "sExcerpt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = m21.corpus.parse('bwv66.6')\n",
    "s = s.measures(0,2)\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = m21.note.Note('C#4')\n",
    "print(m21.vexflow.toMusic21j.fromObject(n, mode='jsbody'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_xml = m21.converter.parse('./data/wip/scores/bach minuet in g classic medium.xml')\n",
    "s_xml = s_xml.measures(1,1)\n",
    "s_xml.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_xml.show('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txtCnv = m21.converter.subConverters.ConverterText()\n",
    "txtCnv(s_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = \"3/4 d'4 g8 a8 b8 c'8\"\n",
    "tmp = m21.tinyNotation.Converter(tmp)\n",
    "tmp = tmp.parse().stream\n",
    "tmp.show('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tn = TinyNotationWriter().streamToTinyNotation(s_xml)\n",
    "tn\n",
    "# is the problem with the tiny notation writer or reader?\n",
    "# notation below:\n",
    "# 1. has no clefs and, therefore, isn't parsing stream\n",
    "# 2. has not measure markers\n",
    "# 2. has not instr\n",
    "# 3. isn't tieing. \n",
    "# 5. no key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = tmp.parse().stream\n",
    "tmp.show()\n",
    "# 1. reader is setting clef by first note. then ignoring subsequent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp.show('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_tn = m21.tinyNotation.Converter(tn)\n",
    "s_tn = s_tn.parse().stream\n",
    "s_tn.show('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_tn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Music, Strip Instrument, Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''importing music midi'''\n",
    "fname = './data/wip/tmp/canon.mid'\n",
    "m = m21.converter.parse(fname)\n",
    "m = m.measures(1,5)\n",
    "m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''importing music xml'''\n",
    "url = 'http://kern.humdrum.org/cgi-bin/ksdata?l=users/craig/classical/pachelbel&file=canon.krn&f=xml'\n",
    "x = m21.converter.parse(url)\n",
    "x = x.measures(1,5)\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''importing humdrum'''\n",
    "url = 'http://kern.humdrum.org/cgi-bin/ksdata?l=users/craig/classical/pachelbel&file=canon.krn&f=kern'\n",
    "h = m21.converter.parse(url)\n",
    "h = h.measures(1,5)\n",
    "h.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = score_similarity(m, x, level=5, playFl=False, showFl=False)\n",
    "print(\"mvx:\")\n",
    "print(tmp)\n",
    "print\n",
    "tmp = score_similarity(m, h, level=5, playFl=False, showFl=False)\n",
    "print(\"mvh\")\n",
    "print(tmp)\n",
    "print\n",
    "tmp = score_similarity(h, x, level=5, playFl=False, showFl=False)\n",
    "print(\"hvx\")\n",
    "print(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [\"the\", \"old\", \"\", \"fox\"]\n",
    "\n",
    "' '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cmdDF = pd.DataFrame({'nxtTgt':pd.Series([None], dtype='str'),\n",
    "                      'nxtAct':pd.Series([None], dtype='str'),\n",
    "                      'nxtFile':pd.Series([None], dtype='str'),\n",
    "                         'nxtNum':pd.Series([None], dtype='str'),\n",
    "                         'nxtUnit':pd.Series([None], dtype='str'),\n",
    "                         'nxtMsg':pd.Series([None], dtype='str')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nxtTgt = 'the'\n",
    "nxtAct = 'old'\n",
    "nxtFile = 'grey'\n",
    "nxtNum = 'fox'\n",
    "\n",
    "a = [nxtTgt, nxtAct, nxtFile, nxtNum]\n",
    "b = [nxtTgt, nxtAct, nxtFile, nxtNum]\n",
    "\n",
    "a = np.array(a)\n",
    "b = np.array(b)\n",
    "c = np.append(a, b, axis=0)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = m21.instrument.partitionByInstrument(x)\n",
    "len(x.parts)\n",
    "#XML's problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = m21.instrument.partitionByInstrument(h)\n",
    "len(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h[0].show(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h[0].show()\n",
    "# COLLAPSES LEFT AND RIGHT INTO SINGLE TRIPLE CLEF - OFFSET ISSUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h[1].show()\n",
    "# COLLAPSES LEFT AND RIGHT INTO SINGLE TRIPLE CLEF - SO, S/B AN OFFSET ISSUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = m21.instrument.partitionByInstrument(m)\n",
    "print(len(mi.parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m[0].show()\n",
    "# COLLAPSING CLEF/TRIPLE CLEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m[1].show()\n",
    "# FAULTY LABELING OF HARP ON IMPORT CAUSES IT TO MISS HARP TRIPLE CLEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''importing music midi'''\n",
    "fname = './data/wip/tmp/canon_unk.mid'\n",
    "m = m21.converter.parse(fname)\n",
    "m = m.measures(1,5)\n",
    "m.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = './data/wip/tmp/canon_unk.xml'\n",
    "m = m21.converter.parse(fname)\n",
    "m = m.measures(1,5)\n",
    "m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = m21.instrument.partitionByInstrument(m)\n",
    "print(len(m.parts))\n",
    "\n",
    "for p in mi.parts:\n",
    "    unused = p.makeRests(fillGaps=True, inPlace=True)\n",
    "\n",
    "for p in mi.parts:\n",
    "    p.makeMeasures(inPlace=True)\n",
    "    p.makeTies(inPlace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "play_m21(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "play_m21(mi[1])\n",
    "mi[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_instrument(inM21Stream):\n",
    "    '''seggregates instruments in a score. prioritizes instruments by similarity to target.\n",
    "    returns closest instrument score. '''\n",
    "    \n",
    "    s = m21.instrument.partitionByInstrument(inM21Stream)\n",
    "    \n",
    "    m21.Instrument.bestName()\n",
    "Find a viable name, looking first at instrument, then part, then abbreviations\n",
    "\n",
    "#byInst = m21.instrument.partitionByInstrument(inM21Stream)\n",
    "\n",
    "#Instrument.bestName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(m21.instrument.Instrument())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(m21.instrument.Instrument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p1 = m21.converter.parse(\"tinynotation: 4/4 c4  d  e  f  g  a  b  c'  c1\")\n",
    "p1.getElementsByClass('Measure')[0].insert(0.0, m21.instrument.Harp())\n",
    "p1.getElementsByClass('Measure')[1].insert(3.0, m21.instrument.Harp())\n",
    "\n",
    "p2 = m21.converter.parse(\"tinynotation: 4/4 C#4 D# E# F# G# A# B# c#  C#1\")\n",
    "p2.getElementsByClass('Measure')[0].insert(3.0, m21.instrument.Harp()) # not likely...\n",
    "\n",
    "s = m21.stream.Score()\n",
    "s.insert(0, p1)\n",
    "s.insert(0, p2)\n",
    "\n",
    "play_m21(s)\n",
    "s.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build score \n",
    "p1 = m21.converter.parse(\"tinynotation: 4/4 c4  d  e  f  g  a  b  c'  c1\")\n",
    "p1.getElementsByClass('Measure')[0].insert(0.0, m21.instrument.Piccolo())\n",
    "p1.getElementsByClass('Measure')[0].insert(2.0, m21.instrument.AltoSaxophone())\n",
    "p1.getElementsByClass('Measure')[1].insert(3.0, m21.instrument.Piccolo())\n",
    "\n",
    "p2 = m21.converter.parse(\"tinynotation: 4/4 C#4 D# E# F# G# A# B# c#  C#1\")\n",
    "p2.getElementsByClass('Measure')[0].insert(0.0, m21.instrument.Trombone())\n",
    "p2.getElementsByClass('Measure')[0].insert(3.0, m21.instrument.Piccolo()) # not likely...\n",
    "p2.getElementsByClass('Measure')[1].insert(1.0, m21.instrument.Trombone())\n",
    "\n",
    "s = m21.stream.Score()\n",
    "s.insert(0, p1)\n",
    "s.insert(0, p2)\n",
    "\n",
    "play_m21(s)\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.show(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2 = m21.instrument.partitionByInstrument(s)\n",
    "print(len(s2.parts))\n",
    "\n",
    "play_m21(s2)\n",
    "s2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in s2.parts:\n",
    "    unused = p.makeRests(fillGaps=True, inPlace=True)\n",
    "    \n",
    "play_m21(s2)\n",
    "s2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in s2.parts:\n",
    "    p.makeMeasures(inPlace=True)\n",
    "    p.makeTies(inPlace=True)\n",
    "\n",
    "play_m21(s2)\n",
    "s2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sm12 = s.measures(1,2)\n",
    "sm12.show('text')\n",
    "\n",
    "inst = m21.instrument.partitionByInstrument(sm12)\n",
    "print(len(inst))\n",
    "\n",
    "for p in inst.parts:\n",
    "    unused = p.makeRests(fillGaps=True, inPlace=True)\n",
    "\n",
    "for p in inst.parts:\n",
    "    p.makeMeasures(inPlace=True)\n",
    "    p.makeTies(inPlace=True)\n",
    "    \n",
    "sm12.show('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''importing from midi... is risky. first file below failed to load (m21 error). \n",
    "second file .show() killed musescore rendering'''\n",
    "\n",
    "#url = 'http://kern.humdrum.org/cgi-bin/ksdata?l=users/craig/classical/pachelbel&file=canon.krn&f=midi'\n",
    "url = 'http://www.midiworld.com/midis/other/n1/EspanjaPrelude.mid'\n",
    "s = m21.converter.parse(url)\n",
    "play_m21(s.measures(1,2))\n",
    "#s.measures(1,2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''importing from musicxml'''\n",
    "\n",
    "url = 'http://kern.ccarh.org/cgi-bin/ksdata?l=cc/bach/cello&file=bwv1007-01.krn&f=xml'\n",
    "s = m21.converter.parse(url)\n",
    "play_m21(s[1][:2])\n",
    "s[1][:2].show() # show first 2 measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_scores():\n",
    "    '''\n",
    "        returns list of scores available for display on frontend'''\n",
    "    \n",
    "    scoresDir = './data/wip/scores/'\n",
    "    scoreFiles = glob.glob(scoresDir + '*.pgz')\n",
    "    \n",
    "    fileNames=[]; fileExtensions=[]\n",
    "    for i in range(len(scoreFiles)):\n",
    "        fileName, fileExtension = os.path.splitext(os.path.basename(scoreFiles[i]))\n",
    "        fileNames.append(fileName)\n",
    "    \n",
    "    return fileNames\n",
    "\n",
    "load_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''get_scores parses (speech) keyword input. finds corresponding score. returns file address'''\n",
    "\n",
    "def get_score(searchWords):\n",
    "    \n",
    "    from difflib import SequenceMatcher\n",
    "    \n",
    "    def similar(a, b):\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "    scoresDir = './data/wip/scores/'\n",
    "    searchWords = searchWords.split(' ')\n",
    "    scoreFiles = glob.glob(scoresDir + '*.pgz')\n",
    "\n",
    "    fileNames=[]; fileExtensions=[]\n",
    "    for i in range(len(scoreFiles)):\n",
    "        fileName, fileExtension = os.path.splitext(os.path.basename(scoreFiles[i]))\n",
    "        fileNames.append(fileName)\n",
    "        fileExtensions.append(fileExtension)\n",
    "\n",
    "    matchScores=[]\n",
    "    for fileName in fileNames:\n",
    "        wordScore=[]\n",
    "        for keyword in fileName:\n",
    "            nearMatch=[]\n",
    "            for searchWord in searchWords:\n",
    "                nearMatch.append(similar(searchWord,keyword))\n",
    "            wordScore.append(sum(nearMatch))\n",
    "    \n",
    "        matchScores.append(round((sum(wordScore) / len(fileName)),3))\n",
    "\n",
    "    fileAddr = scoresDir +  fileNames[matchScores.index(max(matchScores))] + fileExtensions[matchScores.index(max(matchScores))]\n",
    "    return fileNames[matchScores.index(max(matchScores))], fileAddr\n",
    "\n",
    "get_score(\"pack of bells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''score_pickler.py converts music files to m21 stream objects, then pickles them off'''\n",
    "\n",
    "scoresDir = ''\n",
    "fileTypes = ('*.mid', '*.xml', '*.krn')\n",
    "scoreFiles = []\n",
    "for fileType in fileTypes:\n",
    "    scoreFiles.extend(glob.glob(scoresDir + fileType))\n",
    "\n",
    "for scoreFile in scoreFiles:\n",
    "    s = m21.converter.parse(scoreFile)\n",
    "    fileName, fileExtension = os.path.splitext(os.path.basename(scoreFile))\n",
    "    filePath = scoresDir + fileName + '.pgz'\n",
    "    m21.converter.freeze(s, fmt='pickle', fp=filePath, fastButUnsafe=False, zipType='zlib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = m21.converter.thaw('./data/wip/scores/beethoven fur elise short medium.pgz', zipType='zlib')\n",
    "play_m21(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = m21.converter.thaw('./data/wip/scores/bach minuet in g classic medium.pgz', zipType='zlib')\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoresDir = './data/wip/scores/'\n",
    "scoreFiles = glob.glob(scoresDir + '*.pgz')\n",
    "    \n",
    "fileNames=[]; fileExtensions=[]\n",
    "for i in range(len(scoreFiles)):\n",
    "    fileName, fileExtension = os.path.splitext(os.path.basename(scoreFiles[i]))\n",
    "    fileNames.append(fileName)\n",
    "    \n",
    "fileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnv = m21.converter.subConverters.ConverterVexflow()\n",
    "v = cnv(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# don't forget metronome\n",
    "\n",
    "# read in the file\n",
    "\n",
    "#fname = 'data/wip/tmp/MAPS_MUS-alb_se3_AkPnBcht_after_MS.mid' - marginal incremental value from going thu MS first\n",
    "fname = 'data/maps/AkPnBcht/MUS/MAPS_MUS-chpn-p1_AkPnBcht.mid'\n",
    "mf = m21.midi.MidiFile()\n",
    "mf.open(fname)\n",
    "mf.read()\n",
    "mf.close()\n",
    "\n",
    "s = m21.midi.translate.midiFileToStream(mf)\n",
    "\n",
    "print(len(s))\n",
    "print(len(s.getElementsByClass(m21.stream.Part)))\n",
    "print\n",
    "print(len(s[0]))\n",
    "print(len(s.getElementsByClass(m21.stream.Part)[0].getElementsByClass(m21.stream.Measure)))\n",
    "print\n",
    "s.show('text')\n",
    "for i in range(1,len(s[0])):\n",
    "    print(len(s[0][i].notes))\n",
    "\n",
    "#print(s[0][1].notes[0:10]) # chord is classified as \"note\"\n",
    "\n",
    "# PROBLEM IS THAT TAKING NOTES OUT AND APPENDING TO NEW STREAM IS LOOSING HE OFFSET INFORMATION.\n",
    "\n",
    "# build bars\n",
    "#tmp = s[0][5].notesAndRests\n",
    "#print(len(tmp))\n",
    "\n",
    "#for el in s[0][5].recurse().notesAndRests: #http://web.mit.edu/music21/doc/usersGuide/usersGuide_04_stream1.html#usersguide-04-stream1\n",
    "#        print(el.offset, el, el.activeSite)\n",
    "\n",
    "# build measures\n",
    "tmp = s[0][1].flat.notesAndRests\n",
    "print(len(tmp))\n",
    "tmp.show('text')\n",
    "\n",
    "for j in range(1): # int(len(tmp)/8 or 0, 1, 2, 3\n",
    "    s1 = m21.stream.Stream()\n",
    "    s1.append(s[0][0]) <<<<<<<<<<<<<< did not like this. need to find a way to extract non-note header data. also, need to think about running thru musescore to generate the midi, then use that better midi here.\n",
    "    for i in range(8):\n",
    "        s1.append(tmp[((j*8)+i)])\n",
    "        \n",
    "    s1.show()\n",
    "    time.sleep(2)\n",
    "    \n",
    "music21.midi.translate.midiFileToStream(mf, inputM21=None, quantizePost=True, **keywords)\n",
    "\n",
    "# translate the midi\n",
    "akp = m21.midi.translate.midiFileToStream('MAPS_MUS-chpn-p1_AkPnBcht.mid')\n",
    "\n",
    "# note xtics\n",
    "print(\"via index s[0][1]\")\n",
    "print(s[0][1])\n",
    "print\n",
    "\n",
    "print(\"s.flat creats a flattened object containing a list:\")\n",
    "print(s.flat)\n",
    "print\n",
    "\n",
    "print(\"s.elements only finds the first element. here it's a 'part'\")\n",
    "print(s.elements)\n",
    "print\n",
    "\n",
    "print(\"s[0].elements takes the indexes of that first part\")\n",
    "print(s[0].elements)\n",
    "print\n",
    "\n",
    "print(\"...s.flat.elements would show elements across parts\")\n",
    "print(s.flat.elements)\n",
    "print\n",
    "\n",
    "print(\"s.flat.notes or (here) notesAndRests produces an object\")\n",
    "print(s.flat.notesAndRests)\n",
    "print\n",
    "\n",
    "print(\"to get at notes in that object, use a for:\")\n",
    "for myElement in s.flat.notes:\n",
    "    print(myElement)\n",
    "    print(myElement.pitch.midi)\n",
    "    print(myElement.duration)\n",
    "#offsets? Think they're only avail if they've been explicitly placed. \n",
    "print\n",
    "\n",
    "print(s.flat.notes[0])\n",
    "print\n",
    "\n",
    "# make notation\n",
    "music21.stream.makeNotation.makeMeasures(s, meterStream=None, refStreamOrTimeRange=None, searchContext=False, innerBarline=None, finalBarline='final', bestClef=False, inPlace=False)\n",
    "\n",
    "# compare \n",
    "midiConv = m21.converter.subConverters.ConverterMidi()\n",
    "fname1 = midiConv.write(s.flat.notes[0], fmt='midi')\n",
    "\n",
    "mf = m21.midi.MidiFile()\n",
    "mf.open(fname1)\n",
    "mf.read()\n",
    "mf.close()\n",
    "\n",
    "print(mf)\n",
    "#mf.show()\n",
    "\n",
    "#print(\"to get at details of each note use %\")\n",
    "#g = \"\"\n",
    "#for myNote in s.flat.notes:\n",
    "#    g += \"%s: %s; \" % (myNote.pitch, myNote.pitch.midi)\n",
    "\n",
    "#print(dir(client))\n",
    "#print(dir(m21.note))\n",
    "#print(dir(m21.stream))\n",
    "#print(dir(m21.midi.MidiFile))\n",
    "\n",
    "# call mscore to process midi\n",
    "#mscore 'data/maps/AkPnBcht/MUS/MAPS_MUS-chpn-p3_AkPnBcht.mscz' -o 'data/wip/tmp/MAPS_MUS-chpn-p3_AkPnBcht.mpos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repositories\n",
    "\n",
    "https://musescore.com/sheetmusic\n",
    "\n",
    "http://kern.ccarh.org\n",
    "\n",
    "http://www.musicxml.com/music-in-musicxml/\n",
    "\n",
    "http://www.midiworld.com/\n",
    "\n",
    "http://www.gutenberg.org/\n",
    "\n",
    "http://imslp.org/wiki/Canon_and_Gigue_in_D_major_(Pachelbel,_Johann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Process Sound Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Listen / Record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Recognize Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Recognizer(AudioSource):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Recognizer`` instance, which represents a collection of speech recognition functionality.\n",
    "        \"\"\"\n",
    "        self.energy_threshold = 300 # minimum audio energy to consider for recording\n",
    "        self.dynamic_energy_threshold = False\n",
    "        self.dynamic_energy_adjustment_damping = 0.15\n",
    "        self.dynamic_energy_ratio = 1.5\n",
    "        self.pause_threshold = 0.8 # seconds of non-speaking audio before a phrase is considered complete\n",
    "        # PROBABLY NEEDS TO COME DOWN TO 0.6-ISH\n",
    "        self.phrase_threshold = 0.3 \n",
    "        # minimum seconds of speaking audio before we consider the speaking audio a phrase - values below this are ignored (for filtering out clicks and pops)\n",
    "        self.non_speaking_duration = 0.5 # seconds of non-speaking audio to keep on both sides of the recording\n",
    "\n",
    "    def record(self, source, duration = None, offset = None):\n",
    "        \"\"\"\n",
    "        Records up to ``duration`` seconds of audio from ``source`` (an ``AudioSource`` instance) starting at \n",
    "        ``offset`` (or at the beginning if not specified) into an ``AudioData`` instance, which it returns.\n",
    "        If ``duration`` is not specified, then it will record until there is no more audio input.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n",
    "        assert source.stream is not None, \"Audio source must be entered before recording, see documentation for `AudioSource`; are you using `source` outside of a `with` statement?\"\n",
    "\n",
    "        frames = io.BytesIO()\n",
    "        seconds_per_buffer = (source.CHUNK + 0.0) / source.SAMPLE_RATE\n",
    "        elapsed_time = 0\n",
    "        offset_time = 0\n",
    "        offset_reached = False\n",
    "        while True: # loop for the total number of chunks needed\n",
    "            if offset and not offset_reached:\n",
    "                offset_time += seconds_per_buffer\n",
    "                if offset_time > offset:\n",
    "                    offset_reached = True\n",
    "\n",
    "            buffer = source.stream.read(source.CHUNK)\n",
    "            if len(buffer) == 0: break\n",
    "\n",
    "            if offset_reached or not offset:\n",
    "                elapsed_time += seconds_per_buffer\n",
    "                if duration and elapsed_time > duration: break\n",
    "\n",
    "                frames.write(buffer)\n",
    "\n",
    "        frame_data = frames.getvalue()\n",
    "        frames.close()\n",
    "        return AudioData(frame_data, source.SAMPLE_RATE, source.SAMPLE_WIDTH)\n",
    "\n",
    "    def adjust_for_ambient_noise(self, source, duration = 1):\n",
    "        \"\"\"\n",
    "        Adjusts the energy threshold dynamically using audio from ``source`` (an ``AudioSource`` instance) to \n",
    "        account for ambient noise. Intended to calibrate the energy threshold with the ambient energy level. \n",
    "        Should be used on periods of audio without speech - will stop early if any speech is detected. The \n",
    "        ``duration`` parameter is the maximum number of seconds that it will dynamically adjust the threshold\n",
    "        for before returning. This value should be at least 0.5 in order to get a representative sample of the \n",
    "        ambient noise.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n",
    "        assert source.stream is not None, \"Audio source must be entered before adjusting, see documentation for `AudioSource`; are you using `source` outside of a `with` statement?\"\n",
    "        assert self.pause_threshold >= self.non_speaking_duration >= 0\n",
    "\n",
    "        seconds_per_buffer = (source.CHUNK + 0.0) / source.SAMPLE_RATE\n",
    "        elapsed_time = 0\n",
    "\n",
    "        # adjust energy threshold until a phrase starts\n",
    "        while True:\n",
    "            elapsed_time += seconds_per_buffer\n",
    "            if elapsed_time > duration: break\n",
    "            buffer = source.stream.read(source.CHUNK)\n",
    "            energy = audioop.rms(buffer, source.SAMPLE_WIDTH) # energy of the audio signal\n",
    "\n",
    "            # dynamically adjust the energy threshold using assymmetric weighted average\n",
    "            damping = self.dynamic_energy_adjustment_damping ** seconds_per_buffer # account for different chunk sizes and rates\n",
    "            target_energy = energy * self.dynamic_energy_ratio\n",
    "            self.energy_threshold = self.energy_threshold * damping + target_energy * (1 - damping)\n",
    "\n",
    "    def listen(self, source, timeout = None):       \n",
    "        \"\"\"\n",
    "        Records a single phrase from ``source`` (an ``AudioSource`` instance) into an ``AudioData`` instance, which\n",
    "        it returns. This is done by waiting until the audio has an energy above ``recognizer_instance.energy_threshold``\n",
    "        (the user has started speaking), and then recording until it encounters ``recognizer_instance.pause_threshold``\n",
    "        seconds of non-speaking or there is no more audio input. The ending silence is not included.\n",
    "        \n",
    "        The ``timeout`` parameter is the maximum number of seconds that it will wait for a phrase to start before \n",
    "        giving up and throwing an ``speech_recognition.WaitTimeoutError`` exception. If ``timeout`` is ``None``, \n",
    "        it will wait indefinitely.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n",
    "        assert source.stream is not None, \"Audio source must be entered before listening, see documentation for `AudioSource`; are you using `source` outside of a `with` statement?\"\n",
    "        assert self.pause_threshold >= self.non_speaking_duration >= 0\n",
    "\n",
    "        seconds_per_buffer = (source.CHUNK + 0.0) / source.SAMPLE_RATE\n",
    "        pause_buffer_count = int(math.ceil(self.pause_threshold / seconds_per_buffer)) \n",
    "        # number of buffers of non-speaking audio before the phrase is complete\n",
    "        phrase_buffer_count = int(math.ceil(self.phrase_threshold / seconds_per_buffer)) \n",
    "        # minimum number of buffers of speaking audio before we consider the speaking audio a phrase\n",
    "        non_speaking_buffer_count = int(math.ceil(self.non_speaking_duration / seconds_per_buffer)) \n",
    "        # maximum number of buffers of non-speaking audio to retain before and after\n",
    "\n",
    "        # read audio input for phrases until there is a phrase that is long enough\n",
    "        elapsed_time = 0 # number of seconds of audio read\n",
    "        buffer = b\"\" # an empty buffer means that the stream has ended and there is no data left to read\n",
    "        while True:\n",
    "            frames = collections.deque()\n",
    "\n",
    "            # store audio input until the phrase starts\n",
    "\n",
    "            while True:\n",
    "                \"\"\"JUST LISTENING.\"\"\"\n",
    "                elapsed_time += seconds_per_buffer\n",
    "                if timeout and elapsed_time > timeout: # handle timeout if specified\n",
    "                    raise WaitTimeoutError(\"listening timed out\")\n",
    "\n",
    "                buffer = source.stream.read(source.CHUNK)\n",
    "                if len(buffer) == 0: break # reached end of the stream\n",
    "                frames.append(buffer)\n",
    "                if len(frames) > non_speaking_buffer_count: # ensure we only keep the needed amount of non-speaking buffers\n",
    "                    frames.popleft()\n",
    "\n",
    "                # detect whether speaking has started on audio input\n",
    "                energy = audioop.rms(buffer, source.SAMPLE_WIDTH) # energy of the audio signal\n",
    "                \"\"\"ENSURE THAT MUSIC THRESHOLD > VOICE THRESHOLD. OTHERWISE ADJUST\"\"\"\n",
    "                if energy > self.energy_threshold: break\n",
    "\n",
    "                # dynamically adjust the energy threshold using assymmetric weighted average\n",
    "                if self.dynamic_energy_threshold:\n",
    "                    damping = self.dynamic_energy_adjustment_damping ** seconds_per_buffer # account for different chunk sizes and rates\n",
    "                    target_energy = energy * self.dynamic_energy_ratio\n",
    "                    self.energy_threshold = self.energy_threshold * damping + target_energy * (1 - damping)\n",
    "\n",
    "            # read audio input until the phrase ends\n",
    "            pause_count, phrase_count = 0, 0\n",
    "            while True:\n",
    "                \"\"\"RECORDING WHAT IT HEARS\"\"\"\n",
    "                elapsed_time += seconds_per_buffer\n",
    "\n",
    "                buffer = source.stream.read(source.CHUNK)\n",
    "                if len(buffer) == 0: break # reached end of the stream\n",
    "                frames.append(buffer)\n",
    "                phrase_count += 1\n",
    "\n",
    "                # check if speaking has stopped for longer than the pause threshold on the audio input\n",
    "                energy = audioop.rms(buffer, source.SAMPLE_WIDTH) # energy of the audio signal\n",
    "                \n",
    "                if energy > self.energy_threshold:\n",
    "                    pause_count = 0\n",
    "                else:\n",
    "                    pause_count += 1\n",
    "                if pause_count > pause_buffer_count: # end of the phrase\n",
    "                    break\n",
    "\n",
    "            # check how long the detected phrase is, and retry listening if the phrase is too short\n",
    "            phrase_count -= pause_count # exclude the buffers for the pause before the phrase\n",
    "            if phrase_count >= phrase_buffer_count or len(buffer) == 0: break # phrase is long enough or we've reached the end of the stream, so stop listening\n",
    "\n",
    "        # obtain frame data\n",
    "        for i in range(pause_count - non_speaking_buffer_count): frames.pop() # remove extra non-speaking frames at the end\n",
    "        frame_data = b\"\".join(list(frames))\n",
    "\n",
    "        return AudioData(frame_data, source.SAMPLE_RATE, source.SAMPLE_WIDTH)\n",
    "    \n",
    "    \n",
    "    def listen_in_background(self, source, callback):\n",
    "        \"\"\"\n",
    "        Spawns a thread to repeatedly record phrases from ``source`` (an ``AudioSource`` instance) into an ``AudioData``\n",
    "        instance and call ``callback`` with that ``AudioData`` instance as soon as each phrase are detected.\n",
    "        \n",
    "        Returns a function object that, when called, requests that the background listener thread stop, and waits until\n",
    "        it does before returning. The background thread is a daemon and will not stop the program from exiting if there\n",
    "        are no other non-daemon threads.\n",
    "        \n",
    "        Phrase recognition uses the exact same mechanism as ``recognizer_instance.listen(source)``.\n",
    "        The ``callback`` parameter is a function that should accept two parameters - the ``recognizer_instance``, \n",
    "        and an ``AudioData`` instance representing the captured audio. Note that ``callback`` function will be called\n",
    "        from a non-main thread.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n",
    "        running = [True]\n",
    "        def threaded_listen():\n",
    "            with source as s:\n",
    "                while running[0]:\n",
    "                    try: # listen for 1 second, then check again if the stop function has been called\n",
    "                        audio = self.listen(s, 1)\n",
    "                    except WaitTimeoutError: # listening timed out, just try again\n",
    "                        pass\n",
    "                    else:\n",
    "                        if running[0]: callback(self, audio)\n",
    "        def stopper():\n",
    "            running[0] = False\n",
    "            listener_thread.join() # block until the background thread is done, which can be up to 1 second\n",
    "        listener_thread = threading.Thread(target=threaded_listen)\n",
    "        listener_thread.daemon = True\n",
    "        listener_thread.start()\n",
    "        return stopper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_flac_converter():\n",
    "    # determine which converter executable to use\n",
    "    system = platform.system()\n",
    "    \n",
    "    path = os.path.dirname(os.path.abspath('music_study_aid')) # __file__\n",
    "    # directory of the current module file, where all the FLAC bundled binaries are stored\n",
    "    \n",
    "    flac_converter = shutil_which(\"flac\") # check for installed version first\n",
    "    if flac_converter is None: # flac utility is not installed\n",
    "        compatible_machine_types = [\"i686\", \"i786\", \"x86\", \"x86_64\", \"AMD64\"] \n",
    "        # whitelist of machine types our bundled binaries are compatible with\n",
    "        if system == \"Windows\" and platform.machine() in compatible_machine_types:\n",
    "            flac_converter = os.path.join(path, \"flac-win32.exe\")\n",
    "        elif system == \"Linux\" and platform.machine() in compatible_machine_types:\n",
    "            flac_converter = os.path.join(path, \"flac-linux-x86\")\n",
    "        elif system == \"Darwin\" and platform.machine() in compatible_machine_types:\n",
    "            flac_converter = os.path.join(path, \"flac-mac\")\n",
    "        else:\n",
    "            raise OSError(\"FLAC conversion utility not available - consider installing the FLAC command line application using `brew install flac` or your operating system's equivalent\")\n",
    "\n",
    "    # mark FLAC converter as executable if possible\n",
    "    try:\n",
    "        stat_info = os.stat(flac_converter)\n",
    "        os.chmod(flac_converter, stat_info.st_mode | stat.S_IEXEC)\n",
    "    except OSError: pass\n",
    "\n",
    "    return flac_converter\n",
    "\n",
    "def shutil_which(pgm):\n",
    "    \"\"\"Python 2 compatibility: backport of ``shutil.which()`` from Python 3\"\"\"\n",
    "    path = os.getenv('PATH')\n",
    "    for p in path.split(os.path.pathsep):\n",
    "        p = os.path.join(p, pgm)\n",
    "        if os.path.exists(p) and os.access(p, os.X_OK):\n",
    "            return p\n",
    "\n",
    "class tempfile_TemporaryDirectory(object):\n",
    "    \"\"\"Python 2 compatibility: backport of ``tempfile.TemporaryDirectory`` from Python 3\"\"\"\n",
    "    def __enter__(self):\n",
    "        self.name = tempfile.mkdtemp()\n",
    "        return self.name\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        shutil.rmtree(self.name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classify Sound Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Continuous Frequency Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from essentia.standard import *\n",
    "import numpy as np\n",
    "#from numpy import inf\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "def cfa_classifier(inAudio, inType='wavDir', sourceType='mic', numPeaks=5, showGraphics=False):\n",
    "    #import file_namer as fn\n",
    "    \n",
    "    '''The following two approaches for separating voice commands from music are based on the observation that\n",
    "        speech signals usually display patterns of harmonics influenced by the shape of the vocal tract. Within a\n",
    "        time frame, they manifest themselves as peaks within the spectrum. Further, the partials (echos) can be found at the\n",
    "        fundamental frequency of a tone and at/near its integer multiples. Finally, the harmonics are sustained over\n",
    "        a certain span of time in which they are likely to vary in frequency. This last characteristic is highly\n",
    "        discriminative vis a vis both noise and music. Stated simply, voice spectrograms exhibit characteristic curved\n",
    "        trajectories. Music spectrograms exhibit strictly horizontal and minor vertical structures and noise looks\n",
    "        like... noise. Noticed as early as 1993 by M. Hawley @ MIT.\n",
    "        \n",
    "        Both approaches have three basic steps. First, they slice the audio stream into small frames and extract\n",
    "        features for each frame. Second, they train a classifier on a distinct training set. The classifier learns\n",
    "        to distinguish two classes (i.e., music/no-music and voice/no-voice). Third, the classifier is used to predict\n",
    "        class labels for all the frames in the test set. Finally, classification results are smoothed in a post-\n",
    "        processing step to obtain a label sequence for continuous audio segments.\n",
    "\n",
    "        Continuous Frequencey Activation (CFA) for detecting music among voice:\n",
    "        \n",
    "        In their 2007 work, Seyerlehner et al recognized that music can be differentiated by structural properties\n",
    "        like harmony and rhythm. If clarity and consistency of partial frequency emissions are evidence of music,\n",
    "        then a feature could be developed to reliably detect continuous frequency activations... even in the presence\n",
    "        of other audio signals. That feature was Continuous Frequency Activation (CFA). The computation of CFA can be\n",
    "        subdivided into:\n",
    "        1. Convert input audio stream into 11 kHz mono.'''\n",
    "    \n",
    "    estMean = 45.02\n",
    "    estSd = 31.65\n",
    "    \n",
    "    if inType == 'wavDir':\n",
    "        loader = essentia.standard.MonoLoader(downmix='mix', filename=inAudio, sampleRate=22050)\n",
    "        inAudio = loader()\n",
    "    #inAudio = inAudio[1*44100:180*44100]\n",
    "    \n",
    "    '''\n",
    "        2. Compute the power spectrum using a Hanning window function and a window size of 1024 samples\n",
    "        (roughly) 100ms of audio. A hop-size of 256 samples is used, resulting in an overlap of 75% percent.\n",
    "        After the conversion to decibel, we obtain a standard spectrogram representation.'''\n",
    "    \n",
    "    w = essentia.standard.Windowing(type = 'hann') #'hamming'?\n",
    "    power = essentia.standard.PowerSpectrum()\n",
    "    spectrum = essentia.standard.Spectrum()\n",
    "    \n",
    "    pwrSpec=[]\n",
    "    for frame in FrameGenerator(inAudio, frameSize = 1024, hopSize = 256):\n",
    "        pwrSpec.append(spectrum(w(frame))) # NOTE: USING SPECTRUM. power returns: power spectrum of input\n",
    "    \n",
    "    pwrSpec = essentia.array(pwrSpec).T #transpose, then convert list to an essentia.array first (== numpy.array of floats)\n",
    "    \n",
    "    if showGraphics == True:\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.subplot(2,3,1); plt.plot(inAudio)\n",
    "        plt.subplot(2,3,2); plt.imshow(pwrSpec[:100,:], aspect = 'auto')\n",
    "    \n",
    "    pwrSpec = np.square(pwrSpec)\n",
    "    pwrSpec = 10*np.log10(pwrSpec)\n",
    "    pwrSpec[pwrSpec == -np.inf] = 0\n",
    "    \n",
    "    # NOTE: CBA DOES A NORMALIZATION STEP AFTER HANNING. GIVEN LOCAL NORMALIZATION BELOW, IT SEEMS REDUNDANT\n",
    "    '''\n",
    "        3. Emphasize local peaks within each frame of the STFT by subtracting from the power spectrum of each frame the running average using a window size of N = 21 frequency bins: x_emph = x_i - 1/N * Sigma_k=-N/2... Xmin(max(k,1),N Were Xi denotes the energy of the i-th frequency component of the current frame. This step is useful to emphasize very soft tones, belonging to background music. The perceivable horizontal bars in the spectogram are compositions of consecutive local maxima. Thus, we try to emphasize these soft bars by emphasizing all local maxima in the spectrum of a frame.'''\n",
    "    \n",
    "    wndw = 21\n",
    "    for i in range(pwrSpec.shape[1]):\n",
    "        if i < int(wndw/2): pwrSpec[:,i] = pwrSpec[:,0] # set left side to initial value\n",
    "        elif i > (pwrSpec.shape[1] - int(wndw/2)): pwrSpec[:,i] = pwrSpec[:,-1] # right side to initial value\n",
    "        else: pwrSpec[:,i] = pwrSpec[:,i] - np.mean(pwrSpec[:,(i-int(wndw/2)):(i+int(wndw/2))]) # de-mean center\n",
    "    \n",
    "    '''\n",
    "        4. Binarize the frequency component to eliminate strength of activation (energy) in a given frame j,X_emph_ij by comparing to a fixed binarization threshold of 0.1 keeps even soft activations in the spectogram. But, inactive frequency bins are set to 0 using this low threshold.'''\n",
    "    \n",
    "    binThresh = 0.1\n",
    "    \n",
    "    pwrSpec[pwrSpec >= binThresh] = 1\n",
    "    pwrSpec[pwrSpec < binThresh] = 0\n",
    "    \n",
    "    if showGraphics == True: plt.subplot(2,3,4); plt.imshow(pwrSpec[:100,:], aspect = 'auto')\n",
    "    \n",
    "    '''\n",
    "        5. Compute frequency activation. Process the binarized power spectrum in \n",
    "        terms of blocks. Each block consists of F = 100 frames and blocks overlap\n",
    "        by 50%, which means that a block is an excerpt of the binarized spectrogram \n",
    "        corresponding to 0.26 seconds of audio. For each block we compute the frequency \n",
    "        activation function Activation(i). For each frequency bin i, the frequency \n",
    "        activation function measures how often a frequency component is active in a \n",
    "        block. We obtain the frequency activation function for a block by simply \n",
    "        summing up the binarized values for each frequency bin i: \n",
    "        Activation(i) = 1/F * Sigma_j=1^F Bij'''\n",
    "    \n",
    "    timeBlockSize = 30\n",
    "    timeIncr = timeBlockSize #/2\n",
    "    numTimeBlocks = (int(pwrSpec.shape[1] / timeIncr)) #-1\n",
    "    pwrAct = np.zeros((pwrSpec.shape[0],numTimeBlocks))\n",
    "    \n",
    "    for freqRow in range(pwrSpec.shape[0]):\n",
    "        timePosCtr = 0\n",
    "        for timeCol in range(numTimeBlocks):\n",
    "            pwrAct[freqRow,timeCol] = np.sum(pwrSpec[freqRow,timePosCtr:(timePosCtr+timeBlockSize)])\n",
    "            timePosCtr = timePosCtr + timeIncr\n",
    "\n",
    "    if showGraphics == True:\n",
    "        plt.subplot(2,3,5); plt.imshow(pwrAct[0:100,:], aspect = 'auto')\n",
    "        freqSum = np.sum(pwrAct,1)\n",
    "        plt.subplot(2,3,6); plt.plot(freqSum[0:100])\n",
    "    \n",
    "    '''6. Detect strong peaks. Peaks in the frequency activation function of a given\n",
    "     block indicate steady activations of narrow frequency bands. The spikier the \n",
    "    activation function, the more likely horizontal bars, which are characteristic\n",
    "    of sustained musical tones, are present. Even one large peak is quite a good\n",
    "    indicator for the presence of a tone. The peakiness of the freuquency activation\n",
    "    function is consequently a good indicator for the presence of music. To extract\n",
    "    the peaks we use the following simple peak picking algorithm. a. Collect all \n",
    "    local peaks, starting from the lowest frequency. Each local maximum of the \n",
    "    activation function is a potential peak. b. For each peak x_p, compute its \n",
    "    height-to-width index or peak value pv(xp) = h(xp)/w(xp), where the height \n",
    "    h(xp) is defined as min[f(p) - f(xl),f(p) - f(xr)], with f (x) the value of \n",
    "    the activation function at point (frequency bin) x and xl and xr are closest \n",
    "    local minima of f to the left and right of xp, respec- tively. The width w(xp)\n",
    "    of the peak is given by: w(x_p) = p - x_l, f(p) - f(xl) < f(p) - f(xr) ELSE x_r - p otherwise'''\n",
    "    \n",
    "    outPeaks = []\n",
    "    \n",
    "    for freqRow in range(pwrAct.shape[0]):\n",
    "        \n",
    "        tmpRow = pwrAct[freqRow,:]\n",
    "        N = len(tmpRow)\n",
    "        peaks = np.zeros((N))\n",
    "        maxList = np.zeros((N))\n",
    "        minList = np.zeros((N))\n",
    "        \n",
    "        for i in range(N):\n",
    "            maxList[i] = -1\n",
    "            minList[i] = -1\n",
    "        lastMaxIndex = 0; lastMinIndex = 0\n",
    "        direction = 0\n",
    "        cf = 0\n",
    "        \n",
    "        # advance if steady initial value\n",
    "        while ((cf < (N-1)) and (tmpRow[cf] == tmpRow[cf + 1])):\n",
    "            cf = cf + 1\n",
    "        \n",
    "        # in some cases that takes you to end of input row. So, check.\n",
    "        if(cf < (N-1)):\n",
    "            # if start is a max or min, account for it\n",
    "            if tmpRow[cf] > tmpRow[cf+1]:\n",
    "                maxList[cf] = tmpRow[cf]\n",
    "                lastMaxIndex = cf\n",
    "                direction = 0\n",
    "            \n",
    "            elif tmpRow[cf] < tmpRow[cf+1]:\n",
    "                minList[cf] = tmpRow[cf]\n",
    "                lastMinIndex = cf\n",
    "                direction = 1\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        count = 1\n",
    "        \n",
    "        for i in range(1,N):\n",
    "            \n",
    "            if ((tmpRow[i] > tmpRow[i-1]) and (direction == 0)): # minimum detected\n",
    "                \n",
    "                # scan backards for earliest occurrence of that minimum\n",
    "                cb = i-1\n",
    "                while ((cb > 1) and (tmpRow[cb-1] == tmpRow[cb])):\n",
    "                    cb = cb - 1\n",
    "                \n",
    "                # save minimum to list\n",
    "                minList[cb] = tmpRow[cb] # save first minimuim\n",
    "                minList[i-1] = tmpRow[i-1] # save second minimum\n",
    "                count = count + 1\n",
    "                direction = 1\n",
    "                \n",
    "                # calculate area of the peak\n",
    "                if count < 3: # first value was a max\n",
    "                    peaks[lastMaxIndex] = (maxList[lastMaxIndex] - minList[cb]) / (cb - lastMaxIndex)\n",
    "                # (hmax - hmin) / w\n",
    "                \n",
    "                else:\n",
    "                    if minList[lastMinIndex] > minList[cb]: # hminL > hminR\n",
    "                        peaks[lastMaxIndex] = (maxList[lastMaxIndex] - minList[lastMinIndex])/(lastMaxIndex - lastMinIndex)\n",
    "                    # (hmax - hminL) / w\n",
    "                    else:\n",
    "                        peaks[lastMaxIndex] = (maxList[lastMaxIndex] - minList[cb])/(cb-lastMaxIndex)\n",
    "                        # (hmsx - hminR) / w\n",
    "        \n",
    "                lastMinIndex = i - 1\n",
    "                                \n",
    "            elif ((tmpRow[i] < tmpRow[i-1]) and (direction == 1)):\n",
    "                                    \n",
    "                # save maximum to list\n",
    "                maxList[i-1] = tmpRow[i-1]\n",
    "                count = count + 1\n",
    "                direction = 0\n",
    "                lastMaxIndex = i - 1\n",
    "                                            \n",
    "        '''\n",
    "            7. Quantify the CFA of the activation function of a block, the pv values of all detected peaks are sorted in descending order, and the sum of the five largest peak values is taken to characterize the overall peakiness of the activation function.'''\n",
    "                                                    \n",
    "        # sort peaks\n",
    "        for i in range(len(peaks)):\n",
    "            outPeaks.append(peaks[i])\n",
    "\n",
    "    outPeaks = sorted(outPeaks,reverse=True)\n",
    "    outPeaks = outPeaks[0:numPeaks]\n",
    "    return (sum(outPeaks) - estMean) / estSd\n",
    "    #print(outPeaks)\n",
    "    #print('cfa: sum(outPeaks):', sum(outPeaks))\n",
    "    #if sourceType == \"file\":\n",
    "    #    if sum(outPeaks) > 85:\n",
    "    #        return(\"music\")\n",
    "    #    else:\n",
    "    #        return(\"voice\")\n",
    "    #elif sourceType == \"mic\":\n",
    "    #    if sum(outPeaks) > 90:\n",
    "    #        return(\"music\")\n",
    "    #    else:\n",
    "    #        return(\"voice\")\n",
    "    #else: print(\"error: invalid source type\")\n",
    "    \n",
    "    #plt.savefig(fn.get_save_name(\"png\"))\n",
    "    '''Thus we obtain one numeric value for each block of frames, which quantifies the presence of steady frequency\n",
    "        components within the current audio segment.'''\n",
    "    \n",
    "print 'human voice'\n",
    "print cfa_classifier(inAudio='data/wip/musicVoiceDescr/test/15_Jan_17_17.04.55.wav', inType = \"wavDir\", \n",
    "               sourceType='mic', numPeaks=5, showGraphics=True)\n",
    "\n",
    "print 'computer voice'\n",
    "print cfa_classifier(inAudio='data/wip/musicVoiceDescr/train/13_Jan_17_14.20.04.wav', inType = \"wavDir\", \n",
    "               sourceType='mic', numPeaks=5, showGraphics=True)\n",
    "\n",
    "print 'music'\n",
    "print cfa_classifier(inAudio='data/wip/musicVoiceDescr/test/15_Jan_17_17.05.03.wav', inType = \"wavDir\", \n",
    "               sourceType='mic', numPeaks=5, showGraphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Curved Frequency Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''The second, Curved Frequency Trajectory (CFT), was developed in 2012 to detect voices in xyz. It identifies \n",
    "human voices within mixed audio signals by detecting the curved frequency trajectory of the harmonics over a \n",
    "certain time periods. So, taking the opposite view of CFA, it uses change in observed frequencies as an indicator\n",
    "of human voice.'''\n",
    "\n",
    "def cft_classifier(inAudio, inType='wavDir', sourceType='mic', showGraphics=False):\n",
    "            \n",
    "    from scipy import signal\n",
    "    import operator\n",
    "    estMean = 1.62\n",
    "    estSd = 1.11\n",
    "    \n",
    "    if showGraphics == True: plt.figure(figsize=(12,9))\n",
    "        \n",
    "    # Sample input to 22.05 kHz monaural audio.\n",
    "    if inType == 'wavDir':\n",
    "        sig, samp_rate = mad.audio.signal.load_wave_file(inAudio, num_channels=1)\n",
    "      \n",
    "    # Generate STFT magnitude spectrogram \n",
    "    spec = mad.audio.spectrogram.Spectrogram(inAudio, frame_size=2048, hop_size=210, fft_size=4096, num_channels=1)\n",
    "    if showGraphics == True: plt.subplot(2,3,1); plt.imshow(spec[:,:].T, aspect='auto', origin='lower')\n",
    "        \n",
    "    # Map the STFT magnitude spectrum to a perceptual scale using log transform and 24 frequency bins \n",
    "    filtSpec = mad.audio.spectrogram.FilteredSpectrogram(spec,\n",
    "                                                         filterbank=mad.audio.filters.LogFilterbank,\n",
    "                                                         num_bands=24)\n",
    "    if showGraphics == True: plt.subplot(2,3,2); plt.imshow(filtSpec[:,:].T, aspect = 'auto')\n",
    "        \n",
    "    # Scale the filtered spectrogram logarithmically (after adding a constant value of 1 to avoid negative values). \n",
    "    logSpec = mad.audio.spectrogram.LogarithmicSpectrogram(filtSpec, add=1)\n",
    "    if showGraphics == True: plt.subplot(2,3,3); plt.imshow(logSpec[:,:].T, aspect = 'auto')\n",
    "    \n",
    "    # Consider only the lower 150 cent-scaled frequency bins of the spectrum\n",
    "    logSpec = logSpec[:,:150].T\n",
    "    if showGraphics == True: plt.subplot(2,3,4); plt.imshow(logSpec[:,:], aspect = 'auto')\n",
    "    \n",
    "    '''Compute cross-correlation between the two time frames Xt and Xt+offset. The cross-correlation\n",
    "    can be used to estimate the degree of correlation between shifted versions of these vectors, \n",
    "    for a range of frequency lags (l). Think of it as dampening signal by allowing for slight \n",
    "    (+/- 3 band) frequency variation. '''\n",
    "    timeOffset = 3\n",
    "    freqLags = [1,2,3]\n",
    "    \n",
    "    zeroLagCorrs = []\n",
    "    maxLagCorrs = []\n",
    "    for i in range(2,logSpec.shape[1]):\n",
    "        \n",
    "        # take time slices separated by timeOffset\n",
    "        x = logSpec[:,i]\n",
    "        y = logSpec[:,i-timeOffset]\n",
    "        \n",
    "        # calculate same frequency (i.e., zero frequency lag) correlation\n",
    "        lagCorrs = []\n",
    "        zlc = np.corrcoef(x[0:146], y[0:146], rowvar=1)[0][1]\n",
    "        zeroLagCorrs.append(zlc); lagCorrs.append(zlc)\n",
    "        \n",
    "        # calculate lagged frequency correlations\n",
    "        lagCorrs.append(np.corrcoef(x[0:146], y[1:147], rowvar=1)[0][1])\n",
    "        lagCorrs.append(np.corrcoef(x[0:146], y[2:148], rowvar=1)[0][1])\n",
    "        lagCorrs.append(np.corrcoef(x[0:146], y[3:149], rowvar=1)[0][1])\n",
    "        \n",
    "        # find maximum frequency lagged correlation\n",
    "        maxLagCorrs.append(np.max(lagCorrs))\n",
    "    \n",
    "    # Calculate gain\n",
    "    gain = map(operator.sub, maxLagCorrs, zeroLagCorrs)\n",
    "    if showGraphics == True: plt.subplot(2,3,5); plt.plot(gain)\n",
    "    \n",
    "    return (sum(gain) - estMean) / estSd\n",
    "    \n",
    "\n",
    "# test it\n",
    "# voice\n",
    "print('human voice:')\n",
    "print(cft_classifier(inAudio='data/wip/musicVoiceDescr/test/15_Jan_17_17.04.55.wav', \n",
    "               inType='wavDir', sourceType='mic', showGraphics=True))\n",
    "\n",
    "print('computer voice:')\n",
    "print(cft_classifier(inAudio='data/wip/musicVoiceDescr/train/13_Jan_17_14.20.04.wav', \n",
    "               inType='wavDir', sourceType='mic', showGraphics=True))\n",
    "\n",
    "# music\n",
    "print('music:')\n",
    "print(cft_classifier(inAudio='data/wip/musicVoiceDescr/test/15_Jan_17_17.05.03.wav', \n",
    "               inType='wavDir', sourceType='mic', showGraphics=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the model: 1. pre-process wav files breaking them into random lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# source for voice: http://www.voxforge.org/home/downloads/speech/english?pn=2\n",
    "#%xdel range\n",
    "import random as rnd\n",
    "import time\n",
    "import io\n",
    "import random as rnd\n",
    "\n",
    "rawDir = './data/wip/musicVoiceDiscr/music/raw/'\n",
    "processedDir = './data/wip/musicVoiceDiscr/music/processed/' #tmp/\n",
    "inFiles = glob.glob(rawDir + '*.wav')\n",
    "#inFiles = ['./data/wip/musicVoiceDiscr/voice/raw/e0055.wav']\n",
    "\n",
    "ctr = 0\n",
    "for inFile in inFiles:\n",
    "    \n",
    "    audio_reader = wave.open(inFile, \"rb\")\n",
    "\n",
    "    numChan = audio_reader.getnchannels()\n",
    "    sampWidth = audio_reader.getsampwidth()\n",
    "    frameRate = audio_reader.getframerate()\n",
    "    nFrames = audio_reader.getnframes()\n",
    "    parms = audio_reader.getparams()\n",
    "\n",
    "    seconds = nFrames/frameRate\n",
    "    endBuffer = 0.2\n",
    "    totalTime = 0\n",
    "    randTime = 0\n",
    "\n",
    "    timeIncrements = []\n",
    "    while totalTime < (seconds - endBuffer):\n",
    "        \n",
    "        if randTime != 0: \n",
    "            timeIncrements.append(randTime)\n",
    "            randTime = 0\n",
    "        \n",
    "        while randTime < 0.6:\n",
    "            randTime = rnd.normalvariate(1.2, 0.6)\n",
    "            \n",
    "        else:\n",
    "            totalTime = totalTime + randTime\n",
    "            \n",
    "    print(seconds, sum(timeIncrements), len(timeIncrements))\n",
    "    tmp = []\n",
    "    for ti in timeIncrements: tmp.append(int(ti))\n",
    "    print(tmp)\n",
    "    print\n",
    "    \n",
    "    #tm = time.strftime(\"%d_%b_%y_%H.%M.%S\", time.localtime())\n",
    "    for incr in timeIncrements:\n",
    "        segment = audio_reader.readframes(int(frameRate*incr))\n",
    "        wavName = str(time.clock()) + \".wav\"\n",
    "        \n",
    "        # generate the WAV file contents\n",
    "        with io.BytesIO() as wav_file:\n",
    "            wav_writer = wave.open(processedDir + wavName, \"wb\")\n",
    "            try: # note that we can't use context manager, since that was only added in Python 3.4\n",
    "                wav_writer.setframerate(frameRate)\n",
    "                wav_writer.setsampwidth(sampWidth)\n",
    "                wav_writer.setnchannels(numChan)\n",
    "                wav_writer.writeframes(segment)       \n",
    "                wav_data = wav_file.getvalue()\n",
    "            finally:  # make sure resources are cleaned up\n",
    "                wav_writer.close()\n",
    "                \n",
    "        ctr = ctr+1\n",
    "\n",
    "print(ctr)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processedDir = './data/wip/musicVoiceDiscr/music/processed/complete/' #tmp/\n",
    "inFiles = glob.glob(processedDir + '*.wav')\n",
    "#inFiles = ['./data/wip/musicVoiceDiscr/voice/raw/e0055.wav']\n",
    "\n",
    "ctr = 0\n",
    "for inFile in inFiles:\n",
    "    ctr = ctr + 1\n",
    "print ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. generate x values including cfa, cft, zcr, loudness, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import madmom as mad\n",
    "from essentia.standard import *\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from soundProcessing import cfa_classifier as cfa\n",
    "from soundProcessing import cft_classifier as cft\n",
    "\n",
    "# 1. read wav files from last week getting volume and duration distributions\n",
    "\n",
    "trainDir = './data/wip/musicVoiceDiscr/voice/processed/complete/'\n",
    "inFiles = glob.glob(trainDir + '*.wav')         \n",
    "#inFiles = ['./users/user1/wav/01_Feb_17_09.47.03.wav', './users/user1/wav/01_Feb_17_09.47.03.wav']\n",
    "\n",
    "outFile = []\n",
    "for inFile in inFiles:\n",
    "    \n",
    "    #fileName, _ = os.path.splitext(os.path.basename(inFile))\n",
    "    #plt.figure(figsize=(12,9))\n",
    "    \n",
    "    # pre-process chunks file        \n",
    "    rate = 44100\n",
    "    tmpCtr = 1\n",
    "    \n",
    "    gains = [0, -6]\n",
    "    outLine = []\n",
    "    for gain in gains:\n",
    "        \n",
    "        loader = essentia.standard.EqloudLoader(downmix='mix', \n",
    "                                                filename=inFile, \n",
    "                                                replayGain=gain,\n",
    "                                                sampleRate=rate)\n",
    "        \n",
    "        inAudio = loader()\n",
    "        #plt.subplot(2,2,tmpCtr+1); plt.plot(inAudio)\n",
    "        #tmpCtr=tmpCtr+2\n",
    "        \n",
    "        loudness = essentia.standard.Loudness()\n",
    "        loud = loudness(inAudio)\n",
    "        \n",
    "        if loud == 0:\n",
    "            print('file 0 loud:', inFile)\n",
    "            break\n",
    "        else:\n",
    "            outLine.append(loud)\n",
    "        \n",
    "        startStop = essentia.standard.StartStopSilence(threshold=-45)\n",
    "        for frame in FrameGenerator(inAudio, frameSize = 1024, hopSize = 256):\n",
    "            ss = startStop(frame)\n",
    "        outLine.append(ss[1]-ss[0])\n",
    "    \n",
    "        duration = essentia.standard.Duration()\n",
    "        dur = duration(inAudio)\n",
    "        outLine.append(dur)\n",
    "        \n",
    "        zcr = essentia.standard.ZeroCrossingRate()\n",
    "        zero = zcr(inAudio)\n",
    "        outLine.append(zero)\n",
    "        \n",
    "        cfaRaw, _ = cfa.cfa_classifier(inAudio, inType='wavFile', numPeaks=5, showGraphics=False)\n",
    "        outLine.append(cfaRaw)\n",
    "        \n",
    "        norms = [False, True]\n",
    "        for norm in norms: \n",
    "            sig = mad.audio.signal.Signal(inFile,\n",
    "                                          sample_rate=rate,\n",
    "                                          num_channels=1,\n",
    "                                          norm=norm,\n",
    "                                          gain=gain,\n",
    "                                          dtype=float)\n",
    "            #plt.subplot(2,2,tmpCtr); plt.plot(sig)\n",
    "            \n",
    "            cftRaw, _ = cft.cft_classifier(sig, inType='wavFile', showGraphics=False)\n",
    "            outLine.append(cftRaw)\n",
    "            \n",
    "    outFile.append(outLine)\n",
    "\n",
    "try:\n",
    "    toSave = pd.DataFrame(outFile,\n",
    "                      columns=['loud0', 'sndDur0', 'dur0', 'zero0', 'cfa0', 'cft0', 'cft0Norm', 'loud6', 'sndDur6', 'dur6', 'zero6', 'cfa6', 'cft6', 'cft6Norm']) #\n",
    "\n",
    "    toSave.to_csv('./data/wip/musicVoiceDiscr/voice/voice_results.csv')\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toSave.to_csv('./data/wip/musicVoiceDiscr/music/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import random as rnd\n",
    "\n",
    "trainDir = 'data/wip/musicVoiceDescr/train/'\n",
    "#trainDir = 'users/user1/wav/'\n",
    "inFiles = glob.glob(trainDir + '*.wav')\n",
    "#tests = rnd.sample(range(len(inFiles)), 20)\n",
    "print(len(inFiles))\n",
    "trainFName = []\n",
    "trainX = []\n",
    "\n",
    "for i in range(len(inFiles)):\n",
    "    \n",
    "    fN, fE = os.path.splitext(os.path.basename(inFiles[i]))\n",
    "    trainFName.append(fN)\n",
    "    \n",
    "    cft = cft_classifier(inAudio=inFiles[i], inType='wavDir', sourceType='mic', showGraphics=False)\n",
    "    cfa = cfa_classifier(inAudio=inFiles[i], inType='wavDir', sourceType='mic', numPeaks=5, showGraphics=False)\n",
    "    \n",
    "    trainX.append([cft, cfa])\n",
    "    \n",
    "trainX = np.array(trainX)\n",
    "print(np.mean(trainX[:,0]))\n",
    "print(np.std(trainX[:,0]))\n",
    "\n",
    "print(np.mean(trainX[:,1]))\n",
    "print(np.std(trainX[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use below to: \n",
    "\n",
    "1. read in manually encoded (excel) x/y values are re-norm for training\n",
    "\n",
    "2. create test/train datasets\n",
    "\n",
    "3. create cross validation folds\n",
    "\n",
    "4. fit, predict and score model\n",
    "\n",
    "5. summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train a logistic on the raw values\n",
    "#print(__doc__)\n",
    "\n",
    "#from sklearn import datasets, neighbors, linear_model, svm, discriminant_analysis\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#knn = neighbors.KNeighborsClassifier()\n",
    "#print('KNN score: %f' % knn.fit(trnX1, trnY).score(tstX1, tstY))\n",
    "#print\n",
    "#logistic = linear_model.LogisticRegression()\n",
    "#print('LR score: %f' % logistic.fit(trnX1, trnY).score(tstX1, tstY))\n",
    "#print\n",
    "#qda = discriminant_analysis.QuadraticDiscriminantAnalysis()\n",
    "#lda = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "#print('QDA score: %f' % lda.fit(trnX1, trnY).score(tstX1, tstY))\n",
    "#print\n",
    "#clf = svm.SVC(kernel='linear', C=1.0) # ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "#print('SVM score: %f' % clf.fit(trnX1, trnY).score(tstX1, tstY))\n",
    "#print\n",
    "#ada_discrete = AdaBoostClassifier(algorithm=\"SAMME\")\n",
    "#print('ADA D score: %f' % ada_discrete.fit(trnX1, trnY).score(tstX1, tstY))\n",
    "#ada_real = AdaBoostClassifier(algorithm=\"SAMME.R\")\n",
    "#print('ADA R score: %f' % ada_real.fit(trnX1, trnY).score(tstX1, tstY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1990, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "m1    0.976382\n",
       "dtype: float64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tests:\n",
    "# 1. cfa0 (0.663) vs cfa6 (0.683)\n",
    "# 2. cft0 (0.902) vs cft0Norm (0.885) vs cft6 (0.899) vs cft6Norm (0.891)\n",
    "# 3. zero0 (0.694) vs zero6 (0.688)\n",
    "# 4. cfa6/cft0 (0.913) vs cfa6/cft0/zero0 (0.919)\n",
    "# 5. loud0 (0.728) vs loud6 (0.725)\n",
    "# 6. sndDur0 (0.614) vs sndDur6 (0.63) vs dur0 (0.581) vs dur6 (0.581)\n",
    "# 7. cfa6/cft0/zero0/loud0/sndDur6 (0.984) vs cfa6/cft0/zero6/loud6/sndDur6(0.976)\n",
    "\n",
    "# load data\n",
    "tstDat = pd.read_csv('./data/wip/musicVoiceDiscr/combined.csv', header=0)\n",
    "#plt.hist(tstDat[:, 0])\n",
    "print(tstDat.shape)\n",
    "\n",
    "# Impute missing ('NaN') values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "impDS = imp.fit(tstDat)\n",
    "tstDatImp = pd.DataFrame(impDS.transform(tstDat))\n",
    "\n",
    "# process adaboost prediction folds\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "scores = np.zeros((10,1),dtype=float)\n",
    "scores = pd.DataFrame(scores, columns=['m1'])\n",
    "\n",
    "rcds = tstDatImp.shape[0]\n",
    "folds = 10\n",
    "rcdsPerFold = rcds/folds\n",
    "\n",
    "idx = range(rcds)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "for i in range(folds):\n",
    "    \n",
    "    features = [(11,5,3,7,10)]\n",
    "    \n",
    "    for j in range(len(features)):\n",
    "            \n",
    "        tstIdx = idx[(i*rcdsPerFold):((i+1)*rcdsPerFold)]\n",
    "        trnIdx = [element for z, element in enumerate(idx) if z not in tstIdx]\n",
    "        \n",
    "        tstY = tstDatImp.iloc[tstIdx,14]\n",
    "        trnY = tstDatImp.iloc[trnIdx,14] \n",
    "        \n",
    "        if type(features[j]) == tuple:\n",
    "            tstX = tstDatImp.iloc[tstIdx,features[j]]\n",
    "            trnX = tstDatImp.iloc[trnIdx,features[j]]\n",
    "            \n",
    "        else:\n",
    "            tstX = np.array(tstDatImp.iloc[tstIdx,features[j]]).reshape(-1,1)\n",
    "            trnX = np.array(tstDatImp.iloc[trnIdx,features[j]]).reshape(-1,1)\n",
    "            \n",
    "        scores.iloc[i,j] = ada_real.fit(trnX, trnY).score(tstX, tstY)\n",
    "\n",
    "scores.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get the separating hyperplane\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-5, 5)\n",
    "yy = a * xx - clf.intercept_[0] / w[1]\n",
    "\n",
    "# plot separating hyperplanes and samples\n",
    "h0 = plt.plot(xx, yy, 'k-', )\n",
    "plt.scatter(trnX1.iloc[:,7], trnX1.iloc[:,8], c=trainY, cmap=plt.cm.Paired)\n",
    "#plt.legend()\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use below to save off model to be re-imported / used in uberi main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(ada_real, open('./soundProcessing/vmAdaMin.pickle', 'wb'))\n",
    "tmp = pickle.load(open('./soundProcessing/vmAdaMin.pickle', 'rb'))\n",
    "\n",
    "dat = np.array([108, 0.042819983, 0.058845844]).reshape(1,-1)\n",
    "tmp.predict(dat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import random as rnd\n",
    "\n",
    "x = []; y = []\n",
    "for i in range(100):\n",
    "    x.append(rnd.randrange(100))\n",
    "    y.append(x[i] + rnd.randrange(100))\n",
    "\n",
    "x = np.reshape(x,(10,10))\n",
    "y = np.reshape(y,(10,10))\n",
    "\n",
    "x = np.reshape(x,(1,100))\n",
    "y = np.reshape(y,(1,100))\n",
    "\n",
    "np.corrcoef(x, y, rowvar=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Classification Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sound_classifier(inAudio='data/wip/tmp/dcp1.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/weird-al.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/dcp2.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/mudhole.wav', sourceType=\"file\",\n",
    "                 numPeaks = 5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/dcp3.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/window.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recognize_google(self, audio_data, key = None, language = \"en-US\", show_all = False):\n",
    "    \"\"\"\n",
    "    Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Google Speech Recognition\n",
    "    API. The Google Speech Recognition API key is specified by ``key``. If not specified, it uses a generic key \n",
    "    that works out of the box. This should generally be used for personal or testing purposes only, as it \n",
    "    **may be revoked by Google at any time**.\n",
    "    \n",
    "    To obtain your own API key, simply follow the steps on the `API Keys <http://www.chromium.org/developers/how-tos/api-keys>`__ page \n",
    "    at the Chromium Developers site. In the Google Developers Console, Google Speech Recognition is listed as \"Speech API\".\n",
    "        \n",
    "    The recognition language is determined by ``language``, an RFC5646 language tag like ``\"en-US\"`` (US English)\n",
    "    or ``\"fr-FR\"`` (International French), defaulting to US English. A list of supported language values can be \n",
    "    found in this `StackOverflow answer <http://stackoverflow.com/a/14302134>`__.\n",
    "        \n",
    "    Returns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the raw API \n",
    "    response as a JSON dictionary.\n",
    "        \n",
    "    Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a \n",
    "    ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't \n",
    "    valid, or if there is no internet connection.\n",
    "    \"\"\"\n",
    "    assert isinstance(audio_data, AudioData), \"`audio_data` must be audio data\"\n",
    "    assert key is None or isinstance(key, str), \"`key` must be `None` or a string\"\n",
    "    assert isinstance(language, str), \"`language` must be a string\"\n",
    "\n",
    "    flac_data = audio_data.get_flac_data(\n",
    "        convert_rate = none if audio_data.sample_rate >= 8000 else 8000, # audio samples must be at least 8 kHz\n",
    "        convert_width = 2 # audio samples must be 16-bit\n",
    "    )\n",
    "    if key is None: key = \"AIzaSyBOti4mM-6x9WDnZIjIeyEU21OpBXqWBgw\"\n",
    "    url = \"http://www.google.com/speech-api/v2/recognize?{0}\".format(urlencode({\n",
    "                \"client\": \"chromium\",\n",
    "                \"lang\": language,\n",
    "                \"key\": key,\n",
    "            }))\n",
    "    request = Request(url, data = flac_data, headers = {\"Content-Type\": \"audio/x-flac; rate={0}\".format(audio_data.sample_rate)})\n",
    "\n",
    "    # obtain audio transcription results\n",
    "    try:\n",
    "        response = urlopen(request)\n",
    "    except HTTPError as e:\n",
    "        raise RequestError(\"recognition request failed: {0}\".format(getattr(e, \"reason\", \"status {0}\".format(e.code)))) # use getattr to be compatible with Python 2.6\n",
    "    except URLError as e:\n",
    "        raise RequestError(\"recognition connection failed: {0}\".format(e.reason))\n",
    "    response_text = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # ignore any blank blocks\n",
    "    actual_result = []\n",
    "    for line in response_text.split(\"\\n\"):\n",
    "        if not line: continue\n",
    "        result = json.loads(line)[\"result\"]\n",
    "        if len(result) != 0:\n",
    "            actual_result = result[0]\n",
    "            break\n",
    "\n",
    "    # return results\n",
    "    if show_all: return actual_result\n",
    "    if \"alternative\" not in actual_result: raise UnknownValueError()\n",
    "    for entry in actual_result[\"alternative\"]:\n",
    "        if \"transcript\" in entry:\n",
    "            return entry[\"transcript\"]\n",
    "    raise UnknownValueError() # no transcriptions available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Process Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = range(10)\n",
    "Y = range(10)\n",
    "Z = np.zeros((len(X), len(Y)))\n",
    "from random import randint\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(Y)):\n",
    "        Z[i,j]=randint(0,9)\n",
    "    \n",
    "plt.contour(X,Y,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x. Join Wav Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print dir(essentia.standard)\n",
    "\n",
    "fnames = ['data/wip/tmp/window.wav'] + ['data/maps/AkPnBcht/ISOL/TR1/MAPS_ISOL_TR1_F_S0_M47_AkPnBcht.wav'] + ['data/wip/tmp/cops.wav']\n",
    "\n",
    "wavs = []\n",
    "outfile = \"data/wip/tmp/stringTest.wav\"\n",
    "blank = essentia.array(np.repeat(0.0, 1*44100, axis=0))\n",
    "\n",
    "for fname in fnames:\n",
    "    loader = essentia.standard.MonoLoader(downmix='mix', filename=fname, sampleRate=44100)\n",
    "    w = loader()\n",
    "    wavs.append(blank)\n",
    "    if(fname == 'data/maps/AkPnBcht/ISOL/TR1/MAPS_ISOL_TR1_F_S0_M47_AkPnBcht.wav'):\n",
    "        w = 10*w\n",
    "    wavs.append(w)\n",
    "    \n",
    "out = np.concatenate(wavs, axis=0)\n",
    "essentia.standard.MonoWriter(filename=outfile, format='wav', sampleRate=44100)(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Manage Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = cops.wav\n",
    "#'data/wip/tmp/cops.wav'\n",
    "#'data/maps/AkPnBcht/ISOL/CH/MAPS_ISOL_CH0.1_M_AkPnBcht.wav'\n",
    "\n",
    "sourceType = \"file\"\n",
    "signOff = False\n",
    "clipDuration = None # None if no set duration i.e., listen until eof or pause\n",
    "pastDuration = 0\n",
    "\n",
    "r = Recognizer()\n",
    "\n",
    "try:\n",
    "    if sourceType == \"mic\":\n",
    "        m = Microphone()\n",
    "        print(\"Me: Calibrating background noise...\")\n",
    "        with m as source: r.adjust_for_ambient_noise(source)\n",
    "        print\n",
    "        print(\"Me: Setting min energy to {}\".format(r.energy_threshold))\n",
    "    \n",
    "    while signOff == False:\n",
    "        print\n",
    "        print(\"Me: Listening...\")\n",
    "        \n",
    "        if sourceType == \"mic\":\n",
    "            with m as source: audio = r.listen(source)\n",
    "            #audio = r.listen(source)\n",
    "                \n",
    "        elif sourceType == \"file\":\n",
    "            #if clipDuration == None: # you will listen to entire clip\n",
    "            #    signOff = True\n",
    "            #else:\n",
    "            #    pastDuration = pastDuration + clipDuration\n",
    "            #    \n",
    "            #if (fileDuration - pastDuration) <= clipDuration:\n",
    "            #    clipDuration = (fileDuration - pastDuration)\n",
    "            #    signOff = True\n",
    "            #Records up to ``duration`` seconds of audio from ``source`` (an ``AudioSource`` instance) starting at ``offset`` (or beginning) into an ``AudioData`` instance, which it returns.\n",
    "        \n",
    "            with AudioFile(fname) as source:\n",
    "                print(fname)\n",
    "                print(isinstance(source,AudioSource))\n",
    "                print(type(source))\n",
    "                print(\"file duration2:\", source.DURATION)\n",
    "                audio = r.record(source) # , duration=clipDuration\n",
    "        \n",
    "        #wavData = audio.get_wav_data(\n",
    "        #    convert_rate = None if audio.sample_rate in [8000, 16000] else 16000,\n",
    "        #    convert_width = 2)\n",
    "        # audio samples must be 8 kHz or 16 kHz for speech recognition services, but 24-42 for analysis\n",
    "        # audio samples should be 16-bit\n",
    "\n",
    "        print(type(audio))\n",
    "        print\n",
    "        print(\"Me: Determining sound type...\")\n",
    "        print\n",
    "        if 1+1==2:\n",
    "            #if cfa_classifier(wavData, sourceType, numPeaks=5, showGraphics=False) == \"voice\": # 'data/wip/tmp/vrTmp.wav'\n",
    "            try: \n",
    "                print(\"Me: Interpreting command...\")\n",
    "                # recognize speech using Google Speech Recognition\n",
    "                value = r.recognize_google(audio)\n",
    "                print(1)\n",
    "                print(u\"You: {}\".format(value).encode(\"utf-8\"))\n",
    "                print(2)\n",
    "                nextAction, actionText = command_interpreter(value)\n",
    "                print(3)\n",
    "                if nextAction == \"end\":\n",
    "                    signOff = True\n",
    "                else:\n",
    "                    print(u\"Me: {}\".format(actionText).encode(\"utf-8\"))\n",
    "                print(4)\n",
    "            \n",
    "            except UnknownValueError:\n",
    "                print\n",
    "                print(\"Me: Say again?\")\n",
    "            except RequestError as e:\n",
    "                print(\"Me: Uh oh! Couldn't request results from Google Speech Recognition service; {0}\".format(e))\n",
    "                \n",
    "        else:\n",
    "            # it's music, save it. ideally, you'd have usr + date + score + bar... ideally, you'd also save as mp3\n",
    "            print(\"Me: Calling music recognition...\")\n",
    "            print(\"Me: You played...\")\n",
    "            print(np.array(rnn_note_and_chord_rec('single', 'data/wip/tmp/vrTmp.wav', outFileBase=None)))\n",
    "    print\n",
    "    print(\"Me: Thanks for playing! Ending now.\")\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import music21 as m21\n",
    "\n",
    "s = m21.converter.parse('./data/wip/scores/bach minuet in g classic medium.xml')\n",
    "s.measures(1,3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build Session Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store user, start time, stop time\n",
    "# identify song from input. assign it a name\n",
    "# store songs played, start / stop times, link to session\n",
    "# score session\n",
    "\n",
    "class Session():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Session`` instance, which represents a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq\n",
    "        \n",
    "    def xyz():\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lay Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analyze song midi indexing bars, lines\n",
    "# coordinate wav and midi (musescore)\n",
    "# coordinate master\n",
    "# maintain track stats e.g., time spent, errors,...\n",
    "\n",
    "class Track():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Track`` instance, which represents a song played in a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq\n",
    "        \n",
    "    def xyz():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Save Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save and index played bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Replace Fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### c. Assemble Track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Score Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Score():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Score`` instance, which represents a song played in a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq\n",
    "        \n",
    "    def xyz():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Configure MuseScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set defaults for ux including continuous view, mentronome, tempo\n",
    "# load score standard\n",
    "\n",
    "class Musescore_context():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Session`` instance, which represents a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Convert speech text to position refrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. low level regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "holds code/decode values for common translations including integer and ordinal words\n",
    "'''\n",
    "def getDict(dictType):\n",
    "    \n",
    "    if dictType == 'wordIntegers':\n",
    "        \n",
    "        return {\n",
    "            # integer ordinals\n",
    "            '1st': 1, '2nd': 2, '3rd': 3, '4th': 4, '5th': 5, '6th': 6,\n",
    "            '7th': 7, '8th': 8, '9th': 9, '10th': 10, '11th': 11, '12th': 12,\n",
    "            \n",
    "            # text ordinals\n",
    "            'first': 1, 'second': 2, 'third': 3, 'fourth': 4, 'fifth': 5, 'sixth': 6,\n",
    "            'seventh': 7, 'eigth': 8, 'ninth': 9, 'tenth': 10, 'eleventh': 11, 'twelveth': 12,\n",
    "            \n",
    "            # text integers\n",
    "            'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6,\n",
    "            'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10, 'eleven': 11, 'twelve': 12,\n",
    "        }\n",
    "    \n",
    "    if dictType == 'wordTokens':\n",
    "        \n",
    "        return {\n",
    "            # integer ordinals\n",
    "            '1st': 'ORD1', '2nd': 'ORD2', '3rd': 'ORD3', '4th': 'ORD4', '5th': 'ORD5', \n",
    "            '6th': 'ORD6', '7th': 'ORD7', '8th': 'ORD8', '9th': 'ORD9', '10th': 'ORD10',\n",
    "            '11th': 'ORD11', '12th': 'ORD12',\n",
    "            \n",
    "            # text ordinals\n",
    "            'first': 'ORD1', 'second': 'ORD2', 'third': 'ORD3', 'fourth': 'ORD4', \n",
    "            'fifth': 'ORD5', 'sixth': 'ORD6', 'seventh': 'ORD7', 'eigth': 'ORD8', \n",
    "            'ninth': 'ORD9', 'tenth': 'ORD10', 'eleventh': 'ORD11', 'twelveth': 'ORD12',\n",
    "            \n",
    "            # text integers\n",
    "            'one': 'INT1', 'two': 'INT2', 'three': 'INT3', 'four': 'INT4', 'five': 'INT5', \n",
    "            'six': 'INT6', 'seven': 'INT7', 'eight': 'INT8', 'nine': 'INT9', 'ten': 'INT10',\n",
    "            'eleven': 'INT11', 'twelve': 'INT12',\n",
    "        }\n",
    "\n",
    "\n",
    "'''\n",
    "holds regular expressions used to translate text values including integer and ordinal\n",
    "words\n",
    "'''\n",
    "def getRegExp(regExpType):\n",
    "    \n",
    "    if regExpType == 'commandTokens':\n",
    "        \n",
    "        return [\n",
    "            \n",
    "            # relative position\n",
    "            (r'go back', 'REL_POS_BACK'), (r'backup', 'REL_POS_BACK'), (r'back up', 'REL_POS_BACK'),\n",
    "            (r'go forward', 'REL_POS_FWD'), (r'move forward', 'REL_POS_FWD'), (r'skip forward', 'REL_POS_FWD'),\n",
    "            (r'forward', 'REL_POS_FWD'), (r'advance', 'REL_POS_FWD'),\n",
    "            \n",
    "            # absolute position\n",
    "            (r'go to', 'ABS_POS'), (r'move to', 'ABS_POS'), (r'skip to', 'ABS_POS'), (r'find', 'ABS_POS'),\n",
    "            \n",
    "            # replay\n",
    "            (r'replay', 'REPLAY'), (r'repeat', 'REPLAY'), (r'again', 'REPLAY'), \n",
    "            \n",
    "            # play actions\n",
    "            (r'start playing', 'PLAY'), (r'play\\s', 'PLAY'),          \n",
    "        ]\n",
    "    \n",
    "    if regExpType == 'wordTokens':\n",
    "        \n",
    "        return [\n",
    "            # word ordinals\n",
    "            (r'first', 'ORD1'), (r'second', 'ORD2'), (r'third', 'ORD3'), (r'fourth', 'ORD4'),\n",
    "            (r'fifth', 'ORD5'), (r'sixth', 'ORD6'), (r'seventh', 'ORD7'), (r'eighth', 'ORD8'),\n",
    "            (r'ninth', 'ORD9'), (r'tenth', 'ORD10'), (r'eleventh', 'ORD11'), (r'twelveth', 'ORD12'),\n",
    "            \n",
    "            # mixed ordinals\n",
    "            (r'1st', 'ORD1'), (r'2nd', 'ORD2'), (r'3rd', 'ORD3'), (r'4th', 'ORD4'), (r'5th', 'ORD5'),\n",
    "            (r'6th', 'ORD6'), (r'7th', 'ORD7'), (r'8th', 'ORD8'), (r'9th', 'ORD9'), (r'10th', 'ORD10'), \n",
    "            (r'11th', 'ORD11'), (r'12th', 'ORD12'),\n",
    "            \n",
    "            # word integers\n",
    "            (r'one', 'INT1'), (r'two', 'INT2'), (r'three', 'INT3'), (r'four', 'INT4'),\n",
    "            (r'five', 'INT5'), (r'six', 'INT6'), (r'seven', 'INT7'), (r'eight', 'INT8'),\n",
    "            (r'nine', 'INT9'), (r'ten', 'INT10'), (r'eleven', 'INT11'), (r'twelve', 'INT12'),\n",
    "            \n",
    "            # text integers\n",
    "            (r'\\s1\\s', 'INT1'), (r'\\s2\\s', 'INT2'), (r'\\s3\\s', 'INT3'), (r'\\s4\\s', 'INT4'),\n",
    "            (r'\\s5\\s', 'INT5'), (r'\\s6\\s', 'INT6'), (r'\\s7\\s', 'INT7'), (r'\\s8\\s', 'INT8'), \n",
    "            (r'\\s9\\s', 'INT9'), (r'\\s10\\s', 'INT10'), (r'\\s11\\s', 'INT11'), (r'\\s12\\s', 'INT12'),\n",
    "            \n",
    "            # notes\n",
    "            (r'\\snote\\s', 'SINGN'), (r'\\snotes\\s', 'PLUN'), \n",
    "            \n",
    "            # measures\n",
    "            (r'\\smeasure\\s', 'SINGM'), (r'\\smeasures\\s', 'PLUM'),\n",
    "            (r'\\sbar\\s', 'SINGM'), (r'\\sbars\\s', 'PLUM'),\n",
    "            \n",
    "            # score\n",
    "            (r'\\sscore\\s', 'SCORE'),\n",
    "                        \n",
    "            # linkage\n",
    "            (r'\\sand\\s', 'LINK'), (r'\\sthru\\s', 'LINK'), (r'\\sthrough\\s', 'LINK'),\n",
    "        ]\n",
    "    \n",
    "    elif regExpType == 'frmToTokens':\n",
    "        \n",
    "        return [\n",
    "            # from / to linkage\n",
    "            (r'from.+to', 'FROM_TO'), (r'from.+until', 'FROM_UNTIL'), \n",
    "            (r'from.+thru', 'FROM_THRU'), (r'from.+through', 'FROM_THROUGH'),\n",
    "            (r'starting.+ending', 'STARTING_ENDING'), (r'start.+end', 'START_END'),\n",
    "            (r'begin.+end', 'BEGIN_END'), (r'beginning.+ending', 'BEGINNING_ENDING'),\n",
    "            (r'start.+to', 'START_TO'), (r'begin.to', 'BEGIN_TO'),\n",
    "        ]\n",
    "    \n",
    "    elif regExpType == 'linkTokens':\n",
    "        \n",
    "        return [\n",
    "            # measure 1 notes 1 and 5\n",
    "            (r'SINGM\\sINT\\d+\\sPLUN\\sINT\\d+\\sLINK\\sINT\\d+' , 'ABSM_ABSN_LINK_NUM'),\n",
    "            \n",
    "            # note/measure 1 and note/measure 5, \n",
    "            (r'SINGM\\s\\INT\\d+\\sLINK\\sSINGM\\s\\INT\\d+', 'ABSM_LINK_ABSM'),\n",
    "            (r'SINGN\\s\\INT\\d+\\sLINK\\sSINGN\\s\\INT\\d+', 'RELN_LINK_RELN'),\n",
    "\n",
    "            # notes/measures 1 and 5 \n",
    "            (r'PLUM\\sINT\\d+\\sLINK\\sINT\\d+' , 'ABSM_LINK_NUM'), \n",
    "            # for a score, I'm treating measures as absolute references i.e., there's only one measure 1\n",
    "            (r'PLUN\\sINT\\d+\\sLINK\\sINT\\d+' , 'RELN_LINK_NUM'),\n",
    "        ]\n",
    "    \n",
    "    elif regExpType == 'lstNxtTokens':\n",
    "        \n",
    "        return [\n",
    "            # last 5 notes, last 2 measures\n",
    "            (r'last\\sINT\\d+\\sPLUM', 'LAST_RELM_NUM'),\n",
    "            (r'previous\\sINT\\d+\\sPLUM', 'LAST_RELM_NUM'),\n",
    "            (r'prior\\sINT\\d+\\sPLUM', 'LAST_RELM_NUM'),\n",
    "            (r'last\\sINT\\d+\\sPLUN', 'LAST_RELN_NUM'),\n",
    "            (r'previous\\sINT\\d+\\sPLUN', 'LAST_RELN_NUM'),\n",
    "            (r'prior\\sINT\\d+\\sPLUN', 'LAST_RELN_NUM'),\n",
    "            \n",
    "            # last note, last measure\n",
    "            (r'last\\sSINGM', 'LAST_RELM'),\n",
    "            (r'previous\\sSINGM', 'LAST_RELM'),\n",
    "            (r'prior\\sSINGM', 'LAST_RELM'),\n",
    "            (r'last\\sSINGN', 'LAST_RELN'),\n",
    "            (r'previous\\sSINGN', 'LAST_RELN'),\n",
    "            (r'prior\\sSINGN', 'LAST_RELN'),            \n",
    "            \n",
    "            # next 5 notes/measures\n",
    "            (r'next\\sINT\\d+\\sPLUM', 'NEXT_RELM_NUM'),\n",
    "            (r'next\\sINT\\d+\\sPLUN', 'NEXT_RELN_NUM'),\n",
    "            \n",
    "            # next note/measure\n",
    "            (r'next\\sSINGM', 'NEXT_RELM'),\n",
    "            (r'next\\sSINGN', 'NEXT_RELN'),\n",
    "        ]\n",
    "    \n",
    "    elif regExpType == 'ordinalTokens':\n",
    "        \n",
    "        return [\n",
    "            # st/nd/rd/th n notes of the st/nd/rd/th measure\n",
    "            (r'ORD\\d+\\sINT\\d+\\sPLUN\\sof the\\sORD\\d+\\sSINGM', 'ORD_NUM_ABSN_OFTHE_ORD_ABSM'),\n",
    "            \n",
    "            # st/nd/rd/th n notes of measure n\n",
    "            (r'ORD\\d+\\sINT\\d+\\sPLUN\\sof\\sSINGM\\sINT\\d+', 'ORD_NUM_ABSN_OF_ABSM_NUM'),\n",
    "            \n",
    "            # st/nd/rd/th and st/nd/rd/th notes of measure n\n",
    "            (r'ORD\\d+\\sLINK\\sORD\\d+\\sPLUN\\sof\\sSINGM\\sINT\\d+', 'ORD_LINK_ORD_OF_ABSM_NUM'),\n",
    "            \n",
    "            # st/nd/rd/th and st/nd/rd/th measures\n",
    "            (r'ORD\\d+\\sLINK\\sORD\\d+\\sPLUM', 'ORD_LINK_ORD_ABSM'),\n",
    "            \n",
    "            # st/nd/rd/th and st/nd/rd/th notes\n",
    "            (r'ORD\\d+\\sLINK\\sORD\\d+\\sPLUN', 'ORD_LINK_ORD_RELN'),\n",
    "            \n",
    "            # st/nd/rd/th n measures\n",
    "            (r'ORD\\d+\\sINT\\d+\\sPLUM', 'ORD_NUM_ABSM'),\n",
    "            \n",
    "            # st/nd/rd/th n notes\n",
    "            (r'ORD\\d+\\sINT\\d+\\sPLUN', 'ORD_NUM_ABSN'),\n",
    "            \n",
    "            # st/nd/rd/th measure\n",
    "            (r'ORD\\d+\\sSINGM', 'ORD_ABSM'),\n",
    "            \n",
    "            # st/nd/rd/th note\n",
    "            (r'ORD\\d+\\sSINGN', 'ORD_RELN'),\n",
    "        ]\n",
    "    \n",
    "    elif regExpType == 'miscTokens':\n",
    "        \n",
    "        return [\n",
    "            # measure n note n\n",
    "            (r'SINGM\\sINT\\d+\\sSINGN\\sINT\\d+', 'ABSM_ABSN'),\n",
    "            \n",
    "            # measure n\n",
    "            (r'SINGM\\sINT\\d+', 'ABSM_NUM'),\n",
    "            \n",
    "            # note n\n",
    "            (r'SINGN\\sINT\\d', 'RELN_NUM'),\n",
    "            \n",
    "            # score\n",
    "            (r'SCORE', 'ABSS'),\n",
    "            \n",
    "            # measure\n",
    "            (r'this\\sSINGM', 'RELM'), (r'the\\sSINGM', 'RELM'), (r'that\\sSINGM', 'RELM'), (r'SINGM', 'RELM'),\n",
    "            \n",
    "            # note\n",
    "            (r'this\\sSINGN', 'RELN'), (r'the\\sSINGN', 'RELN'), (r'that\\sSINGN', 'RELN'), (r'SINGN', 'RELN'), \n",
    "        ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. low-level regular expression functions tokenize input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "'''\n",
    "takes dict-like structure of regular expression tuples, compiles and inserts each into\n",
    "a list ready for search/findall algorithms.\n",
    "'''\n",
    "def compileIt(inRegExp):\n",
    "\n",
    "    compiled = []\n",
    "\n",
    "    for expression, functionCall in inRegExp:\n",
    "        try:\n",
    "            compiled.append( (re.compile(expression), functionCall) )\n",
    "        except:\n",
    "            print(\"Error compiling inRegExp, %s: %s\" % (expression, functionCall))\n",
    "            \n",
    "    return compiled\n",
    "\n",
    "\n",
    "'''\n",
    "takes compiled list of tuples with regular expressions and corresponding matched values. \n",
    "the matched values may be tokens or sub-function calls. it then iterates over input text\n",
    "returning the matched (or not) text, the matched value, the text and the value or the \n",
    "result of the function call. NOTE: USED FOR TEXT WITH SINGLE INTEGER WORD\n",
    "'''\n",
    "def tokenizeIt(inCompile, inText, returnType):\n",
    "    # valid returnTypes 1. matchedText, 2. matchedValue, 3. textAndValue, 4. functionResult\n",
    "\n",
    "    tokenized = []\n",
    "    \n",
    "    for regularExpression, matchedValue in inCompile:\n",
    "        \n",
    "        matches = regularExpression.search(' ' + inText + ' ')\n",
    "            \n",
    "        if matches is not None:\n",
    "                \n",
    "            if returnType == 'matchedText':\n",
    "                tokenized.append(matches.group(0))\n",
    "                    \n",
    "            elif returnType == 'matchedValue':\n",
    "                tokenized.append(matchedValue)\n",
    "                    \n",
    "            elif returnType == 'textAndValue':\n",
    "                tokenized.append(matches.group(0))\n",
    "                tokenized.append(matchedValue)\n",
    "                    \n",
    "            elif returnType == 'functionResult':\n",
    "                tokenized.append(functionCall(matches.group(0)))\n",
    "    \n",
    "    if len(tokenized) == 0: return False\n",
    "    else: return tokenized\n",
    "\n",
    "    \n",
    "'''\n",
    "takes a text stream, compiled regular expression and replacement word. applies the \n",
    "regular expression to the text replacing the target word with the replacement word. \n",
    "returns the result.\n",
    "'''\n",
    "def subIt(inText, inExpression, replaceWith):\n",
    "\n",
    "    if type(inExpression) == list:\n",
    "        \n",
    "        returnText = inText\n",
    "        \n",
    "        for i in range(len(inExpression)):\n",
    "            p = re.compile(inExpression[i])\n",
    "            returnText = p.sub(str(replaceWith[i]), returnText)\n",
    "        \n",
    "        return returnText\n",
    "    \n",
    "    else:\n",
    "        p = re.compile(inExpression)\n",
    "        return p.sub(str(replaceWith), inText)\n",
    "\n",
    "    \n",
    "'''\n",
    "function takes a single word or phrase containing a word or words that represent integers. \n",
    "returns either: the word(s) that were matched, the corresponding token(s) or the original \n",
    "text with the token(s) inserted. can run against any compiled input regular expressions'''\n",
    "\n",
    "def convKeyWord(inText, returnValues='tokenizedText', inRegExp=None):\n",
    "    \n",
    "    # valid returnValues: 1. 'matchedText', 2. 'matchedToken', 3. 'tokenizedText\n",
    "        \n",
    "    inText = inText.split(' ')\n",
    "    matchedText = []\n",
    "    matchedTokens = []\n",
    "    tokenizedText = ''\n",
    "        \n",
    "    # find / extract integer word tokens\n",
    "    if inRegExp: \n",
    "        compiled = compileIt(inRegExp)\n",
    "    else: \n",
    "        regExp = getRegExp('wordTokens')\n",
    "        compiled = compileIt(regExp)\n",
    "        \n",
    "    # process that list converting words to tokens or integers\n",
    "    for item in inText:\n",
    "            \n",
    "        # get tokens\n",
    "        token = tokenizeIt(compiled, item, 'matchedValue')\n",
    "            \n",
    "        if token:\n",
    "            matchedText.append(item)\n",
    "            matchedTokens.append(token[0])\n",
    "            tokenizedText = tokenizedText + token[0] + ' '\n",
    "                \n",
    "        else:\n",
    "            tokenizedText = tokenizedText + item + ' '\n",
    "        \n",
    "        \n",
    "    if returnValues == 'matchedText': \n",
    "        return matchedText\n",
    "            \n",
    "    elif returnValues == 'matchedToken':\n",
    "        return matchedTokens\n",
    "        \n",
    "    elif returnValues == 'tokenizedText':\n",
    "        return tokenizedText.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. mid-level functions convert tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "'''gets the last integer value from a token. starts at the end of the token and works\n",
    "backwards. so, it will find 7 in INT7, 10 in ORD10 and 10 in 10ORD'''\n",
    "def getLastInt(inText):\n",
    "    integerFound = False\n",
    "    windowStart = len(inText)-1\n",
    "    windowStop = len(inText)\n",
    "    integer = None\n",
    "    end = False\n",
    "    \n",
    "    while not end:\n",
    "        # if last digit is integer, move startWindow\n",
    "        if isInt(inText[windowStart:windowStop]): \n",
    "            integer = inText[windowStart:windowStop]\n",
    "            windowStart = windowStart - 1\n",
    "            integerFound = True\n",
    "        \n",
    "        # else if you haven't found an integer yet, move start and stop\n",
    "        elif integerFound == False:\n",
    "            windowStart = windowStart -1\n",
    "            windowStop = windowStop -1\n",
    "            \n",
    "        else:\n",
    "            end = True\n",
    "        \n",
    "    if integer != None:\n",
    "        return int(integer)\n",
    "    else:\n",
    "        return integer\n",
    "\n",
    "\n",
    "def getMeasureNum(inText, span=False):\n",
    "    \n",
    "    p = re.compile('SINGM\\sINT\\d+') # \"measure <number>\" convention\n",
    "    m = p.search(inText)\n",
    "    \n",
    "    if m: \n",
    "        if span == False:\n",
    "            return getLastInt(m.group(0))\n",
    "        else:\n",
    "            return getLastInt(m.group(0)), m.span()\n",
    "    else: \n",
    "        p = re.compile('PLUM\\sINT\\d+') # \"measures <number>\" convention\n",
    "        m = p.search(inText)\n",
    "        \n",
    "        if m: \n",
    "            if span == False:\n",
    "                return getLastInt(m.group(0))\n",
    "            else:\n",
    "                return getLastInt(m.group(0)), m.span()\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def getNoteNum(inText, span=False):\n",
    "    \n",
    "    p = re.compile('SINGN\\sINT\\d+') # \"note <number>\" convention\n",
    "    m = p.search(inText)\n",
    "        \n",
    "    if m: \n",
    "        if span == False:\n",
    "            return getLastInt(m.group(0))\n",
    "        else: \n",
    "            return getLastInt(m.group(0)), m.span()\n",
    "        \n",
    "    else: \n",
    "        p = re.compile('PLUN\\sINT\\d+') # \"notes <number>\" convention\n",
    "        m = p.search(inText)\n",
    "        \n",
    "        if m: \n",
    "            if span == False:\n",
    "                return getLastInt(m.group(0))\n",
    "            else:\n",
    "                return getLastInt(m.group(0)), m.span()\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "def getMeasureOrd(inText):\n",
    "        \n",
    "    p = re.compile('ORD\\d+\\sSINGM') # \"<st/nd/rd/th> measure\" convention\n",
    "    m = p.search(inText)\n",
    "    \n",
    "    if m: \n",
    "        p = re.compile('ORD\\d+') # \"<st/nd/rd/th><number>\" convention\n",
    "        m = p.search(m.group(0))\n",
    "\n",
    "        return getLastInt(m.group(0))\n",
    "    \n",
    "    else: return False\n",
    "\n",
    "    \n",
    "def getNoteOrd(inText):\n",
    "    \n",
    "    p = re.compile('ORD\\d+\\sSINGN') # \"<st/nd/rd/th> note\" convention\n",
    "    m = p.search(inText)\n",
    "\n",
    "    if m: \n",
    "        p = re.compile('ORD\\d+') # \"<st/nd/rd/th><number>\" convention\n",
    "        m = p.search(m.group(0))\n",
    "\n",
    "        return getLastInt(m.group(0))\n",
    "    \n",
    "    else: return False\n",
    "\n",
    "    \n",
    "def getNumMeasures(inText):\n",
    "    \n",
    "    p = re.compile('INT\\d+\\sPLUM') # \"<number> measures\" convention\n",
    "    m = p.search(inText)\n",
    "        \n",
    "    if m:\n",
    "        p = re.compile('INT\\d+') # \"<number> measure\" convention\n",
    "        m = p.search(m.group(0))\n",
    "        \n",
    "        return getLastInt(m.group(0))\n",
    "        \n",
    "    return False\n",
    "    \n",
    "    \n",
    "def getNumNotes(inText):\n",
    "    \n",
    "    p = re.compile('INT\\d+\\sPLUN') # \"<number> notes\" convention\n",
    "    m = p.search(inText)\n",
    "        \n",
    "    if m:\n",
    "        p = re.compile('INT\\d+') # \"<number> note\" convention\n",
    "        m = p.search(m.group(0))\n",
    "        \n",
    "        return getLastInt(m.group(0))\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "'''takes single text input or to/from text pair. extracts absolute music position \n",
    "references and formats for export to front end'''\n",
    "def getAbsolutePositions(inList):\n",
    "    \n",
    "    startMVal = 0\n",
    "    startNVal = 0\n",
    "    stopMVal = 0\n",
    "    stopNVal = 0\n",
    "    \n",
    "    for i in range(len(inList)):\n",
    "        \n",
    "        # match absolute measure number references i.e. \"measure 1/one\"      \n",
    "        m = getMeasureNum(inList[i])\n",
    "\n",
    "        if m:\n",
    "            if i == 0: startMVal = m\n",
    "            else: stopMVal = m\n",
    "        \n",
    "        else:\n",
    "            # match absolute ordinal measure references i.e., 1st/first measure\"\n",
    "            m = getMeasureOrd(inList[i])\n",
    "            \n",
    "            if m: \n",
    "                if i == 0: startMVal = m\n",
    "                else: stopMVal = m\n",
    "                    \n",
    "        # \"note 1/one\" references        \n",
    "        m = getNoteNum(inList[i])\n",
    "        \n",
    "        if m: \n",
    "            if i == 0: startNVal = m\n",
    "            else: stopNVal = m\n",
    "        \n",
    "        else:\n",
    "            # ordinal note references\n",
    "            m = getNoteOrd(inList[i])\n",
    "            \n",
    "            if m:               \n",
    "                if i == 0: startNVal = m\n",
    "                else: stopNVal = m\n",
    "        \n",
    "    return startMVal, startNVal, stopMVal, stopNVal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. high level pattern matching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. spoken text\n",
    "testDat = [\n",
    "    'play measure 3',\n",
    "    'play again',\n",
    "    'play measure four',\n",
    "    'replay notes 2 and 3 again',\n",
    "    'play measure five note one',\n",
    "    'go to the 3rd measure',\n",
    "    'play measure',\n",
    "    'play measure 6 notes 1 and 2',\n",
    "    'repeat last note',\n",
    "    'replay last 3 bars',\n",
    "    'repeat last measure',\n",
    "    'play measure 7 notes 2 thru 5',\n",
    "    'go forward three notes',\n",
    "    'play the next measure',\n",
    "    'play the next four notes',\n",
    "    'play notes 1 and 2 again',\n",
    "    'play the second and third notes of measure 3',\n",
    "    'play note',\n",
    "    'play the 4th measure',\n",
    "    'play the 3rd note',\n",
    "    'go back to measure three',\n",
    "    'go back 3 notes',\n",
    "    'skip forward 5 notes',\n",
    "    'play the 4th and 5th measures',\n",
    "    'play the 3rd thru 5th notes',\n",
    "    'play the 2nd two measures',\n",
    "    'play the 1st three notes',\n",
    "    'move to measure two',\n",
    "    'play measure 7 notes 3 through 5',\n",
    "    'play the first five notes of the 2nd measure',\n",
    "    'back up to measure 1',\n",
    "    'replay from measure 1 to measure 2',\n",
    "    'play the first three notes of measure 2',\n",
    "    'skip to the fifth measure',\n",
    "    'find measure one note two',\n",
    "    'starting at measure 1 play to measure 2',\n",
    "    'replay the second measure',\n",
    "    'play from measure 1 note two to measure 2 note 1',\n",
    "    'play score', \n",
    "    'pftlurgt',\n",
    "]\n",
    "            \n",
    "for item in testDat:\n",
    "    #print\n",
    "    #print(item)\n",
    "    \n",
    "    navCommand = None\n",
    "    nxtAct = None\n",
    "    startMVal = None\n",
    "    startNVal = None\n",
    "    stopMVal = None\n",
    "    stopNVal = None\n",
    "    relMVal = None\n",
    "    relNVal = None\n",
    "    mNum = None\n",
    "    nNum = None\n",
    "        \n",
    "    hlMatch = False\n",
    "\n",
    "# 2. preprocess: standardize number references first \n",
    "    tokenized = convKeyWord(item, returnValues='tokenizedText')\n",
    "\n",
    "# 3. high-level pattern matching used to identify command type, information included\n",
    "    # command patterns\n",
    "    compiled = compileIt(getRegExp('commandTokens'))\n",
    "    for regularExpression, matchedValue in compiled:\n",
    "        m = regularExpression.search(tokenized)\n",
    "        if m:\n",
    "            navCommand = matchedValue\n",
    "            tokenized = tokenized.replace(m.group(0), '').strip()\n",
    "            \n",
    "            if navCommand == 'REL_POS_BACK':\n",
    "                tokenized = 'last ' + tokenized\n",
    "                nxtAct = 'POSITION'\n",
    "                \n",
    "            elif navCommand == 'REL_POS_FWD':\n",
    "                tokenized = 'next ' + tokenized\n",
    "                nxtAct = 'POSITION'\n",
    "            \n",
    "            elif navCommand == 'ABS_POS':\n",
    "                nxtAct = 'POSITION'\n",
    "                \n",
    "            else:\n",
    "                nxtAct = navCommand\n",
    "            \n",
    "            break\n",
    "    \n",
    "    #print(tokenized)\n",
    "    #from / to combinations:\n",
    "    compiled = compileIt(getRegExp('frmToTokens'))\n",
    "    for regularExpression, matchedValue in compiled:\n",
    "        m = regularExpression.search(tokenized)\n",
    "        if m:\n",
    "            hlMatch = True\n",
    "            spn = m.span()\n",
    "            frmCls = tokenized[spn[0]:spn[1]]\n",
    "            toCls = tokenized[spn[1]:len(tokenized)]\n",
    "            startMVal, startNVal, stopMVal, stopNVal = getAbsolutePositions([frmCls, toCls])\n",
    "            \n",
    "            break\n",
    "            \n",
    "    # get and\n",
    "    if hlMatch == False:\n",
    "        compiled = compileIt(getRegExp('linkTokens'))\n",
    "        for regularExpression, matchedValue in compiled:\n",
    "            m = regularExpression.search(tokenized)\n",
    "            if m:\n",
    "                hlMatch = True\n",
    "                \n",
    "                if matchedValue == 'ABSM_ABSN_LINK_NUM':\n",
    "                    startMVal = stopMVal = getMeasureNum(m.group(0))\n",
    "                    startNVal = getNoteNum(m.group(0))\n",
    "                    stopNVal = getLastInt(m.group(0))\n",
    "                    \n",
    "                elif matchedValue == 'ABSM_LINK_ABSM':\n",
    "                    startMVal, spn = getMeasureNum(m.group(0), True)\n",
    "                    stopMVal = getMeasureNum(m.group(0)[spn[0]:len(m.group(0))])\n",
    "                    startNVal = stopNVal = 0\n",
    "\n",
    "                elif matchedValue == 'RELN_LINK_RELN':\n",
    "                    startMVal = stopMVal = lastCommand[3]\n",
    "                    startNVal, spn = getNoteNum(m.group(0), True)\n",
    "                    stopNVal = getNoteNum(m.group(0)[spn[0]:len(m.group(0))])\n",
    "                    \n",
    "                elif matchedValue == 'ABSM_LINK_NUM':\n",
    "                    startMVal = getMeasureNum(m.group(0))\n",
    "                    stopMVal = getLastInt(m.group(0))\n",
    "                    startNVal = stopNVal = 0\n",
    "                    \n",
    "                elif matchedValue == 'RELN_LINK_NUM':\n",
    "                    startMVal = stopMVal = lastCommand[3]\n",
    "                    startNVal = getNoteNum(m.group(0))\n",
    "                    stopNVal = getLastInt(m.group(0))                    \n",
    "                \n",
    "                break\n",
    "                \n",
    "    # get next / last\n",
    "    if hlMatch == False:    \n",
    "        # below will handle 'replay the last xyz', 'play the last/next xyz',...\n",
    "        compiled = compileIt(getRegExp('lstNxtTokens'))\n",
    "        for regularExpression, matchedValue in compiled:\n",
    "            m = regularExpression.search(tokenized)\n",
    "            \n",
    "            if m:\n",
    "                hlMatch = True\n",
    "\n",
    "                if matchedValue == 'LAST_RELM_NUM':\n",
    "                    mNum = getNumMeasures(m.group(0))\n",
    "                    relMVal = -mNum\n",
    "                    nNum = relNVal = 0\n",
    "                    \n",
    "                elif matchedValue == 'LAST_RELN_NUM':\n",
    "                    nNum = getNumNotes(m.group(0))\n",
    "                    relNVal = -nNum\n",
    "                    relMVal = mNum = 0\n",
    "                    \n",
    "                elif matchedValue == 'NEXT_RELM_NUM':\n",
    "                    relMVal = 1\n",
    "                    mNum = getNumMeasures(m.group(0))\n",
    "                    relNVal = nNum = 0\n",
    "                    \n",
    "                elif matchedValue == 'NEXT_RELN_NUM':\n",
    "                    if nxtAct == 'POSITION':\n",
    "                        relMVal = mNum = nNum = 0\n",
    "                        relNVal = getNumNotes(m.group(0))\n",
    "                    else:\n",
    "                        relNVal = 1\n",
    "                        nNum = getNumNotes(m.group(0))\n",
    "                        relMVal = mNum = 0\n",
    "                    \n",
    "                elif matchedValue == 'LAST_RELM':\n",
    "                    relMVal = -1\n",
    "                    mNum = 1\n",
    "                    relNVal = nNum = 0\n",
    "                    \n",
    "                elif matchedValue == 'LAST_RELN':\n",
    "                    relNVal = -1\n",
    "                    nNum = 1\n",
    "                    relMVal = mNum = 0\n",
    "                    \n",
    "                elif matchedValue == 'NEXT_RELM':\n",
    "                    relMVal = 1\n",
    "                    mNum = 1\n",
    "                    relNVal = nNum = 0\n",
    "                    \n",
    "                elif matchedValue == 'NEXT_RELN':                        \n",
    "                    relNVal = 1\n",
    "                    nNum = 1\n",
    "                    relMVal = mNum = 0\n",
    "                \n",
    "                break\n",
    "    \n",
    "    if hlMatch == False:\n",
    "        # matches ordinal values\n",
    "        compiled = compileIt(getRegExp('ordinalTokens'))\n",
    "        for regularExpression, matchedValue in compiled:\n",
    "            m = regularExpression.search(tokenized)\n",
    "            if m:\n",
    "                hlMatch = True\n",
    "                \n",
    "                if matchedValue == 'ORD_NUM_ABSN_OFTHE_ORD_ABSM':\n",
    "                    # assuming greedy re\n",
    "                    p1 = re.compile('ORD\\d+') # getting 1st ordinal value. \n",
    "                    multiplier = getLastInt(p1.search(m.group(0)).group(0))\n",
    "                    \n",
    "                    p2 = re.compile('INT\\d+') # getting 1st integer value\n",
    "                    notes = getLastInt(p2.search(m.group(0)).group(0))\n",
    "                    \n",
    "                    startNVal = (multiplier - 1) * notes\n",
    "                    stopNVal = startNVal + notes\n",
    "                    \n",
    "                    # get start measure from 2nd ordinal value\n",
    "                    m2 = p1.search(m.group(0)[m1.span()[1]:len(m.group(0))])\n",
    "                    startMVal = stopMVal = getLastInt(m2.group(0))\n",
    "                \n",
    "                elif matchedValue == 'ORD_NUM_ABSN_OF_ABSM_NUM':\n",
    "                    p1 = re.compile('ORD\\d+') # getting 1st ordinal value. \n",
    "                    multiplier = getLastInt(p1.search(m.group(0)).group(0))\n",
    "                    \n",
    "                    p2 = re.compile('INT\\d+') # getting 1st integer value\n",
    "                    notes = getLastInt(p2.search(m.group(0)).group(0))\n",
    "                    \n",
    "                    startMVal = stopMVal = getMeasureNum(m.group(0))\n",
    "                    startNVal = ((multiplier - 1) * notes) + 1\n",
    "                    stopNVal = startNVal + (notes - 1)\n",
    "                \n",
    "                elif matchedValue == 'ORD_LINK_ORD_OF_ABSM_NUM':\n",
    "                    startMVal = stopMVal = getMeasureNum(m.group(0))\n",
    "                    \n",
    "                    p = re.compile('ORD\\d+') \n",
    "                    m1 = p.search(m.group(0))# getting 1st ordinal value.\n",
    "                    \n",
    "                    startNVal = getLastInt(m1.group(0))\n",
    "                    stopNVal = getLastInt(p.search(m.group(0)[m1.span()[1]:len(m.group(0))]).group(0)) # getting 2nd ordinal value. \n",
    "                    \n",
    "                elif matchedValue == 'ORD_LINK_ORD_ABSM':\n",
    "                    p = re.compile('ORD\\d+') \n",
    "                    m1 = p.search(m.group(0))# getting 1st ordinal value.\n",
    "                    startMVal = getLastInt(m1.group(0))\n",
    "                    stopMVal = getLastInt(p.search(m.group(0)[m1.span()[1]:len(m.group(0))]).group(0)) + 1# getting 2nd ordinal value.\n",
    "                    startNVal = stopNVal = 0\n",
    "                    \n",
    "                elif matchedValue == 'ORD_LINK_ORD_RELN':\n",
    "                    p = re.compile('ORD\\d+') \n",
    "                    m1 = p.search(m.group(0))# getting 1st ordinal value.\n",
    "                    \n",
    "                    startMVal = stopMVal = lastCommand[3]\n",
    "                    startNVal = getLastInt(m1.group(0))\n",
    "                    stopNVal = getLastInt(p.search(m.group(0)[m1.span()[1]:len(m.group(0))]).group(0)) + 1 # getting 2nd ordinal value.\n",
    "                \n",
    "                elif matchedValue == 'ORD_NUM_ABSM':\n",
    "                    p1 = re.compile('ORD\\d+') # getting 1st ordinal value. \n",
    "                    multiplier = getLastInt(p1.search(m.group(0)).group(0))\n",
    "                    \n",
    "                    p2 = re.compile('INT\\d+') # getting 1st integer value\n",
    "                    measures = getLastInt(p2.search(m.group(0)).group(0))\n",
    "                    \n",
    "                    startMVal = ((multiplier - 1) * measures) + 1\n",
    "                    stopMVal = startMVal + (measures - 1)\n",
    "                    startNVal = stopNVal = 0\n",
    "                \n",
    "                elif matchedValue == 'ORD_NUM_ABSN':\n",
    "                    p1 = re.compile('ORD\\d+') # getting 1st ordinal value. \n",
    "                    multiplier = getLastInt(p1.search(m.group(0)).group(0))\n",
    "                    \n",
    "                    p2 = re.compile('INT\\d+') # getting 1st integer value\n",
    "                    notes = getLastInt(p2.search(m.group(0)).group(0))\n",
    "                    \n",
    "                    startMVal = stopMVal = lastCommand[3]\n",
    "                    startNVal = ((multiplier - 1) * notes) + 1\n",
    "                    stopNVal = startNVal + (notes - 1)\n",
    "                \n",
    "                elif matchedValue == 'ORD_ABSM':\n",
    "                    startMVal = getMeasureOrd(m.group(0))\n",
    "                    stopMVal = startMVal + 1\n",
    "                    startNVal = stopNVal = 0\n",
    "                    \n",
    "                elif matchedValue == 'ORD_RELN':\n",
    "                    startMVal = stopMVal = lastCommand[3]\n",
    "                    startNVal = getNoteOrd(m.group(0))\n",
    "                    stopNVal = startNVal + 1\n",
    "                \n",
    "                break\n",
    "        \n",
    "    if hlMatch == False:\n",
    "        # matches misc values\n",
    "        compiled = compileIt(getRegExp('miscTokens'))\n",
    "        for regularExpression, matchedValue in compiled:\n",
    "            m = regularExpression.search(tokenized)\n",
    "            \n",
    "            if m:\n",
    "                hlMatch = True\n",
    "                \n",
    "                if matchedValue == 'ABSM_ABSN':\n",
    "                    startMVal = stopMVal = getMeasureNum(m.group(0))\n",
    "                    startNVal = getNoteNum(m.group(0))\n",
    "                    stopNVal = startNVal + 1\n",
    "                \n",
    "                elif matchedValue == 'ABSM_NUM':\n",
    "                    startMVal = stopMVal = getMeasureNum(m.group(0))\n",
    "                    #stopMVal = startMVal + 1\n",
    "                    startNVal = stopNVal = 0\n",
    "                    \n",
    "                elif matchedValue == 'RELN_NUM':\n",
    "                    startMVal = stopMVal = lastCommand[3]\n",
    "                    startNVal = stopNVal = getMeasureNum(m.group(0))\n",
    "                    #stopNVal = startNVal + 1\n",
    "                    \n",
    "                elif matchedValue == 'ABSS':\n",
    "                    startMVal = 0\n",
    "                    startNVal = 0\n",
    "                    stopMVal = 100\n",
    "                    stopNVal = 100\n",
    "                    \n",
    "                elif matchedValue == 'RELM':\n",
    "                    relNVal = nNum = relMVal = 0\n",
    "                    mNum = 1\n",
    "                    \n",
    "                elif matchedValue == 'RELN':\n",
    "                    relMVal = mNum = relNVal = 0\n",
    "                    nNum = 1\n",
    "\n",
    "                break\n",
    "    \n",
    "    if hlMatch == False:\n",
    "        if navCommand == 'REPLAY':\n",
    "            startMVal = lastCommand[1]\n",
    "            startNVal = lastCommand[2]\n",
    "            stopMVal = lastCommand[3]\n",
    "            stopNVal = lastCommand[4]\n",
    "            relMVal = lastCommand[5]\n",
    "            relNVal = lastCommand[6]\n",
    "            mNum = lastCommand[7]\n",
    "            nNum = lastCommand[8]\n",
    "                  \n",
    "    print([nxtAct, [startMVal, startNVal, stopMVal, stopNVal, relMVal, relNVal, mNum, nNum]])\n",
    "    lastCommand = [nxtAct, startMVal, startNVal, stopMVal, stopNVal, relMVal, relNVal, mNum, nNum]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x. Convert Scores For Front End Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#-------------------------------------------------------------------------------\n",
    "# Name:         tinyNotation.py\n",
    "# Purpose:      A simple notation input format.\n",
    "#\n",
    "# Authors:      Michael Scott Cuthbert\n",
    "#\n",
    "# Copyright:    Copyright © 2009-2012, 2015 Michael Scott Cuthbert and the music21 Project\n",
    "# License:      LGPL or BSD, see license.txt\n",
    "#-------------------------------------------------------------------------------\n",
    "'''\n",
    "    Module translates from M21 stream object to tiny notation, reversing processing\n",
    "    found in convert_m21_to_tiny.py. Specifcally, it writes:\n",
    "    \n",
    "    1. Note objects to text: a,b,c,d,e,f,g and r for rest\n",
    "    \n",
    "    2. Flats, sharps, and naturals as -, # and n (latter only if specifically designated).\n",
    "    If the accidental is above the staff (i.e., editorial), enclose it in\n",
    "    parentheses: (#), etc.  Make sure that flats in the key signatures are\n",
    "    explicitly specified.\n",
    "    \n",
    "    3. Octaves as:\n",
    "        a. CC to BB = from C below bass clef to second-line B in bass clef\n",
    "        b. C to B = from bass clef C to B below middle C.\n",
    "        c. c  to b = from middle C to the middle of treble clef\n",
    "        d. c' to b' = from C in treble clef to B above treble clef\n",
    "        e. CCC... and c'' for octaves below and above.\n",
    "    \n",
    "    4. Note length as: 1 = whole note, 2 = half, 4 = quarter, 8 = eighth, 16 = sixteenth.\n",
    "    etc.  [If the number is omitted then it is assumed to be the same\n",
    "    as the previous note.  I.e., c8 B c d  is a string of eighth notes.]\n",
    "    \n",
    "    5. Ties as ~ after number.\n",
    "    \n",
    "    6. Dotted notes as \".\".\n",
    "    \n",
    "    7. Triplets and quads as: `trip{c4 d8}` and `quad{c16 d e8}`.\n",
    "    \n",
    "    8. Instruments as 'i<instrumentName>'.\n",
    "    \n",
    "    9. Key Signature as 'ks<positive num for sharps, negative num for flats>'\n",
    "    \n",
    "    10. Treble / Bass clef as 'ts' / 'bs'\n",
    "    \n",
    "    11. Time Signature as 'num/num'\n",
    "    \n",
    "    12. Chord as 'chord{<notes as above separated by spaces}'\n",
    "    \n",
    "    13. Metronome as 'me<num>'\n",
    "    \n",
    "'''\n",
    "\n",
    "import unittest\n",
    "import copy\n",
    "import re\n",
    "import sre_parse\n",
    "\n",
    "from music21 import note\n",
    "from music21 import duration\n",
    "from music21 import common\n",
    "from music21 import exceptions21\n",
    "from music21 import stream\n",
    "from music21 import tie\n",
    "from music21 import expressions\n",
    "from music21 import meter\n",
    "from music21 import pitch\n",
    "from music21 import chord # Downs, 16 Nov 2016\n",
    "from music21 import tempo # Downs, 16 Nov 2016\n",
    "from music21 import clef # Downs, 16 Nov 2016\n",
    "from music21 import key # Downs, 16 Nov 2016\n",
    "from music21 import instrument # Downs, 16 Nov 2016\n",
    "\n",
    "from music21 import environment\n",
    "_MOD = \"tinyNotation.py\"\n",
    "environLocal = environment.Environment(_MOD)\n",
    "\n",
    "class TinyNotationException(exceptions21.Music21Exception):\n",
    "    pass\n",
    "\n",
    "class State(object):\n",
    "    '''\n",
    "        State tokens apply something to\n",
    "        every note found within it.\n",
    "        '''\n",
    "    autoExpires = False # expires after N tokens or never.\n",
    "    \n",
    "    def __init__(self, parent, stateInfo):\n",
    "        self.affectedTokens = []\n",
    "        self.parent = common.wrapWeakref(parent)\n",
    "        self.stateInfo = stateInfo\n",
    "    \n",
    "    def start(self):\n",
    "        '''\n",
    "            called when the state is initiated\n",
    "            '''\n",
    "        pass\n",
    "    \n",
    "    def end(self):\n",
    "        '''\n",
    "            called just after removing state\n",
    "            '''\n",
    "        pass\n",
    "    \n",
    "    def affectTokenBeforeParse(self, tokenStr):\n",
    "        '''\n",
    "            called to modify the string of a token.\n",
    "            '''\n",
    "        return tokenStr\n",
    "    \n",
    "    def affectTokenAfterParseBeforeModifiers(self, m21Obj):\n",
    "        '''\n",
    "            called after the object has been acquired but before modifiers have been applied.\n",
    "            '''\n",
    "        return m21Obj\n",
    "    \n",
    "    def affectTokenAfterParse(self, m21Obj):\n",
    "        '''\n",
    "            called to modify the tokenObj after parsing\n",
    "            \n",
    "            tokenObj may be None if another state has deleted it.\n",
    "            '''\n",
    "        self.affectedTokens.append(m21Obj)\n",
    "        if self.autoExpires is not False:\n",
    "            if len(self.affectedTokens) == self.autoExpires:\n",
    "                self.end()\n",
    "                p = common.unwrapWeakref(self.parent)\n",
    "                for i in range(len(p.activeStates)):\n",
    "                    backCount = -1 * (i+1)\n",
    "                    if p.activeStates[backCount] is self:\n",
    "                        p.activeStates.pop(backCount)\n",
    "                        break\n",
    "        return m21Obj\n",
    "\n",
    "class TieState(State):\n",
    "    '''\n",
    "        A TieState is an autoexpiring state that applies a tie start to this note and a\n",
    "        tie stop to the next note.\n",
    "        '''\n",
    "    autoExpires = 2\n",
    "    \n",
    "    def end(self):\n",
    "        '''\n",
    "            end the tie state by applying tie ties to the appropriate notes\n",
    "            '''\n",
    "        if self.affectedTokens[0].tie is None:\n",
    "            self.affectedTokens[0].tie = tie.Tie('start')\n",
    "        else:\n",
    "            self.affectedTokens[0].tie.type = 'continue'\n",
    "        if len(self.affectedTokens) > 1: # could be end...\n",
    "            self.affectedTokens[1].tie = tie.Tie('stop')\n",
    "\n",
    "\n",
    "class TupletState(State):\n",
    "    '''\n",
    "        a tuplet state applies tuplets to notes while parsing and sets 'start' and 'stop'\n",
    "        on the first and last note when end is called.\n",
    "        '''\n",
    "    actual = 3\n",
    "    normal = 2\n",
    "    \n",
    "    def end(self):\n",
    "        '''\n",
    "            end a tuplet by putting start on the first note and stop on the last.\n",
    "            '''\n",
    "        if len(self.affectedTokens) == 0:\n",
    "            return\n",
    "        self.affectedTokens[0].duration.tuplets[0].type = 'start'\n",
    "        self.affectedTokens[-1].duration.tuplets[0].type = 'stop'\n",
    "    \n",
    "    \n",
    "    def affectTokenAfterParse(self, n):\n",
    "        '''\n",
    "            puts a tuplet on the note\n",
    "            '''\n",
    "        super(TupletState, self).affectTokenAfterParse(n)\n",
    "        newTup = duration.Tuplet()\n",
    "        newTup.durationActual = duration.durationTupleFromTypeDots(n.duration.type, 0)\n",
    "        newTup.durationNormal = duration.durationTupleFromTypeDots(n.duration.type, 0)\n",
    "        newTup.numberNotesActual = self.actual\n",
    "        newTup.numberNotesNormal = self.normal\n",
    "        n.duration.appendTuplet(newTup)\n",
    "        return n\n",
    "\n",
    "class TripletState(TupletState):\n",
    "    '''\n",
    "        a 3:2 tuplet\n",
    "        '''\n",
    "    actual = 3\n",
    "    normal = 2\n",
    "\n",
    "class QuadrupletState(TupletState):\n",
    "    '''\n",
    "        a 4:3 tuplet\n",
    "        '''\n",
    "    actual = 4\n",
    "    normal = 3\n",
    "\n",
    "class ChordState(State): # Downs, 16 Nov 2016\n",
    "    '''\n",
    "        <music21.chord.Chord G#4 F4 B-3>\n",
    "        '''\n",
    "    def start(self):\n",
    "        global inChord; inChord = True\n",
    "    \n",
    "    def end(self):\n",
    "        '''\n",
    "            end a chord by combining notes into chord.Chord obj.\n",
    "            '''\n",
    "        if len(self.affectedTokens) == 0:\n",
    "            return\n",
    "        \n",
    "        maxDur = 0\n",
    "        \n",
    "        for token in self.affectedTokens:\n",
    "            if token.duration > maxDur:\n",
    "                maxDur = token.duration\n",
    "    \n",
    "        global inChord; inChord = False\n",
    "    \n",
    "        return chord.Chord(self.affectedTokens, duration = maxDur)\n",
    "\n",
    "\n",
    "class Modifier(object):\n",
    "    '''\n",
    "        a modifier is something that changes the current\n",
    "        token, like setting the Id or Lyric.\n",
    "        '''\n",
    "    def __init__(self, modifierData, modifierString, parent):\n",
    "        self.modifierData = modifierData\n",
    "        self.modifierString = modifierString\n",
    "        self.parent = common.wrapWeakref(parent)\n",
    "    \n",
    "    def preParse(self, tokenString):\n",
    "        '''\n",
    "            called before the tokenString has been\n",
    "            turned into an object\n",
    "            '''\n",
    "        pass\n",
    "    \n",
    "    def postParse(self, m21Obj):\n",
    "        '''\n",
    "            called after the tokenString has been\n",
    "            truend into an m21Obj.  m21Obj may be None\n",
    "            '''\n",
    "        pass\n",
    "\n",
    "\n",
    "class IdModifier(Modifier):\n",
    "    '''\n",
    "        sets the .id of the m21Obj, called with =\n",
    "        '''\n",
    "    def postParse(self, m21Obj):\n",
    "        if hasattr(m21Obj, 'id'):\n",
    "            m21Obj.id = self.modifierData\n",
    "\n",
    "class LyricModifier(Modifier):\n",
    "    '''\n",
    "        sets the .lyric of the m21Obj, called with _\n",
    "        '''\n",
    "    def postParse(self, m21Obj):\n",
    "        if hasattr(m21Obj, 'lyric'):\n",
    "            m21Obj.lyric = self.modifierData\n",
    "\n",
    "class StarModifier(Modifier):\n",
    "    '''\n",
    "        does nothing, but easily subclassed.  Uses *...* to make it happen\n",
    "        '''\n",
    "    pass\n",
    "\n",
    "\n",
    "class Token(object):\n",
    "    '''\n",
    "        A single token made from the parser.\n",
    "        \n",
    "        Call .parse(parent) to make it work.\n",
    "        '''\n",
    "    def __init__(self, token=\"\"):\n",
    "        self.token = token\n",
    "    \n",
    "    def parse(self, parent):\n",
    "        '''\n",
    "            do NOT store parent -- probably\n",
    "            too slow\n",
    "            '''\n",
    "        pass\n",
    "\n",
    "\n",
    "class TimeSignatureToken(Token):\n",
    "    '''\n",
    "        Represents a single time signature, like 1/4\n",
    "        '''\n",
    "    def parse(self, parent):\n",
    "        tsObj = meter.TimeSignature(self.token)\n",
    "        parent.stateDict['currentTimeSignature'] = tsObj\n",
    "        return tsObj\n",
    "\n",
    "# Downs, 16 Nov 2016\n",
    "class KeySignatureToken(Token):\n",
    "    '''\n",
    "        Represents a key signature indicating number of sharps(+) or flats(-)\n",
    "        '''\n",
    "    def parse(self, parent):\n",
    "        mObj = key.KeySignature(int(self.token))\n",
    "        parent.stateDict['currentKeySignature'] = mObj\n",
    "        return mObj\n",
    "\n",
    "# Downs, 16 Nov 2016\n",
    "class MetronomeToken(Token):\n",
    "    '''\n",
    "        Represents a metronome setting, like 80\n",
    "        '''\n",
    "    def parse(self, parent):\n",
    "        mObj = tempo.MetronomeMark(number=float(self.token))\n",
    "        parent.stateDict['currentMetronome'] = mObj\n",
    "        return mObj\n",
    "\n",
    "# Downs, 16 Nov 2016\n",
    "class TrebleClefToken(Token):\n",
    "    '''\n",
    "        Represents a treble clef setting\n",
    "        '''\n",
    "    def parse(self, parent):\n",
    "        mObj = clef.TrebleClef()\n",
    "        parent.stateDict['currentTrebleClef'] = mObj\n",
    "        return mObj\n",
    "\n",
    "# Downs, 16 Nov 2016\n",
    "class BassClefToken(Token):\n",
    "    '''\n",
    "        Represents a bass cleff setting\n",
    "        '''\n",
    "    def parse(self, parent):\n",
    "        mObj = clef.BassClef()\n",
    "        parent.stateDict['currentBassClef'] = mObj\n",
    "        return mObj\n",
    "\n",
    "# Downs, 16 Nov 2016\n",
    "class InstrumentToken(Token):\n",
    "    '''\n",
    "        Represents an instrument name\n",
    "        '''\n",
    "    def parse(self, parent):\n",
    "        mObj = instrument.fromString(self.token)\n",
    "        parent.stateDict['currentInstrument'] = mObj\n",
    "        return mObj\n",
    "\n",
    "class NoteOrRestToken(Token):\n",
    "    '''\n",
    "        represents a Note or Rest.  Chords are represented by Note objects\n",
    "        '''\n",
    "    def __init__(self, token=\"\"):\n",
    "        super(NoteOrRestToken, self).__init__(token)\n",
    "        self.durationMap = [\n",
    "                            (r'(\\d+)', 'durationType'),\n",
    "                            (r'(\\.+)', 'dots'),\n",
    "        ]  ## tie later...\n",
    "        self.durationFound = False\n",
    "    \n",
    "    def applyDuration(self, n, t, parent):\n",
    "        '''\n",
    "            takes the information in the string `t` and creates a Duration object for the\n",
    "            note or rest `n`.\n",
    "            '''\n",
    "        for pm, method in self.durationMap:\n",
    "            searchSuccess = re.search(pm, t)\n",
    "            if searchSuccess:\n",
    "                callFunc = getattr(self, method)\n",
    "                t = callFunc(n, searchSuccess, pm, t, parent)\n",
    "        \n",
    "        if self.durationFound is False and hasattr(parent, 'stateDict'):\n",
    "            n.duration.quarterLength = parent.stateDict['lastDuration']\n",
    "        \n",
    "        # do this by quarterLength here, so that applied tuplets do not persist.\n",
    "        if hasattr(parent, 'stateDict'):\n",
    "            parent.stateDict['lastDuration'] = n.duration.quarterLength\n",
    "        \n",
    "        return t\n",
    "    \n",
    "    def durationType(self, n, search, pm, t, parent):\n",
    "        '''\n",
    "            The result of a successful search for a duration type: puts a Duration in the right place.\n",
    "            '''\n",
    "        \n",
    "        self.durationFound = True\n",
    "        typeNum = int(search.group(1))\n",
    "        if typeNum == 0:\n",
    "            if parent.stateDict['currentTimeSignature'] is not None:\n",
    "                n.duration = copy.deepcopy(parent.stateDict['currentTimeSignature'].barDuration)\n",
    "                n.expressions.append(expressions.Fermata())\n",
    "        else:\n",
    "            n.duration.type = duration.typeFromNumDict[typeNum]\n",
    "        t = re.sub(pm, '', t)\n",
    "        return t\n",
    "    \n",
    "    def dots(self, n, search, pm, t, parent):\n",
    "        '''\n",
    "            adds the appropriate number of dots to the right place.\n",
    "            \n",
    "            Subclassed in TrecentoNotation where two dots has a different meaning.\n",
    "            '''\n",
    "        n.duration.dots = len(search.group(1))\n",
    "        t = re.sub(pm, '', t)\n",
    "        return t\n",
    "\n",
    "\n",
    "class RestToken(NoteOrRestToken):\n",
    "    '''\n",
    "        A token starting with 'r', representing a rest.\n",
    "        '''\n",
    "    def parse(self, parent=None):\n",
    "        r = note.Rest()\n",
    "        self.applyDuration(r, self.token, parent)\n",
    "        return r\n",
    "\n",
    "class NoteToken(NoteOrRestToken):\n",
    "    '''\n",
    "        A NoteToken represents a single Note with pitch\n",
    "        \n",
    "        >>> c3 = tinyNotation.NoteToken('C')\n",
    "        >>> c3\n",
    "        <music21.tinyNotation.NoteToken object at 0x10b07bf98>\n",
    "        >>> n = c3.parse()\n",
    "        >>> n\n",
    "        <music21.note.Note C>\n",
    "        >>> n.nameWithOctave\n",
    "        'C3'\n",
    "        \n",
    "        >>> bFlat6 = tinyNotation.NoteToken(\"b''-\")\n",
    "        >>> bFlat6\n",
    "        <music21.tinyNotation.NoteToken object at 0x10b07bf98>\n",
    "        >>> n = bFlat6.parse()\n",
    "        >>> n\n",
    "        <music21.note.Note B->\n",
    "        >>> n.nameWithOctave\n",
    "        'B-6'\n",
    "        \n",
    "        '''\n",
    "    pitchMap = [\n",
    "                (r'([A-G]+)', 'lowOctave'),\n",
    "                (r'([a-g])(\\'*)', 'highOctave'),\n",
    "                (r'\\(([\\#\\-n]+)\\)(.*)', 'editorialAccidental'),\n",
    "                (r'(\\#+)', 'sharps'),\n",
    "                (r'(\\-+)', 'flats'),\n",
    "                (r'(n)', 'natural'),\n",
    "    ]\n",
    "    def __init__(self, token=\"\"):\n",
    "        super(NoteToken, self).__init__(token)\n",
    "        self.isEditorial = False\n",
    "\n",
    "    def parse(self, parent=None):\n",
    "        '''\n",
    "        Extract the pitch from the note.\n",
    "        '''\n",
    "        t = self.token\n",
    "                \n",
    "        n = note.Note()\n",
    "        t = self.getPitch(n, t)\n",
    "        if parent:\n",
    "            self.applyDuration(n, t, parent)\n",
    "        return n\n",
    "\n",
    "    def getPitch(self, n, t):\n",
    "        for pm, method in self.pitchMap:\n",
    "            searchSuccess = re.search(pm, t)\n",
    "            if searchSuccess:\n",
    "                callFunc = getattr(self, method)\n",
    "                t = callFunc(n, searchSuccess, pm, t)\n",
    "        return t\n",
    "\n",
    "    def editorialAccidental(self, n, search, pm, t):\n",
    "        '''\n",
    "        indicates that the accidental is in parentheses, so set it up to be stored in ficta.\n",
    "        '''\n",
    "        self.isEditorial = True\n",
    "        t = search.group(1) + search.group(2)\n",
    "        return t\n",
    "\n",
    "    def _addAccidental(self, n, alter, pm, t):\n",
    "        '''\n",
    "        helper function for all accidental types.\n",
    "        '''\n",
    "        acc = pitch.Accidental(alter)\n",
    "        if self.isEditorial:\n",
    "            n.editorial.ficta = acc\n",
    "        else:\n",
    "            n.pitch.accidental = acc\n",
    "        t = re.sub(pm, '', t)\n",
    "        return t\n",
    "\n",
    "    def sharps(self, n, search, pm, t):\n",
    "        '''\n",
    "        called when one or more sharps have been found.\n",
    "        '''\n",
    "        alter = len(search.group(1))\n",
    "        return self._addAccidental(n, alter, pm, t)\n",
    "\n",
    "    def flats(self, n, search, pm, t):\n",
    "        '''\n",
    "        called when one or more flats have been found.\n",
    "        '''\n",
    "        alter = -1 * len(search.group(1))\n",
    "        return self._addAccidental(n, alter, pm, t)\n",
    "\n",
    "    def natural(self, n, search, pm, t):\n",
    "        '''\n",
    "        called when an explicit natural has been found.  All pitches are natural without\n",
    "        being specified, so not needed.\n",
    "        '''\n",
    "        return self._addAccidental(n, 0, pm, t)\n",
    "\n",
    "    def lowOctave(self, n, search, pm, t):\n",
    "        '''\n",
    "        Called when a note of octave 3 or below is encountered.\n",
    "        '''\n",
    "        stepName = search.group(1)[0].upper()\n",
    "        octaveNum = 4 - len(search.group(1))\n",
    "        n.step = stepName\n",
    "        n.octave = octaveNum\n",
    "        t = re.sub(pm, '', t)\n",
    "        return t\n",
    "\n",
    "    def highOctave(self, n, search, pm, t):\n",
    "        '''\n",
    "        Called when a note of octave 4 or higher is encountered.\n",
    "            '''\n",
    "        stepName = search.group(1)[0].upper()\n",
    "        octaveNum = 4 + len(search.group(2))\n",
    "        n.step = stepName\n",
    "        n.octave = octaveNum\n",
    "        t = re.sub(pm, '', t)\n",
    "        return t\n",
    "\n",
    "\n",
    "class Converter(object):\n",
    "    '''\n",
    "        Main conversion object for TinyNotation.\n",
    "        \n",
    "        Accepts one keyword: makeNotation=False to get \"classic\" TinyNotation formats.\n",
    "        \n",
    "        '''\n",
    "    def __init__(self, stringRep = \"\", **keywords):\n",
    "        self.stateMap = [\n",
    "                         (r'trip\\{', TripletState),\n",
    "                         (r'quad\\{', QuadrupletState),\n",
    "                         (r'\\~', TieState),\n",
    "                         (r'chord\\{', ChordState)\n",
    "                         ]\n",
    "            \n",
    "        self.endState = re.compile(r'\\}$')\n",
    "                         \n",
    "        self.tokenMap = [\n",
    "                        (r'in(\\S*)', InstrumentToken), # NEW\n",
    "                        (r'ks(\\S*)', KeySignatureToken), # NEW\n",
    "                        (r'tc', TrebleClefToken), # NEW\n",
    "                        (r'bc', BassClefToken), # NEW\n",
    "                        (r'(\\d+\\/\\d+)', TimeSignatureToken),\n",
    "                        (r'me(\\S*)', MetronomeToken),# NEW\n",
    "                        (r'r(\\S*)', RestToken),\n",
    "                        (r'(\\S*)', NoteToken), # last\n",
    "                        ]\n",
    "                         \n",
    "        self.modifierMap = [\n",
    "                            (r'\\=([A-Za-z0-9]*)', IdModifier), # so, \"=\" assigns id or name to object\n",
    "                            (r'_(.*)', LyricModifier),\n",
    "                            (r'\\*(.*)\\*', StarModifier),\n",
    "                            ]\n",
    "                         \n",
    "        self.keywords = keywords\n",
    "        if 'makeNotation' in keywords:\n",
    "            self.makeNotation = keywords['makeNotation']\n",
    "        else:\n",
    "            self.makeNotation = True\n",
    "                                 \n",
    "        self.stream = stream.Part()\n",
    "        self.stateDict = {'currentTimeSignature': None,'lastDuration': 1.0}\n",
    "        self.stringRep = stringRep\n",
    "        #self.regexps = {}\n",
    "        self.activeStates = []\n",
    "        self.preTokens = [] # space-separated strings\n",
    "            \n",
    "        self._stateMapRe = None\n",
    "        self._tokenMapRe = None\n",
    "        self._modifierMapRe = None\n",
    "                                                                     \n",
    "        global inChord; inChord = False\n",
    "\n",
    "\n",
    "    def splitPreTokens(self):\n",
    "        '''\n",
    "        splits the string into textual tokens.\n",
    "        \n",
    "        Right now just splits on spaces, but might be smarter to ignore spaces in\n",
    "        quotes, etc. later.\n",
    "        '''\n",
    "        self.preTokens = self.stringRep.split() # do something better...\n",
    "\n",
    "    def setupRegularExpressions(self):\n",
    "        '''\n",
    "        Regular expressions get compiled for faster usage.\n",
    "        '''\n",
    "        self._stateMapRe = []\n",
    "        for rePre, classCall in self.stateMap:\n",
    "            try:\n",
    "                self._stateMapRe.append( (re.compile(rePre), classCall) )\n",
    "            except sre_parse.error as e:\n",
    "                raise TinyNotationException(\"Error in compiling state, %s: %s\" % (rePre, str(e)))\n",
    "\n",
    "        self._tokenMapRe = []\n",
    "        for rePre, classCall in self.tokenMap:\n",
    "            try:\n",
    "                self._tokenMapRe.append( (re.compile(rePre), classCall) )\n",
    "            except sre_parse.error as e:\n",
    "                raise TinyNotationException(\"Error in compiling token, %s: %s\" % (rePre, str(e)))\n",
    "\n",
    "        self._modifierMapRe = []\n",
    "        for rePre, classCall in self.modifierMap:\n",
    "            try:\n",
    "                self._modifierMapRe.append( (re.compile(rePre), classCall) )\n",
    "            except sre_parse.error as e:\n",
    "                raise TinyNotationException(\"Error in compiling modifier, %s: %s\" % (rePre, str(e)))\n",
    "\n",
    "    def parse(self):\n",
    "        '''\n",
    "        splitPreTokens, setupRegularExpressions, then run through each preToken, and run postParse.\n",
    "        '''\n",
    "        if self.preTokens == [] and self.stringRep != \"\":\n",
    "            self.splitPreTokens()\n",
    "            print(self.splitPreTokens)\n",
    "        if self._tokenMapRe is None:\n",
    "            self.setupRegularExpressions()\n",
    "                        \n",
    "        for i, t in enumerate(self.preTokens):\n",
    "            self.parseOne(i, t)\n",
    "                                \n",
    "        self.postParse()\n",
    "        return self\n",
    "\n",
    "    def parseOne(self, i, t):\n",
    "        '''\n",
    "        parse a single token at position i, with text t.\n",
    "        \n",
    "        Checks for state changes, modifiers, tokens, and end-state brackets.\n",
    "        '''\n",
    "        endBrackets = 0\n",
    "            \n",
    "        for s, c in self._stateMapRe:\n",
    "            matchSuccess = s.search(t)\n",
    "            if matchSuccess is not None:\n",
    "                stateData = matchSuccess.group(0)\n",
    "                t = s.sub('', t)\n",
    "                stateObj = c(self, stateData)\n",
    "                stateObj.start()\n",
    "                self.activeStates.append(stateObj)\n",
    "\n",
    "        while self.endState.search(t):\n",
    "            t = self.endState.sub('', t)\n",
    "            endBrackets += 1\n",
    "                                                                                                    \n",
    "        modifiers = []\n",
    "        for m, c in self._modifierMapRe:\n",
    "            matchSuccess = m.search(t)\n",
    "            if matchSuccess is not None:\n",
    "                modifierData = matchSuccess.group(1)\n",
    "                t = m.sub('', t)\n",
    "                modObj = c(modifierData, t, self)\n",
    "                modifiers.append(modObj)\n",
    "\n",
    "        for mObj in modifiers:\n",
    "            mObj.preParse(t)\n",
    "                                                                                                                                                \n",
    "        for s in self.activeStates[:]:\n",
    "            t = s.affectTokenBeforeParse(t)\n",
    "            \n",
    "        # now that we have token, state and any pre-parse modifiers, we can process tokens\n",
    "        m21Obj = None\n",
    "        tokenObj = None\n",
    "            \n",
    "        # parse token...with state...\n",
    "        for tokenRe, c in self._tokenMapRe:\n",
    "            matchSuccess = tokenRe.search(t)\n",
    "            if matchSuccess is not None:\n",
    "                try:                                                                                                                                                                                        tokenData = matchSuccess.group(1)\n",
    "                except:\n",
    "                    tokenData = matchSuccess.group(0)\n",
    "                tokenObj = c(tokenData)\n",
    "                m21Obj = tokenObj.parse(self)\n",
    "\n",
    "                if m21Obj is not None:\n",
    "                    break\n",
    "                                                                                                                                                                                                                                        \n",
    "        for s in self.activeStates[:]: # iterate over copy so we can remove....\n",
    "            m21Obj = s.affectTokenAfterParseBeforeModifiers(m21Obj)\n",
    "                        \n",
    "        for m in modifiers:\n",
    "            m.postParse(m21Obj)\n",
    "                                                                                                                                                                                                                                                            \n",
    "        for s in self.activeStates[:]: # iterate over copy so we can remove....\n",
    "            m21Obj = s.affectTokenAfterParse(m21Obj)\n",
    "                        \n",
    "        # this is a banged up way of calling end processing only if end bracket is found\n",
    "        for i in range(endBrackets):\n",
    "            stateToRemove = self.activeStates.pop()\n",
    "            tempObj = stateToRemove.end()\n",
    "            if tempObj is not None:\n",
    "                m21Obj = tempObj\n",
    "                                                                                                                                                                                                                                                                                                            \n",
    "        if m21Obj is not None:\n",
    "            if inChord == False:\n",
    "                self.stream._appendCore(m21Obj)\n",
    "\n",
    "    def postParse(self):\n",
    "        '''\n",
    "        Call postParse calls on .stream, currently just .makeMeasures.\n",
    "        '''\n",
    "        if self.makeNotation is not False:\n",
    "            self.stream.makeMeasures(inPlace=True)\n",
    "\n",
    "class Test(unittest.TestCase):\n",
    "    parseTest = \"1/4 trip{C8~ C~_hello C=mine} F~ F~ 2/8 F F# quad{g--16 a## FF(n) g#} g16 F0\"\n",
    "    \n",
    "    def runTest(self):\n",
    "        pass\n",
    "    \n",
    "    def testOne(self):\n",
    "        c = Converter(self.parseTest)\n",
    "        c.parse()\n",
    "        s = c.stream\n",
    "        sfn = s.flat.notes\n",
    "        self.assertEqual(sfn[0].tie.type, 'start')\n",
    "        self.assertEqual(sfn[1].tie.type, 'continue')\n",
    "        self.assertEqual(sfn[2].tie.type, 'stop')\n",
    "        self.assertEqual(sfn[0].step, 'C')\n",
    "        self.assertEqual(sfn[0].octave, 3)\n",
    "        self.assertEqual(sfn[1].lyric, \"hello\")\n",
    "        self.assertEqual(sfn[2].id, \"mine\")\n",
    "        self.assertEqual(sfn[6].pitch.accidental.alter, 1)\n",
    "        self.assertEqual(sfn[7].pitch.accidental.alter, -2)\n",
    "        self.assertEqual(sfn[9].editorial.ficta.alter, 0)\n",
    "        self.assertEqual(sfn[12].duration.quarterLength, 1.0)\n",
    "        self.assertEqual(sfn[12].expressions[0].classes, expressions.Fermata().classes)\n",
    "\n",
    "class TestExternal(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pass\n",
    "    \n",
    "    def testOne(self):\n",
    "        c = Converter(Test.parseTest)\n",
    "        c.parse()\n",
    "        c.stream.show('musicxml.png')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import music21\n",
    "#music21.mainTest(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from soundProcessing import convert_m21_to_tiny as cM2T\n",
    "from soundProcessing import convert_tiny_to_m21 as cT2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(cM2T)\n",
    "reload(cT2M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tup = m21.duration.Tuplet(numberNotesActual = 5, numberNotesNormal = 4)\n",
    "print(type(tup))\n",
    "d = m21.duration.convertTypeToQuarterLength('quarter', 0, [tup])\n",
    "print(type(d))\n",
    "print(type(d) == m21.meter.fractions.Fraction)\n",
    "print(d.numerator)\n",
    "print(d.denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convert XML -> M21 -> Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''convert xml score to tiny notation treble and bass strings using custom function.\n",
    "for conversion code, see soundProcessing/convert_m21_to_tiny.py'''\n",
    "\n",
    "scoreDir = 'data/wip/scores/'\n",
    "inFiles = glob.glob(scoreDir + '*.xml')\n",
    "inFiles = ['data/wip/scores/williams star wars imperial.xml']\n",
    "\n",
    "for inFile in inFiles:\n",
    "    print(inFile)\n",
    "    # import from xml -> m21\n",
    "    toTiny = m21.converter.parse(inFile)\n",
    "    \n",
    "    # collapse voices\n",
    "    bfr, aft = cM2T.collapse_voices(toTiny)\n",
    "    \n",
    "    # create treble and bass tiny strings\n",
    "    treb, bass = cM2T.m21_to_tiny(toTiny)\n",
    "    #print(bfr); print(aft)\n",
    "    \n",
    "    # save strings\n",
    "    fName, _ = os.path.splitext(os.path.basename(inFile))\n",
    "    outFile = open(scoreDir + fName + ' treb.txt', \"w\")\n",
    "    outFile.write(treb); outFile.close()\n",
    "    \n",
    "    outFile = open(scoreDir + fName + ' bass.txt', \"w\")\n",
    "    outFile.write(bass); outFile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toTiny.show('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crd = toTiny[4][13][1]\n",
    "crd.beams.beamsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = toTiny[4][13][1] #.duration.quarterLength\n",
    "b = toTiny[4][13][2]\n",
    "c = toTiny[4][13][3]\n",
    "\n",
    "print(type(a))\n",
    "print(a.duration.quarterLength)\n",
    "print(b.duration.quarterLength)\n",
    "print(c.duration.quarterLength)\n",
    "\n",
    "print(a.duration.quarterLength / (2./3))\n",
    "\n",
    "print(a.duration)\n",
    "\n",
    "print type(a.duration.quarterLength)\n",
    "d = m21.stream.Stream()\n",
    "d.insert(0,a)\n",
    "d.append(b)\n",
    "d.append(c)\n",
    "\n",
    "play_m21(d)\n",
    "d.show()\n",
    "\n",
    "#print(tmp)\n",
    "#print(tmp.numerator)\n",
    "#if type(tmp) == m21.duration.fractions.Fraction:\n",
    "#    print 1\n",
    "#else: \n",
    "#    print 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = m21.note.Note('D')\n",
    "b = m21.note.Note('D')\n",
    "c = m21.note.Note('D')\n",
    "\n",
    "t = m21.duration.Tuplet(3, 2)\n",
    "print(t)\n",
    "d = m21.duration.Duration(0.5)\n",
    "d.appendTuplet(t)\n",
    "print(d)\n",
    "\n",
    "a.duration = d\n",
    "b.duration = d\n",
    "c.duration = d\n",
    "\n",
    "e = m21.stream.Stream()\n",
    "e.insert(0,a)\n",
    "e.append(b)\n",
    "e.append(c)\n",
    "\n",
    "play_m21(e)\n",
    "e.show('text')\n",
    "#;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = m21.duration.Duration()\n",
    "d.quarterLength = 0.5\n",
    "\n",
    "n1 = m21.note.Note('e', duration=d)\n",
    "n2 = m21.note.Note('e', duration=d)\n",
    "n3 = m21.note.Note('a', duration=d)\n",
    "n4 = m21.note.Note('g', duration=d)\n",
    "n5 = m21.note.Note('a', duration=d)\n",
    "\n",
    "crd = m21.chord.Chord([n1,n2])\n",
    "\n",
    "s = m21.stream.Stream()\n",
    "\n",
    "s.insert(0, crd)\n",
    "s.insert(0.5, n3)\n",
    "s.insert(1.0, n4)\n",
    "s.insert(1.5, n5)\n",
    "\n",
    "bo = m21.beam.Beams(); bo.append('start')\n",
    "s[0].beams = bo\n",
    "bo = m21.beam.Beams(); bo.append('continue')\n",
    "s[1].beams = bo\n",
    "bo = m21.beam.Beams(); bo.append('continue')\n",
    "s[2].beams = bo\n",
    "bo = m21.beam.Beams(); bo.append('stop')\n",
    "s[3].beams = bo\n",
    "\n",
    "print(s[0].beams.beamsList)\n",
    "print(s[1].beams.beamsList)\n",
    "print(s[2].beams.beamsList)\n",
    "print(s[3].beams.beamsList)\n",
    "\n",
    "s.show()\n",
    "\n",
    "#chord{e8 e'8} a8%c g8%c a8%e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validate Tiny -> M21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Load xml and tiny scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''convert tiny notation strings back to m21 streams.\n",
    "for code, see soundProcessing/convert_tiny_to_m21.py'''\n",
    "\n",
    "dn = 'data/wip/scores/'\n",
    "fn = 'vivaldi four seasons spring classic medium'\n",
    "baseline = m21.converter.parse(dn + fn + '.xml')\n",
    "\n",
    "tmp = open(dn + fn + ' treb.txt', 'r'); treb = tmp.read()\n",
    "tmp = open(dn + fn + ' bass.txt', 'r'); bass = tmp.read()\n",
    "\n",
    "treb = cT2M.Converter(treb)\n",
    "bass = cT2M.Converter(bass)\n",
    "\n",
    "treb = treb.parse().stream\n",
    "bass = bass.parse().stream\n",
    "\n",
    "'''insert treble and bass streams into new stream object'''\n",
    "converted = m21.stream.Score()\n",
    "converted.insert(0, m21.stream.PartStaff(treb))\n",
    "converted.insert(0, m21.stream.PartStaff(bass))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# next two are prototyping code used in score.py to convert tiny to m21\n",
    "\n",
    "fn = './data/wip/scores/williams star wars main.xml'\n",
    "baseline = m21.converter.parse(fn)\n",
    "\n",
    "# pandas data frame\n",
    "df = np.zeros((0, 4), dtype=float) # dtype defaults to float64\n",
    "df = pd.DataFrame(df, columns=['part', 'measure', 'note', 'offset'])\n",
    "\n",
    "lctr = 0\n",
    "parts = baseline.getElementsByClass('Part')\n",
    "for p in range(len(parts)):\n",
    "    measures = parts[p].getElementsByClass('Measure')\n",
    "    \n",
    "    for m in range(len(measures)):\n",
    "        notes = measures[m].getElementsByClass('GeneralNote')\n",
    "        \n",
    "        nctr = 0\n",
    "        for note in notes:\n",
    "            df.loc[lctr] = [p, m, nctr, note.offset]\n",
    "            \n",
    "            ctr = ctr+1\n",
    "            lctr=lctr+1\n",
    "\n",
    "print(df.loc[0:5])\n",
    "print(df.measure.unique())\n",
    "\n",
    "# for each measure\n",
    "mAndN = []\n",
    "for m in df.measure.unique():\n",
    "    print df.loc[df['measure'] == m]\n",
    "    offsets = df.loc[df['measure'] == m].offset.unique()\n",
    "    for x in sorted(offsets):\n",
    "        mAndN.append()\n",
    "    \n",
    "#mAndN_unique = list(set(mAndN))\n",
    "#print(len(mAndN_unique))\n",
    "\n",
    "#mAndN_sort=[]\n",
    "#for x in sorted(mAndN):\n",
    "#    mAndN_sort.append(x)\n",
    "#print(len(mAndN_sort))\n",
    "#print(mAndN_sort)\n",
    "    \n",
    "\n",
    "#nr = baseline.flat.notesAndRests.stream()\n",
    "#nr.show('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parts = baseline.getElementsByClass('Part')\n",
    "print(len(parts))\n",
    "measures = parts[0].getElementsByClass('Measure')\n",
    "print(len(measures))\n",
    "#for measure in measures:\n",
    "#    measure.getElementsByClass('GeneralNote').offset\n",
    "\n",
    "elements = measures[0].getElementsByClass('GeneralNote')\n",
    "\n",
    "for element in elements:\n",
    "    print element.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline.show('text', addEndTimes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline[3][9][1].offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(cT2M)\n",
    "fn = './users/user1/score/williams star wars main.txt'\n",
    "tmp = open(fn, 'r'); scoreText = tmp.read()\n",
    "converted = m21.stream.Score()\n",
    "m = re.search('ncp', scoreText)\n",
    "treb = scoreText[:(m.span()[0]-1)]\n",
    "bass = scoreText[(m.span()[1]+1):]\n",
    "treb = cT2M.Converter(treb)\n",
    "bass = cT2M.Converter(bass) # bass\n",
    "treb = treb.parse().stream\n",
    "bass = bass.parse().stream\n",
    "converted.insert(0, m21.stream.PartStaff(treb))\n",
    "converted.insert(0, m21.stream.PartStaff(bass))\n",
    "converted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(m21.pitch.Pitch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = m21.note.Note('c4')\n",
    "o = m21.note.Note('c3')\n",
    "c1 = m21.chord.Chord([n,o])\n",
    "c2 = m21.chord.Chord(['c2', 'c3', 'c4'])\n",
    "\n",
    "n.color = 'red'\n",
    "o.color = 'green'\n",
    "c2.color = 'red'\n",
    "\n",
    "n.lyric = n.pitch\n",
    "o.lyric = o.pitch\n",
    "\n",
    "tmp=[]\n",
    "for p in c1.pitches:\n",
    "    tmp.append(p.nameWithOctave)\n",
    "c1.lyric = ' '.join(tmp)\n",
    "\n",
    "c2.lyric = c2.pitchedCommonName\n",
    "\n",
    "s = m21.stream.Stream()\n",
    "s.insert(0,n)\n",
    "s.append(o)\n",
    "s.append(c1)\n",
    "s.append(c2)\n",
    "s.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profileDict = {}\n",
    "with open('./users/user1/profile.txt') as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split()\n",
    "       profileDict[key] = val\n",
    "        \n",
    "profileDict['noteNames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline.flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kTmp = baseline.flat.getElementsByClass('Key')\n",
    "ksTmp = baseline.flat.getElementsByClass('keySignature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(kTmp))\n",
    "print(len(ksTmp))\n",
    "dir(kTmp[0])\n",
    "print(kTmp[0].name)\n",
    "print(kTmp[0].sharps)\n",
    "print(kTmp[0].tonicPitchNameWithCase)\n",
    "tmp = m21.key.Key(kTmp[0].tonicPitchNameWithCase)\n",
    "print(tmp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''go back to originally imported xml (i.e, xml->stream) and...'''\n",
    "\n",
    "mStart = 0\n",
    "mEnd = 2\n",
    "\n",
    "play_m21(baseline.measures(mStart,mEnd))\n",
    "baseline.measures(mStart,mEnd).show('text')\n",
    "#print(baseline.measures(mStart,mEnd).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''...compare with result of xml->stream->tiny/collapse->stream'''\n",
    "\n",
    "play_m21(converted.measures(mStart,mEnd))\n",
    "converted.measures(mStart,mEnd).show()\n",
    "\n",
    "# THREE PROBLEMS:\n",
    "# 1. MINE SHOWS ALL THE SHARPS... EVEN THOUGH KEY IS DOUBLE SHARP\n",
    "# 2. MINE ONLY CONNECTS TWO NOTES IN RAPID (0.5 QL) SUCCESSION WHILE THEIRS DOES FOUR.\n",
    "# 3. WHILE M21 CAN HANDLE QUADS AND CROSS-REFERENCES (A LA STAR WARS), MINE CAN'T HANDLE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part = 0\n",
    "frm = 0\n",
    "to = 2\n",
    "#play_m21(s3a.parts[part].measures(frm,to))\n",
    "baseline.parts[part].measures(frm,to).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline.parts[part].measures(frm,to).show('text', addEndTimes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#play_m21(s3a_col.parts[part].measures(frm,to))\n",
    "converted.parts[part].measures(frm,to).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "converted.parts[part].measures(frm,to).show('text', addEndTimes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#play_m21(s3bCol.parts[part].measures(frm,to))\n",
    "s3bCol.parts[part].measures(frm,to).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3bCol.parts[part].measures(frm,to).show('text', addEndTimes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = m21.stream.Stream()\n",
    "#tmp.insert(0,m21.instrument.fromString('Piano'))\n",
    "tmp.insert(0,m21.instrument.Piano())\n",
    "tmp.insert(0.5,m21.note.Note('C4'))\n",
    "tmp.insert(1.0,m21.note.Note('D4'))\n",
    "\n",
    "#tmp.remove(tmp[1])\n",
    "tmp.show('text')\n",
    "\n",
    "tmp.replace(tmp[0], m21.instrument.fromString('Violin'))\n",
    "\n",
    "tmp.show('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = m21.stream.Measure()\n",
    "n1 = m21.note.Note('C4', quarterLength = 0.5)\n",
    "n2 = m21.note.Note('D4', quarterLength = 0.5)\n",
    "n3 = m21.note.Note('E4', quarterLength = 1.0)\n",
    "m.append(n1)\n",
    "m.append(n2)\n",
    "m.append(n3)\n",
    "\n",
    "n1.beams.fill('eighth', type='start')\n",
    "n2.beams.fill('eighth', type='stop')\n",
    "n1.beams\n",
    "n2.beams\n",
    "\n",
    "m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for thisBeam in n2.beams:\n",
    "    print(thisBeam.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = m21.stream.Measure()\n",
    "n1 = m21.note.Note('C4', quarterLength = 0.25)\n",
    "n2 = m21.note.Note('D4', quarterLength = 0.25)\n",
    "n3 = m21.note.Note('E4', quarterLength = 0.5)\n",
    "n4 = m21.note.Note('F4', quarterLength = 1.0)\n",
    "for n in [n1, n2, n3, n4]:\n",
    "    m.append(n)\n",
    "n1.beams.fill('16th', type='start')\n",
    "n2.beams.append('continue')\n",
    "n2.beams.append('stop')\n",
    "n3.beams.fill('eighth', type='stop')\n",
    "\n",
    "m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if n2.beams:\n",
    "    \n",
    "    b = \"%\"\n",
    "\n",
    "    for thisBeam in n2.beams:\n",
    "        if thisBeam.type == \"start\":\n",
    "            b = b + '1'\n",
    "        elif thisBeam.type == \"continue\":\n",
    "            b = b + '2'\n",
    "        elif thisBeam.type == \"stop\":\n",
    "            b = b + '0'\n",
    "        else:\n",
    "            print(\"convert_m21_to_tiny: unrecognized beam type\", thisBeam.type)\n",
    "            end\n",
    "\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = m21.converter.parse('./data/wip/scores/Pachelbel canon classic medium.xml')\n",
    "tmp.show('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp.parts[0].measures(33,35).show('text', addEndTimes=True) # \n",
    "\n",
    "print\n",
    "tmp.parts[0].measures(33,35).flat.show('text', addEndTimes=True)\n",
    "print\n",
    "a = tmp.parts[0].measures(33,35).flat.notes #.show('text', addEndTimes=True)\n",
    "\n",
    "for note in a: print(note.beams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(m21.chord.Chord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = m21.stream.Part()\n",
    "tc = m21.clef.TrebleClef()\n",
    "n1 = m21.note.Note(\"c4\")\n",
    "bc = m21.clef.BassClef()\n",
    "n2 = m21.note.Note(\"C4\")\n",
    "\n",
    "p.append(tc)\n",
    "p.append(n1)\n",
    "p.append(bc)\n",
    "p.append(n2)\n",
    "\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p[0].duration.quarterLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Misc unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parseTest = \"1/4 trip{C8~ C~_hello C=mine} F~ F~ 2/8 F F# quad{g--16 a## FF(n) g#} g16 F0\"\n",
    "\n",
    "c = m21.tinyNotation.Converter(parseTest)\n",
    "print(type(c))\n",
    "s = c.parse().stream\n",
    "print(type(s))\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TinyNotationWriter(object):\n",
    "    \n",
    "    import music21 as m21\n",
    "    \n",
    "    def _durationToTinyNotation(self, dur):\n",
    "        (qLen, durType, dots, tupleNumerator, tupletDenominator, tupletType) = m21.duration.unitSpec(dur)\n",
    "        numberType = m21.duration.convertTypeToNumber(durType)\n",
    "\n",
    "        return '%d' % numberType + '.' * dots\n",
    "\n",
    "\n",
    "    def _noteToTinyNotation(self, noteObj):\n",
    "        if noteObj.octave <= 3:\n",
    "            return noteObj.name[0] * (4 - noteObj.octave) + noteObj.name[1:]\n",
    "        else:\n",
    "            name = noteObj.name.lower()\n",
    "            return name[0] + \"'\" * (noteObj.octave - 4) + name[1:]\n",
    "        \n",
    "    def _clefToTinyNotation(self, clefObj):\n",
    "        if isinstance(clefObj, m21.clef.TrebleClef):\n",
    "            return trebleClefNotation\n",
    "        elif isinstance(clefObj, m21.clef.BassClef):\n",
    "            return bassClefNoation\n",
    "\n",
    "    def streamToTinyNotation(self, streamObj):\n",
    "        l = []\n",
    "        for elem in streamObj.flat:\n",
    "            if isinstance(elem, m21.clef.Clef):\n",
    "                s = self._clefToTinyNotation(elem)\n",
    "                l.append()\n",
    "            elif isinstance(elem, m21.note.Note):\n",
    "                s = self._noteToTinyNotation(elem)\n",
    "                d = self._durationToTinyNotation(elem.duration)\n",
    "                l.append(s + d)\n",
    "            elif isinstance(elem, m21.note.Rest):\n",
    "                s = 'r'\n",
    "                d = self._durationToTinyNotation(elem.duration)\n",
    "                l.append(s + d)\n",
    "            elif isinstance(elem, m21.meter.TimeSignature):\n",
    "                s = elem.ratioString\n",
    "                l.append(s)\n",
    "\n",
    "        return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''duration object '''\n",
    "\n",
    "c1 = m21.chord.Chord(['c4', 'd4', 'f4'])\n",
    "c1.duration.quarterLength = 3\n",
    "\n",
    "print(c1.duration.fullName)\n",
    "print(c1.duration.type)\n",
    "print(c1.duration.quarterLength)\n",
    "print(c1.duration == 3.0)\n",
    "#a = ' '.join(c1.pitchNames)\n",
    "#d = m21.duration.Duration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = m21.note.Note('c4')\n",
    "    \n",
    "d = m21.duration.Duration()\n",
    "d.quarterLength = 0.25 + 0.125\n",
    "print(m21.duration.unitSpec(d))\n",
    "\n",
    "d = m21.duration.Duration('eighth')\n",
    "print(m21.duration.unitSpec(d))\n",
    "\n",
    "d = m21.duration.Duration('eighth')\n",
    "print(m21.duration.unitSpec(d))\n",
    "\n",
    "d = m21.duration.Duration('16th')\n",
    "print(m21.duration.unitSpec(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''original test'''\n",
    "\n",
    "parseTest = \"1/4 trip{C8~ C~_hello C=mine} F~ F~ 2/8 F F# quad{g--16 a## FF(n) g#} g16 F0\"\n",
    "c1 = Converter(parseTest)\n",
    "s1 = c1.parse().stream\n",
    "s1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''manually adding chord i.e., do changes to converter.py work?'''\n",
    "\n",
    "parseTest = \"1/4 trip{C8~ C~_hello C=mine} F~ F~ 2/8 F F# quad{g--16 a## FF(n) g#} g16 F4 chord{F4 D4 BB4}\"\n",
    "c1 = Converter(parseTest)\n",
    "s1 = c1.parse().stream\n",
    "s1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''programatically adding chord i.e., do changes to m21->tiny and tiny->21 work'''\n",
    "\n",
    "parseTest = '1/4 trip{C8~ C8~_hello C8} F8~ F8~ 2/8 F8 F#8 quad{g--16 a##16 FF16 g#16} g16 F4'\n",
    "c2 = m21.tinyNotation.Converter(parseTest)\n",
    "s2 = c2.parse().stream\n",
    "s2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parseTest = m21_to_tiny(s1)\n",
    "c2 = m21.tinyNotation.Converter(parseTest)\n",
    "s2 = c2.parse().stream\n",
    "s2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''playing with makeChords'''\n",
    "\n",
    "tmp1 = m21.note.Note('C5')\n",
    "tmp1.duration = m21.duration.Duration('quarter')\n",
    "\n",
    "tmp2 = m21.note.Note('C5')\n",
    "tmp2.duration = m21.duration.Duration('16th')\n",
    "\n",
    "tmps = m21.stream.Stream()\n",
    "tmps.insert(0,tmp1)\n",
    "tmps.insert(0,tmp2)\n",
    "\n",
    "tmps.makeChords(minimumWindowSize=0.125,\n",
    "                     includePostWindow=True,\n",
    "                     removeRedundantPitches=True,\n",
    "                     useExactOffsets=False,\n",
    "                     gatherArticulations=True,\n",
    "                     gatherExpressions=True,\n",
    "                     inPlace=True, \n",
    "                     makeRests=True)\n",
    "\n",
    "tmps.show('text', addEndTimes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for element in s3a_col.parts[1].measures(1,1).flat.getElementsByClass('Note'):\n",
    "    print(element.pitch)\n",
    "for element in s3a_col.parts[1].measures(1,1).flat.getElementsByClass('Chord'):\n",
    "    print(element.pitchNames)\n",
    "\n",
    "# a.k.a\n",
    "print(s3a_col.parts[1].measures(1,1).pitches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#s3a.parts[0].flat.show('text')\n",
    "for e in s3a.parts[0].flat:\n",
    "    if type(e) == m21.note.Note or type(e) == m21.chord.Chord or type(e) == m21.note.Rest:\n",
    "        print(\"***\")\n",
    "        print(e.offset)\n",
    "        print(e)\n",
    "        print(e.duration)\n",
    "        \n",
    "print(\"***** next *****\")\n",
    "for e in s3a.parts[1].flat:\n",
    "    if type(e) == m21.note.Note or type(e) == m21.chord.Chord or type(e) == m21.note.Rest:\n",
    "        print(\"***\")\n",
    "        print(e.offset)\n",
    "        print(e)\n",
    "        print(e.duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = Converter(\"3/4 c a b chord{c8 a8 d8} c4 a4 b4_Hello trip{a16 b16 c16}\")\n",
    "tmp = tmp.parse().stream\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''this is the fast way of finding clefs in a stream object'''\n",
    "\n",
    "'''however, you don't know if clef change occurs at part or measure level. so, to find where\n",
    "an item (like a clef, instrument, etc) appears in a stream (like a score, part, measure)'''\n",
    "\n",
    "def get_items_in_stream(itemName, s):\n",
    "    \n",
    "    import music21 as m21\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if type(s) == m21.stream.Score:\n",
    "        items = process_score(itemName, s)\n",
    "        items = sum(sum(items, []),[])\n",
    "    \n",
    "    elif type(s) == m21.stream.Part:\n",
    "        pCtr = 'na'\n",
    "        items = process_part(itemName, pCtr, s)\n",
    "        items = sum(items, [])\n",
    "        \n",
    "    elif type(s) == m21.stream.Measure:\n",
    "        pCtr = 'na'\n",
    "        mCtr = 'na'\n",
    "        items = process_measure(itemName, pCtr, mCtr, s)\n",
    "        \n",
    "    else:\n",
    "        print('get_items: stream object has no score, part or measure')\n",
    "        return\n",
    "        \n",
    "    items = np.array(items)\n",
    "    items = pd.DataFrame(items, columns=('part', 'measure', itemName))\n",
    "    return items\n",
    "        \n",
    "def process_score(itemName, s):\n",
    "    items = []\n",
    "    pCtr = 0\n",
    "    for p in s.getElementsByClass('Part'):\n",
    "        tmp = process_part(itemName, pCtr, p)\n",
    "        if tmp: items.append(tmp)\n",
    "        pCtr = pCtr + 1\n",
    "    return(items)\n",
    "    \n",
    "def process_part(itemName, pCtr, p):\n",
    "    items = []\n",
    "    mCtr =0\n",
    "    for m in p.getElementsByClass('Measure'):\n",
    "        tmp = process_measure(itemName, pCtr, mCtr, m)\n",
    "        if tmp: items.append(tmp)\n",
    "        mCtr = mCtr + 1\n",
    "    return(items)\n",
    "    \n",
    "def process_measure(itemName, pCtr, mCtr, m):\n",
    "    items = []\n",
    "    item = m.getElementsByClass(itemName)\n",
    "    if len(item) > 0:\n",
    "        for i in range(len(item)):\n",
    "            if itemName == 'Clef':\n",
    "                items.append((pCtr, mCtr, item[i].sign))\n",
    "            else:\n",
    "                items.append((pCtr, mCtr, itemName))\n",
    "        return(items)\n",
    "        \n",
    "#tmp = get_items_in_stream('Clef', s3)\n",
    "#tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''get overlaps... basically finds any notes that overlap'''\n",
    "\n",
    "a = stream.Stream()\n",
    "for x in [0,0,0,0,3,3,3]:\n",
    "    n = note.Note('G#')\n",
    "    n.duration = duration.Duration('whole')\n",
    "    n.offset = x\n",
    "    a.insert(n)\n",
    "\n",
    "# default is to not include coincident boundaries\n",
    "d = a.getOverlaps()\n",
    "print(len(d[0]))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''group elements by offset'''\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# group.. picks up non-note, rest and chord elements.\n",
    "# so, need to pre-screen\n",
    "\n",
    "s = stream.Stream()\n",
    "s.insert(3, note.Note('C'))\n",
    "s.insert(4, note.Note('C#'))\n",
    "s.insert(4, chord.Chord(['D-', 'C', 'G']))\n",
    "s.insert(4, note.Rest())\n",
    "s.insert(16.0/3, note.Note('D'))\n",
    "s.insert(0, meter.TimeSignature('2/4'))\n",
    "s.insert(0, note.Note('C'))\n",
    "#s.insert(0, clef.TrebleClef()) # sorts first\n",
    "\n",
    "returnDict = s.groupElementsByOffset(returnDict=True)\n",
    "pp(returnDict)\n",
    "print\n",
    "\n",
    "a = s.notesAndRests.stream()\n",
    "returnDict = a.groupElementsByOffset(returnDict=True)\n",
    "b = pp(returnDict)\n",
    "\n",
    "for k, v in returnDict.iteritems():\n",
    "    if len(v)>1:\n",
    "        for value in v:\n",
    "            if type(value) == m21.chord.Chord:\n",
    "                print(value.pitcheNames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''make chords\n",
    "\n",
    "Stream.makeChords(\n",
    "    minimumWindowSize=0.125, *\n",
    "    includePostWindow=True, \n",
    "    removeRedundantPitches=True, \n",
    "    useExactOffsets=False, *\n",
    "    gatherArticulations=True, \n",
    "    gatherExpressions=True, \n",
    "    inPlace=False, *\n",
    "    transferGroupsToPitches=False, \n",
    "    makeRests=True)\n",
    "                \n",
    "Gathers simultaneously sounding Note objects into Chord objects, each of which contains all the pitches sounding together.\n",
    "'''\n",
    "            \n",
    "p1 = stream.Voice()\n",
    "p1.append([note.Note('B4', type='quarter'), note.Rest(), # note.Note('C4', type='quarter'), \n",
    "           note.Note('D4', type='quarter'), note.Note('E4', type='quarter'),\n",
    "          note.Note('G4', type='quarter')])\n",
    "\n",
    "# appending notes automatically increments their offsets by duration of prior notes. \n",
    "# to put notes in simultaneously, you'd need to use insert.\n",
    "\n",
    "#for item in p1:\n",
    "#    if type(item) == m21.chord.Chord:\n",
    "#        print(item.pitchNames, item.offset, item.duration)\n",
    "#    elif type(item) == m21.note.Note:\n",
    "#        print(item.pitch, item.offset, item.duration)\n",
    "#    else:\n",
    "#        print(item.offset, item.duration)\n",
    "#print\n",
    "\n",
    "p2 = stream.Voice()\n",
    "p2.append([note.Note('B5', type='half'), note.Note('C5', type='quarter'), \n",
    "           chord.Chord([\"D5\",\"E5\",\"G5\"])])\n",
    "\n",
    "#for item in p2:\n",
    "#    if type(item) == m21.chord.Chord:\n",
    "#        print(item.pitchNames, item.offset, item.duration)\n",
    "#    elif type(item) == m21.note.Note:\n",
    "#        print(item.pitch, item.offset, item.duration)\n",
    "#    else:\n",
    "#        print(item.offset, item.duration)\n",
    "#print\n",
    "\n",
    "sc1 = stream.Score()\n",
    "sc1.insert(0, p1)\n",
    "sc1.insert(0, p2)\n",
    "scChords = sc1.flat.makeChords() # flat removes parts, measures, voices\n",
    "scChords.show('text')\n",
    "print\n",
    "unmake_single_note_chords(scChords)\n",
    "scChords.show('text', addEndTimes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3a = m21.converter.parse('data/wip/scores/bach minuet in g classic medium.xml')\n",
    "s3aTmp = m21.converter.parse('data/wip/scores/bach minuet in g classic medium.xml')\n",
    "\n",
    "for p in s3aTmp.getElementsByClass('Part'):\n",
    "    for m in p.getElementsByClass('Measure'):\n",
    "        m.flattenUnnecessaryVoices(force=True, inPlace=True)\n",
    "        m.makeChords(\n",
    "            minimumWindowSize=0.125,\n",
    "            includePostWindow=False,\n",
    "            removeRedundantPitches=True,\n",
    "            useExactOffsets=True,\n",
    "            gatherArticulations=False,\n",
    "            gatherExpressions=False,\n",
    "            inPlace=True, \n",
    "            makeRests=True)\n",
    "                     \n",
    "#for p in s3aTmp.getElementsByClass('Part'):\n",
    "#    for m in p.getElementsByClass('Measure'):\n",
    "#        m.makeChords(minimumWindowSize=0.125,\n",
    "#                 includePostWindow=False,\n",
    "#                 removeRedundantPitches=True,\n",
    "#                 useExactOffsets=False,\n",
    "#                 gatherArticulations=True,\n",
    "#                 gatherExpressions=True,\n",
    "#                 inPlace=False, \n",
    "#                 makeRests=True)\n",
    "\n",
    "#s3aP2.makeChords(minimumWindowSize=0.125, \n",
    "#                      includePostWindow=False,\n",
    "#                      removeRedundantPitches=True,\n",
    "#                      useExactOffsets=False,\n",
    "#                      gatherArticulations=True,\n",
    "#                      gatherExpressions=True,\n",
    "#                      inPlace=True, \n",
    "#                      makeRests=True)\n",
    "\n",
    "#s3aP1.makeMeasures(meterStream=None,\n",
    "#                                   refStreamOrTimeRange=None,\n",
    "#                                   searchContext=False,\n",
    "#                                   innerBarline=None,\n",
    "#                                   finalBarline='final',\n",
    "#                                   bestClef=True,\n",
    "#                                   inPlace=True)\n",
    "\n",
    "#s3aP2.makeMeasures(meterStream=None,\n",
    "#                                   refStreamOrTimeRange=None,\n",
    "#                                   searchContext=False,\n",
    "#                                   innerBarline=None,\n",
    "#                                   finalBarline='final',\n",
    "#                                   bestClef=True,\n",
    "#                                   inPlace=True)\n",
    "\n",
    "#c1 = m21.clef.TrebleClef()\n",
    "#c1.offset = 0.0\n",
    "#c1.priority = -1\n",
    "\n",
    "#b1 = m21.clef.BassClef()\n",
    "#b1.offset = 0.0\n",
    "#b1.priority = -1\n",
    "\n",
    "#s3aP1.insert(0, c1)\n",
    "#s3aP2.insert(0, b1)\n",
    "\n",
    "#tmp = m21.stream.Stream()\n",
    "#tmp.insert(0, s3aP1)\n",
    "#tmp.insert(0, s3aP2)\n",
    "\n",
    "#play_m21(tmp.measures(0,5))\n",
    "s3a.parts[0].measures(0,5).show('text')\n",
    "print\n",
    "print\n",
    "s3aTmp.parts[0].measures(0,5).show('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''make measures\n",
    "\n",
    "Stream.makeMeasures(meterStream=None, refStreamOrTimeRange=None, searchContext=False, \n",
    "                    innerBarline=None, finalBarline='final', bestClef=False, inPlace=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3a.hasVoices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Write Midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notes = np.array([[0.1, 50, 0.3, 60], [0.2, 62, 0.4, 90]])\n",
    "t = mad.utils.midi.MIDITrack.from_notes(notes)\n",
    "print(t.events)\n",
    "\n",
    "outport.clear_buffer()\n",
    "    for offset, indata in inport.incoming_midi_events():\n",
    "        # Note: This may raise an exception:\n",
    "        outport.write_midi_event(offset, indata)  # pass through\n",
    "        if len(indata) == 3:\n",
    "            status, pitch, vel = struct.unpack('3B', indata)\n",
    "            if status >> 4 in (NOTEON, NOTEOFF):\n",
    "                for i in INTERVALS:\n",
    "                    # Note: This may raise an exception:\n",
    "                    outport.write_midi_event(offset, (status, pitch + i, vel))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MIDI_writer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Session`` instance, which represents a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq\n",
    "        \n",
    "    def annotate_standard_midi():\n",
    "        '''Standard score mode is used to teach the basics of note reading and sequencing. It starts with an \n",
    "        existing midi score and writes notes, as played, over that baseline. Therefore, it is not concerned with\n",
    "        waits, tempo or other temporal characteristics.'''\n",
    "\n",
    "    def write_original_midi():\n",
    "        '''Original score mode layers waits and tempo on Standard mode.'''\n",
    "    \n",
    "# position on score\n",
    "# write note\n",
    "\n",
    "madmom.audio.filters.hz2midi(f, fref=440.0)\n",
    "Convert frequencies to the corresponding MIDI notes.\n",
    "Parameters f : numpy array\n",
    "Input frequencies [Hz].\n",
    "fref : float, optional\n",
    "Tuning frequency of A4 [Hz].\n",
    "Returns m : numpy array\n",
    "MIDI notes\n",
    "Notes\n",
    "For details see: at http://www.phys.unsw.edu.au/jw/notes.html This function does not necessarily return a valid\n",
    "MIDI Note, you may need to round it to the nearest integer\n",
    "\n",
    "\n",
    "madmom.features.notes.write_midi(notes, filename, duration=0.6, velocity=100)\n",
    "Write the notes to a MIDI file.\n",
    "Parameters notes : numpy array, shape (num_notes, 2)\n",
    "Notes, one per row (column definition see notes).\n",
    "filename : str\n",
    "Output MIDI file.\n",
    "duration : float, optional\n",
    "Note duration if not defined by notes.\n",
    "velocity : int, optional\n",
    "Note velocity if not defined by notes.\n",
    "Returns numpy array\n",
    "Notes (including note length and velocity).\n",
    "Notes\n",
    "The note columns format must be (duration and velocity being optional):\n",
    "‘note_time’ ‘MIDI_note’ [’duration’ [’MIDI_velocity’]]\n",
    "\n",
    "10.1.1 madmom.utils.midi\n",
    "This module contains MIDI functionality.\n",
    "Almost all code is taken from Giles Hall’s python-midi package: https://github.com/vishnubob/python-midi\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Navigate Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# position on score\n",
    "# playback loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make it a Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate / score\n",
    "# report / compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pa.get_format_from_width(4, unsigned=True)\n",
    "\n",
    "#pa.get_portaudio_version()\n",
    "p = pyaudio.PyAudio()\n",
    "print(\"host api:\")\n",
    "print(pa.PyAudio.get_default_host_api_info(p))\n",
    "print\n",
    "print(\"default input device:\")\n",
    "print(pa.PyAudio.get_default_input_device_info(p))\n",
    "print\n",
    "print(\"default output device:\")\n",
    "print(pa.PyAudio.get_default_output_device_info(p))\n",
    "print\n",
    "print(\"device count\")\n",
    "print(pa.PyAudio.get_device_count(p))\n",
    "print\n",
    "print(\"jack api num\")\n",
    "print(pa.paJACK)\n",
    "print\n",
    "print(\"host api count\")\n",
    "print(pa.PyAudio.get_host_api_count(p))\n",
    "print\n",
    "print(\"host by type\")\n",
    "print(pa.PyAudio.get_host_api_info_by_type(p, paJACK))\n",
    "print\n",
    "print(\"host by index\")\n",
    "print(pa.PyAudio.get_host_api_info_by_index(p))\n",
    "print\n",
    "\n",
    "\n",
    "#get_device_info_by_host_api_device_index()\n",
    "\n",
    "\n",
    "#jackStrm = pyaudio.PaMacCoreStreamInfo()\n",
    "#pyaudio.PaMacCoreStreamInfo(flags=get_flags(), channel_map=get_channel_map())\n",
    "#input_host_api_specific_stream_info(jackStrm)\n",
    "# Return the channel map set at instantiation.\n",
    "# Return the flags set at instantiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from microphone to \n",
    "#if len(sys.argv) < 2:\n",
    "#    print(\"Plays a wave file.\\n\\nUsage: %s filename.wav\" % sys.argv[0])\n",
    "#    sys.exit(-1)\n",
    "\n",
    "#wf = wave.open(sys.argv[1], 'rb')\n",
    "\n",
    "'''PaMacCoreStreamInfo is a PortAudio Host API Specific Stream Info data structure for specifying Mac OS X-only \n",
    "settings. Instantiate this class and pass the instance as the argument in PyAudio.open() to parameters \n",
    "input_host_api_specific_stream_info or output_host_api_specific_stream_info. (See Stream.__init__().)'''\n",
    "\n",
    "strm = pa.PaMacCoreStreamInfo()\n",
    "jackStrm = pa.PaMacCoreStreamInfo(flags=get_flags(), channel_map=get_channel_map())\n",
    "\n",
    "input_host_api_specific_stream_info(jackStrm)\n",
    "# Return the channel map set at instantiation.\n",
    "# Return the flags set at instantiation.\n",
    "\n",
    "'''pyaudio.PyAudio Python interface to PortAudio. Provides methods to: initialize and terminate PortAudio, \n",
    "open and close streams, query and inspect the available PortAudio Host APIs, query and inspect the available\n",
    "PortAudio audio devices. Use this class to open and close streams.'''\n",
    "\n",
    "# instantiate PyAudio (1)\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# define callback (2)\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    data = wf.readframes(frame_count)\n",
    "    return (data, pyaudio.paContinue)\n",
    "\n",
    "# open stream using callback (3)\n",
    "stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                channels=wf.getnchannels(),\n",
    "                rate=wf.getframerate(),\n",
    "                output=True,\n",
    "                stream_callback=callback)\n",
    "\n",
    "'''Use PyAudio.open() to make a new Stream. pyaudio.Stream(PA_manager, rate, channels, format, input=False, output=False, input_device_index=None, \n",
    "output_device_index=None, frames_per_buffer=1024, start=True, input_host_api_specific_stream_info=None, \n",
    "output_host_api_specific_stream_info=None, stream_callback=None)'''\n",
    "\n",
    "# start the stream (4)\n",
    "stream.start_stream()\n",
    "\n",
    "# wait for stream to finish (5)\n",
    "while stream.is_active():\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# stop stream (6)\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "wf.close()\n",
    "\n",
    "# close PyAudio (7)\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### music search and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Audio fingerprinting and recognition in Python: https://github.com/worldveil/dejavu \n",
    "# landmark based Landmark-based audio fingerprinting: https://github.com/dpwe/audfprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Convert music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_music():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_chords(savTo, inDF):\n",
    "    np.savetxt(savTo, inDF,\n",
    "               fmt=['%.3f', '%.3f', '%.0f', '%s', '%.0f', '%s', '%s', '%s', '%s'], delimiter='\\t')\n",
    "    \n",
    "def load_chords(inFname):   \n",
    "    names = ['OnsetTime', 'OffsetTime', 'm21RootPitchClass', \n",
    "             'm21RootPitchAccidental', 'm21RootOctave', \n",
    "             'm21ContainsTriad', 'm21ChordQuality', \n",
    "             'm21PitchedCommonName', 'madChordLabel']\n",
    "    \n",
    "    return pd.read_table(inFname, names = names)\n",
    "\n",
    "# test:\n",
    "# tmp = load_chords('data/wip/AkPnBcht/MUS/MAPS_MUS-ty_mai_AkPnBcht.chords.dnn.txt')\n",
    "\n",
    "def save_list(toSave, toDir, toName, toType):\n",
    "    fullName = ''.join([toDir, toName, toType])\n",
    "    with open(fullName, 'w') as f:\n",
    "        for item in toSave:\n",
    "            f.write(item + '\\n')\n",
    "\n",
    "def pickle_it(toPick, toDir, toName):\n",
    "    with open(toDir + toName, 'wb') as f:\n",
    "        pickle.dump(toPick, f)\n",
    "        print(\"pickled:\", toName)\n",
    "\n",
    "def unpickle_it(frmDir, frmName):\n",
    "    with open(frmDir + frmName, 'rb') as f:\n",
    "        return(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NEW_CHORD_DTYPE = [('OnsetTime', '%.3f'), ('OffsetTime', '%.3f'), \n",
    "#                       ('m21RootPitchClass', np.int), ('m21RootPitchAccidental', np.str),\n",
    "#                       ('m21RootOctave', np.int), ('m21ContainsTriad', np.bool),\n",
    "#                       ('m21ChordQuality', np.str),('m21PitchedCommonName', np.str),\n",
    "#                       ('madChordLabel', np.str)] # 'U32'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put in \"format\" or \"transform\" function\n",
    "\n",
    "# txt_to_y format: [OnsetTime, OffsetTime, Midipitch]\n",
    "\n",
    "def flip_formats(inFile, frmFrmt, toFrmt):\n",
    "    \n",
    "    # valid to/froms: madNote, mapNote, madChord, m21Chord\n",
    "    \n",
    "    inHt, inWd = inFile.shape\n",
    "    \n",
    "    # data types\n",
    "    MAD_NOTE_HEADER_1 = ['note_time', 'MIDI_note']\n",
    "    MAD_NOTE_HEADER_2 = ['note_time', 'MIDI_note', 'duration']\n",
    "    MAD_NOTE_HEADER_3 = ['note_time', 'MIDI_note', 'duration', 'MIDI_note']\n",
    "        \n",
    "    MAD_NOTE_DTYPE_1 = [('note_time', np.float), ('MIDI_note', np.int)]\n",
    "    MAD_NOTE_DTYPE_2 = [('note_time', np.float), ('MIDI_note', np.int), ('duration', np.float)]\n",
    "    MAD_NOTE_DTYPE_3 = [('note_time', np.float), ('MIDI_note', np.int), ('duration', np.float), ('MIDI_note', np.int)]\n",
    "    \n",
    "    MAD_CHORD_DTYPE = [('start', np.float), ('end', np.float), ('label', 'U32')]\n",
    "\n",
    "    MAP_NOTE_HEADER = ['OnsetTime', 'OffsetTime', 'MidiPitch']\n",
    "    MAP_CHORD_HEADER = ['OnsetTime', 'OffsetTime', 'ChordLabel']\n",
    "    \n",
    "    #M21_NOTE_DTYPE = 'f4,f4,int'\n",
    "    #M21_CHORD_DTYPE = 'f4,f4,S10'\n",
    "    \n",
    "    if frmFrmt == \"madNote\":\n",
    "        \n",
    "        if toFrmt == \"mapNote\" or toFrmt == \"m21Chord\" or toFrmt == \"madChord\":\n",
    "            \n",
    "            notes = np.zeros(shape=(inHt, 3), dtype=float) # dtype defaults to float64\n",
    "            notes[:,0] = inFile[:,0]\n",
    "            notes[:,2] = inFile[:,1].astype(int)\n",
    "            \n",
    "            # if no duration\n",
    "            if inFile.shape[1] == 2:\n",
    "                notes[:,1] = 0\n",
    "                \n",
    "            # if duration\n",
    "            else:\n",
    "                notes[:,1] = inFile[:,0] + inFile[:,2]\n",
    "                \n",
    "            if toFrmt == \"mapNote\":\n",
    "                return(notes)\n",
    "        \n",
    "            elif toFrmt == \"m21Chord\" or toFrmt == \"madChord\":\n",
    "                \n",
    "                notes = pd.DataFrame(notes, columns=['OnsetTime', 'OffsetTime', 'MidiPitch'])\n",
    "                notes = tmp.round({'OnsetTime': 2, 'OffsetTime': 2, 'MidiPitch': 0})\n",
    "                notes = tmp.sort_values(['OnsetTime', 'MidiPitch'],\n",
    "                                      axis=0, ascending=True, inplace=False,\n",
    "                                      kind='quicksort', na_position='last')\n",
    "                \n",
    "                notes[\"MidiPitch\"] = tmp['MidiPitch'].astype(int)\n",
    "                \n",
    "                notes, chords = txt_to_y(tmp, mode=\"thisFile\")\n",
    "                \n",
    "                if toFrmt == \"m21Chord\":\n",
    "                    return(chords)\n",
    "            \n",
    "                elif toFrmt == \"madChord\":\n",
    "                    # NOTE: UNTIL YOU FIND OUT HOW THEY'RE CLASSING THEIR CHORDS, DON'T SPEND TIME HERE.\n",
    "                    lines = [line.rstrip('\\n').split('\\t') for line in open('data/mad2m21map.txt', 'U')]\n",
    "                    headers = lines[0]; lines = lines[1:len(lines)]\n",
    "                    lines = pd.DataFrame(lines, columns= headers)\n",
    "                    \n",
    "                    d = dict(zip(lines.m21_chord_pitchedCommonName, lines.mad_chord))\n",
    "                    \n",
    "                    for i in range(chords.shape[0]):\n",
    "                        try:\n",
    "                            chords.ChordLabel.iloc[i] = d[chords.ChordLabel.iloc[i]]\n",
    "                        except:\n",
    "                            chords.ChordLabel.iloc[i] = 'N'\n",
    "                            \n",
    "                    return(chords)\n",
    "        # NOTE: DON'T SPEND TIME GOING FROM MAD.CHORD LABELS TO M21. JUST USE MODEL OUTPUTS TO PREDICT. \n",
    "                \n",
    "                \n",
    "\n",
    "#print(flip_formats(rnn_note_detect, \"madNote\", \"mapNote\"))\n",
    "print(flip_formats(rnn_note_detect, \"madNote\", \"m21Chord\"))\n",
    "print(flip_formats(rnn_note_detect, \"madNote\", \"madChord\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sonic visualizer: music analysis http://www.sonicvisualiser.org/\n",
    "vamp plugins: http://www.vamp-plugins.org/download.html\n",
    "bbc plugins: human/music, intensity, energy https://github.com/bbcrd/bbc-vamp-plugins/blob/master/README.md\n",
    "chordino: maj/min chord recognition http://www.isophonics.net/nnls-chroma\n",
    "music matching: http://www.eecs.qmul.ac.uk/~simond/match/index.html\n",
    "\n",
    "building plugins: http://www.vamp-plugins.org/develop.html\n",
    "annotations: http://www.vamp-plugins.org/sonic-annotator/\n",
    "\n",
    "musescore: score notation from midi, music xml https://github.com/musescore/MuseScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1: Import piano train / test data\n",
    "\n",
    "1. Take their lists of train and test data\n",
    "2. Create list object\n",
    "3. Feed it to a process to either iteratively or bulk load files from directory\n",
    "4. Perform log scale transform of input wav\n",
    "5. Come back to other features, chords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to map sound to notes\n",
    "\n",
    "1. Take professional music sound files\n",
    "2. Play, logging spectrum (frequency / time) and other attributes\n",
    "    a. https://github.com/tyiannak/pyAudioAnalysis/wiki/3.-Feature-Extraction, OR\n",
    "    b. \n",
    "3. Predict notes based on sound\n",
    "    a. input spectrum is the \"X\"\n",
    "    b. sheet music notes are the \"y\" (notes A, B, C, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1: Generate harp note / chord train / test files\n",
    "\n",
    "Garageband or other to generate:\n",
    "1. individual instrument note (pitch?), chord by major/minor, octave, inversion\n",
    "2. sequence files w/ varying amounts of spacing\n",
    "3. mp3's of classical music for which you can easily veryify the notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Standard: Fingerprint music files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n. Fingerprint music\n",
    "\n",
    "https://github.com/dpwe/audfprint\n",
    "\n",
    "Audfprint is a python (and Matlab) script that can take a list of soundfiles and create a database of landmarks, and then subsequently take one or more query audio files and match them against the previously-created database.  The fingerprint is robust to things like time skews, different encoding schemes, and even added noise. It can match small fragments of sound, down to 10 sec or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use FFT, MFCC, etc to create a fignerprint of each of the music files s/t when user starts playing, you can take notes they've played and match "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Streaming: Extract note Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Streaming: Search / match sample to fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display mode\n",
    "\n",
    "1. Accept song selection\n",
    "2. Load / display sheet music\n",
    "3. Listen for start\n",
    "4. Recieve sounds / translate notes\n",
    "5. Track progress w/ vertical bar\n",
    "6. Spot repeats re-setting tracking bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/tyiannak/pyAudioAnalysis\n",
    "# http://essentia.upf.edu/documentation/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate mode\n",
    "\n",
    "1. display mode functionality including tracking progress\n",
    "2. record playing. overlay repeats. you're tracking stats. so, obj s/b:\n",
    "    a. to get through song re-playing pieces as required, THEN\n",
    "    b. to get through song cleanly\n",
    "3. comparing played to professional\n",
    "    a. option to play:\n",
    "        i. metrinome \n",
    "        ii. professional a low volume\n",
    "4. identify discrpancies (timing after prior note, incorrect note)\n",
    "    a. Gaia, a C++ library with python bindings which implement similarity measures and classification on the results of audio analysis, and generate classification models that Essentia can use to compute high-level description of music.\n",
    "5. show discrepancies\n",
    "    a. accept tolerances (+/- time, other?)\n",
    "    b. show played note in red (i.e., before/after, above/below).\n",
    "6. show / log statistics\n",
    "    a. accuracy\n",
    "    b. similarity\n",
    "    c. error types and frequency distribution\n",
    "        i. early,\n",
    "        ii. late\n",
    "        iii. wrong note\n",
    "    d. problem areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive mode\n",
    "\n",
    "1. evaluate mode functionality\n",
    "2. prompt session info and imprint voice of user for command interface\n",
    "    a. \"this is []. the date is []. i'll be practicing for about [] minutes.\"\n",
    "3. voice commands\n",
    "    a. \"replay [] notes\" - defaults to: 5 notes, played version\n",
    "    b. \"replay base [] notes\"\n",
    "    c. \"loop [] notes\" - \n",
    "        ii. Loop [] notes / Stop Loop\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "This project would not be possible without the invaluable assistance of:\n",
    "\n",
    "## Training data\n",
    "\n",
    "The MAPS piano data set. Roughly 40G of piano notes, chords, music assembled by V. Emiya for her PhD thesis at Telecom ParisTech/ENST in 2008 and in conjunction with R. Badeau, B. David for their paper \"Multipitch estimation of piano sounds using a new probabilistic spectral smoothness principle\"<cite data-cite=\"emiya2010multipitch\"></cite>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for installing latex, bibtex and pdf-ing jupyter notenooks: https://www.youtube.com/watch?v=m3o1KXA1Rjk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
