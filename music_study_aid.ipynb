{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Configure Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install image # install's PIL, the python image library\n",
    "# install portaudio is required by pyaudio (http://portaudio.com/docs/v19-doxydocs/tutorial_start.html)\n",
    "pip install pyaudio # for recording within python. used for audioSearch module. install was a bit funky, but this fixed it: http://stackoverflow.com/questions/33513522/when-installing-pyaudio-pip-cannot-find-portaudio-h-in-usr-local-include\n",
    "pip install pygame # for realtime MIDI performance in midi.realtime module\n",
    "\n",
    "# visualization:\n",
    "# sonic visualizer, for viewing and analyzing contents of music audio files (http://www.sonicvisualiser.org/download.html)\n",
    "# install musescore for viewing and editing music notation (http://www.musescore.org, https://github.com/musescore/MuseScore)\n",
    "# install lilypond for displaying musical scores (http://lilypond.org/)\n",
    "\n",
    "# audio processing: \n",
    "# jack audio for osx. I downloaded latest for Mac from here: https://github.com/jackaudio/jack2/issues/73\n",
    "pip install cffi # C Foreign Function Interface for Python, used to access the C-API of the JACK library from within Python\n",
    "pip install JACK-Client # jack client https://jackclient-python.readthedocs.io/en/0.4.1/\n",
    "pip install SpeechRecognition # https://pypi.python.org/pypi/SpeechRecognition/. Git repository has prerequisites including pyaudio version: https://github.com/Uberi/speech_recognition\n",
    "pip install pysoundfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## B. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# directory, files management, etc.\n",
    "import os as os\n",
    "#from os.path import isfile, join\n",
    "import glob\n",
    "from StringIO import StringIO\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# arrays, dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# utilities\n",
    "import random as rand\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# audio\n",
    "import sys\n",
    "import wave\n",
    "import pyaudio as pa\n",
    "import jack as jk\n",
    "import soundfile as sf\n",
    "\n",
    "# MIR\n",
    "import essentia as es\n",
    "import madmom as mad\n",
    "import music21 as m21\n",
    "import yaafelib as yf\n",
    "# neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Envirnonment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mdowns/.virtualenvs/audio_2.7_env/bin/python2.7\n",
      "{'TERM_PROGRAM_VERSION': '361.1', 'LOGNAME': 'mdowns', 'USER': 'mdowns', 'HOME': '/Users/mdowns', 'DEST_DIR': '/usr/local', 'PATH': '/Users/mdowns/.virtualenvs/audio_2.7_env/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Library/TeX/texbin:/usr/local/bin', 'VIRTUALENVWRAPPER_SCRIPT': '/usr/local/bin/virtualenvwrapper.sh', 'DISPLAY': '/private/tmp/com.apple.launchd.2kweWO6Nqb/org.macosforge.xquartz:0', 'TERM_PROGRAM': 'Apple_Terminal', 'LANG': 'en_US.UTF-8', 'TERM': 'xterm-color', 'SHELL': '/bin/bash', 'SHLVL': '1', 'XPC_FLAGS': '0x0', 'PIP_REQUIRE_VIRTUALENV': 'true', '_': '/Users/mdowns/.virtualenvs/audio_2.7_env/bin/jupyter', 'DYLD_FALLBACK_LIBRARY_PATH': '/usr/local/lib', 'ARCHFLAGS': '-arch x86_64', 'WORKON_HOME': '/Users/mdowns/.virtualenvs', 'TERM_SESSION_ID': '8437E498-16A1-4733-9A27-37D9BFD86C71', 'XPC_SERVICE_NAME': '0', 'JPY_PARENT_PID': '58323', 'PYTHONPATH': '/usr/local/python_packages', 'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.QBkIlQW5aN/Listeners', 'VIRTUAL_ENV': '/Users/mdowns/.virtualenvs/audio_2.7_env', 'Apple_PubSub_Socket_Render': '/private/tmp/com.apple.launchd.GhQ3cZzIKl/Render', 'PS1': '(audio_2.7_env) \\\\h:\\\\W \\\\u\\\\$ ', 'GIT_PAGER': 'cat', 'TMPDIR': '/var/folders/66/hq0ljfxd5gl2tsl_swcg1gcc0000gp/T/', 'VIRTUALENVWRAPPER_PROJECT_FILENAME': '.project', 'VIRTUALENVWRAPPER_HOOK_DIR': '/Users/mdowns/.virtualenvs', 'PAGER': 'cat', 'OLDPWD': '/Users/mdowns/Projects', 'CLICOLOR': '1', '__CF_USER_TEXT_ENCODING': '0x1F6:0x0:0x0', 'PWD': '/Users/mdowns/Projects/music_study_aid', 'YAAFE_PATH': '/usr/local/yaafe_extensions', 'VIRTUALENVWRAPPER_WORKON_CD': '1'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pitches, notes\n",
    "PITCH_CLASSES = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "NUM_PITCH_CLASS = len(PITCH_CLASSES)\n",
    "NUM_MIDI_PITCH = 127               # range of audible sounds\n",
    "RANGE_PIANO_PITCH = range(21,109)  # 88 notes (12 notes * 7.25 octave scale), midi 21:108\n",
    "NUM_PIANO_PITCH = len(RANGE_PIANO_PITCH)\n",
    "\n",
    "# chords\n",
    "NUM_COM_CHORD_OCTAVE = 5\n",
    "#NUM_ALL_CHORD_PITCH = NUM_PIANO_PITCH # midi 21:108 inclusive. 7.25 octave piano scale\n",
    "#RANGE_COM_CORD_PITCH = range(36,96) # midi 36:95 inclusive, 5 octave piano scale\n",
    "#NUM_COM_CHORD_PITCH = len(RANGE_COM_CHORD_PITCH)\n",
    "NUM_MAJOR_MINOR = 2\n",
    "CHORD_KEY_SETS = 1 # only 3-key chords \n",
    "\n",
    "# loudness\n",
    "RANGE_MEZZO_FORTE = range(60, 69)\n",
    "RANGE_PIANO_FORTE = range(32, 97)\n",
    "\n",
    "# classes\n",
    "NUM_NOT_TGT_CLASS = 1\n",
    "NOT_TGT_LABEL = [\"not target\"]\n",
    "NUM_PITCH_CLASSES = NUM_PIANO_PITCH + NUM_NOT_TGT_CLASS\n",
    "NUM_CHORD_CLASSES = (NUM_PITCH_CLASS * NUM_COM_CHORD_OCTAVE * NUM_MAJOR_MINOR * CHORD_KEY_SETS) + NUM_NOT_TGT_CLASS\n",
    "\n",
    "#PITCH_CLASSES = RANGE_PIANO_PITCH\n",
    "#PITCH_CLASSES = NOT_TGT_LABEL + PITCH_CLASSES\n",
    "\n",
    "CHORD_CLASSES = []\n",
    "for k in range(3,8):\n",
    "    for i in PITCH_CLASSES:\n",
    "        for j in [\"major\", \"minor\"]:\n",
    "            CHORD_CLASSES.append(str(i+str(k)+\"-\"+j+\" triad\"))\n",
    "CHORD_CLASSES = NOT_TGT_LABEL + CHORD_CLASSES\n",
    "\n",
    "# to standardize feature extract algos\n",
    "NUM_SAMPLES = 22050 # 44100\n",
    "NUM_FRAMES = 1024 # 2048\n",
    "NUM_HOPS = 512 # 441\n",
    "NUM_BANDS = 24 # 48\n",
    "\n",
    "# other \n",
    "dat_dir = 'data/maps/'\n",
    "WINDOW = 0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# II. Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Accoustic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Format X & Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Process .wav and .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''grabs performance (.wav) and ground truth (.txt). runs performance thru neural nets. \n",
    "evaluates performance.'''\n",
    "\n",
    "def process_wavs(inWavDir, outTxtDir):\n",
    "    '''\n",
    "    reads wav, writes note, cnn chord and dnn chord files to target dir\n",
    "    if tgtWavDir provided, appends chord, note and wav files incrementing. saves wav to target dir. \n",
    "    '''\n",
    "    \n",
    "    inWavs = glob.glob(inWavDir + '/*/*.wav')\n",
    "    wavs = []; begTm = 0; noteCtr = 0; chordCtr = 0\n",
    "    \n",
    "    for i in range(len(inWavs)):\n",
    "        if i % 5 == 0: print(\"processing input file #\", i)\n",
    "        \n",
    "        # process X: chord and note recognition from wave\n",
    "        dnm = os.path.dirname(inWavs[i])\n",
    "        fnm, _ = os.path.splitext(os.path.basename(inWavs[i]))\n",
    "        inNm = dnm + '/' + fnm + '.txt'\n",
    "        outNm = outTxtDir + '/' + fnm\n",
    "        \n",
    "        #dnn_chord_rec(inWavs[i], savTo = outNm + '.chords.dnn.txt' )\n",
    "        #cnn_chord_rec(inWavs[i], savTo = outNm + '.chords.cnn.txt'  )\n",
    "        rnn_note_and_chord_rec(\"single\", inWavs[i], outNm) # outNm + '.notes.txt'\n",
    "        \n",
    "        # process Y: corresponding y values from text files\n",
    "        N, C = txt_to_y(inNm)\n",
    "        \n",
    "        if N is not None:\n",
    "            N['Duration'] = N.OffsetTime - N.OnsetTime\n",
    "            N = N[['OnsetTime', 'MidiPitch', 'Duration' ]]\n",
    "            mad.features.notes.write_notes(np.array(N),\n",
    "                                           outNm + '.note.y.txt',\n",
    "                                           fmt=['%.3f', '%d', '%.3f'])\n",
    "            \n",
    "        if C is not None:\n",
    "            save_chords(outNm + '.chords.y.txt', C)\n",
    "\n",
    "#process_wavs(inWavDir = 'data/wip/tmp/',\n",
    "#             outTxtDir = 'data/wip/tmp')\n",
    "    \n",
    "process_wavs(inWavDir = 'data/maps/AkPnBcht/UCHO/I60-68/',\n",
    "             outTxtDir = 'data/wip/AkPnBcht/UCHO/I60-68/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Format Y Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def txt_to_y(inFile, mode=\"savedFile\", out_fmt='runTime', wav_dur=0, wav_frames=0):\n",
    "    '''TODO: implement chord recognition window s/t notes w/in window are considered components of chord.\n",
    "    only do this if you see material chord misclassifications in normal application use.'''\n",
    "    # valid y_modes: \"pitch_only\", \"chord_only\", \"music\"\n",
    "    \n",
    "    '''translates text files w/ onset, offset and pitch into two Y matricies:\n",
    "    #  1. y_notes: [n, 89] vect of binaries representing:\n",
    "    #     a. notes C3 on piano keyboard A1->C#9 (midi 21-109)\n",
    "    #     b. \"not target class\" including notes/pitches outside range AND in-range chords formed by 2+ keys\n",
    "    #  2. y_chords: [n, 120] vect of the \"major\" and \"minor\" triads (some of the most common chords in western music):\n",
    "    #     a. 12 pitch classes i.e., C, C#, D, D#, E, F, F#, G, G#, A, A#, B\n",
    "    #     b. 5 (of total 7.5) octaves i.e., C in 4th octave: C4\n",
    "    #     c. 1 (of total 6 in db) key combinations i.e., triad (not 2, 4, 5, 6 or 7 key combinations)\n",
    "    #     d. major (root pitch, +4 pitches, +3 pitches) or  minor (root pitch, +3, +4) quality \n",
    "    #        i.e., C major triad 4th octave: C4, E4, G4. C minor triad 4th octave: C4, D#4, G4\n",
    "    #     e. NOT IMPLEMENTED: 2 inversions that are defined by the lowest note in the chord\n",
    "    #        i.e., C4, E4, G4 becomes E4, G4, C5 in the first inversion  \n",
    "    #     f. \"not target class\" including single notes and the multitude of less common 2+ note chords\n",
    "    #     other options: http://www.daigleharp.com/Images/Help%20Files/commonchordsforautoharp.pdf, \n",
    "    http://www.hooktheory.com/blog/i-analyzed-the-chords-of-1300-popular-songs-for-patterns-this-is-what-i-found/'''\n",
    "    \n",
    "    if mode == \"savedFile\":\n",
    "        # reads text file into dataframe, sort and round\n",
    "        lines = read_maps_note_file(inFile)\n",
    "    else:\n",
    "        lines = inFile\n",
    "    \n",
    "    chord_rcds_fl = 0; in_chord_fl = 0; pitch_rcds_fl = 0; rval = 0\n",
    "    \n",
    "    # if single line file, save pitch\n",
    "    if lines.shape[0] == 1:\n",
    "        active_pitches = lines.iloc[[0]]\n",
    "        pitch_rcds_fl = 1\n",
    "    \n",
    "    # otherwise, step thru lines assigning notes to pitches or chords\n",
    "    else:\n",
    "        for i in range(1,lines.shape[0]):\n",
    "            \n",
    "            # process pitches: madmom RNN good at extracting notes even from chords. \n",
    "            # so, process all rcds as note records\n",
    "            if pitch_rcds_fl == 0:\n",
    "                # ...then instantiate note array using prior line\n",
    "                active_pitches = lines.iloc[[i-1]]\n",
    "                pitch_rcds_fl = 1\n",
    "                    \n",
    "            # ... and note array already started, then append line to note array\n",
    "            else:\n",
    "                active_pitches = active_pitches.append(lines.iloc[[i-1]])\n",
    "                \n",
    "            # if i is the last line in input array, move it to pitch array also\n",
    "            if i == (lines.shape[0]-1):\n",
    "                if pitch_rcds_fl == 0:\n",
    "                    active_pitches = lines.iloc[[i]]\n",
    "                else:\n",
    "                    active_pitches = active_pitches.append(lines.iloc[[i]])\n",
    "                        \n",
    "            # process chords: if note record has same onset (w/in rounding tolerance of 0.01) as prior...\n",
    "            if lines.iloc[i,0] == lines.iloc[i-1,0]: #and lines.iloc[i,1] == lines.iloc[i-1,1]:\n",
    "                # ... and it's the first chord in piece,...\n",
    "                if chord_rcds_fl == 0:\n",
    "                    # ...then instantiate chord array using the prior pitch (line)\n",
    "                    active_chords = lines.iloc[[i-1]]\n",
    "                    chord_rcds_fl = 1; in_chord_fl = 1\n",
    "                \n",
    "                # otherwise, append the prior pitch (line) to chord array\n",
    "                else:\n",
    "                    active_chords = active_chords.append(lines.iloc[[i-1]])\n",
    "                    in_chord_fl = 1\n",
    "                \n",
    "                # if last line in input array (and it's same as prior), move it to chord array\n",
    "                if i == (lines.shape[0]-1):\n",
    "                    active_chords = active_chords.append(lines.iloc[[i]])\n",
    "                    \n",
    "            # so, current line doesn't have same onset...\n",
    "            else:\n",
    "                #...but you were in a chord... \n",
    "                if in_chord_fl == 1:\n",
    "                    #...append prior pitch (line) to the chord array.\n",
    "                    active_chords = active_chords.append(lines.iloc[[i-1]])\n",
    "                    in_chord_fl = 0\n",
    "                \n",
    "                    # if last line in input array (and you're in a chord), move it to chord array\n",
    "                    if i == (lines.shape[0]-1):\n",
    "                        active_chords = active_chords.append(lines.iloc[[i]])\n",
    "    \n",
    "    if out_fmt == \"runTime\":\n",
    "        if(pitch_rcds_fl == 0):\n",
    "            active_pitches = None\n",
    "            \n",
    "        if(chord_rcds_fl == 0):\n",
    "            active_chords = None\n",
    "        \n",
    "        else:\n",
    "            chordsDF = to_chords(active_chords, inFrmt = \"m21noteDF\")\n",
    "            #print(\"4c. after\", i, \"iterations:\", time.time())\n",
    "        \n",
    "        return(active_pitches, chordsDF)\n",
    "    \n",
    "    elif out_fmt == \"oneHot\":\n",
    "        \n",
    "        # format time index w/ slices = wave frame sample rate        \n",
    "        time_ctr = 0\n",
    "        time_incr = float(wav_dur) / wav_frames\n",
    "        time_idx = []\n",
    "    \n",
    "        for k in range(wav_frames): \n",
    "            time_idx.append(np.round(time_ctr,2))\n",
    "            time_ctr = time_ctr + time_incr\n",
    "    \n",
    "        # initialize y matrices    \n",
    "        Y_pitch = pd.DataFrame(np.zeros((len(time_idx), NUM_PITCH_CLASSES), dtype=int),\n",
    "                               index = time_idx, columns = PITCH_CLASSES)\n",
    "        Y_pitch.iloc[:,0] = 1 # set NOT_TGT_CLASS on as default\n",
    "        \n",
    "        Y_chord = pd.DataFrame(np.zeros((len(time_idx), NUM_CHORD_CLASSES), dtype=int),\n",
    "                               index = time_idx, columns = CHORD_CLASSES)\n",
    "        Y_chord.iloc[:,0] = 1 # set NOT_TGT_CLASS on as default\n",
    "    \n",
    "        time_idx = pd.DataFrame(time_idx)\n",
    "\n",
    "        # step thru active, single pitch records\n",
    "        if(pitch_rcds_fl > 0):\n",
    "        \n",
    "            for i in range(active_pitches.shape[0]):\n",
    "               \n",
    "                # ...find the ids of all time indexes that fall after the onset...\n",
    "                more = time_idx[time_idx[0] >= active_pitches.iloc[i,0]].index.tolist()\n",
    "        \n",
    "                #...and the ids of all time indexes that fall before the offset\n",
    "                less = time_idx[time_idx[0] < active_pitches.iloc[i,1]].index.tolist()\n",
    "        \n",
    "                # the intersection are the id's of time indexes where a pitch was active\n",
    "                net = np.intersect1d(more, less, assume_unique=False)\n",
    "        \n",
    "                # flip the class variable for each time index\n",
    "                for j in range(len(net)):\n",
    "                    # if it's a valid pitch...\n",
    "                    if active_pitches.iloc[i,2] in PITCH_CLASSES:\n",
    "                        # ...flip the corresponding pitch column\n",
    "                        Y_pitch.loc[time_idx.iloc[net[j],0], int(active_pitches.iloc[i,2])] = 1\n",
    "                        Y_pitch.loc[time_idx.iloc[net[j],0], NOT_TGT_LABEL] = 0\n",
    "            \n",
    "        # step thru active chord records\n",
    "        if(chord_rcds_fl > 0):\n",
    "            uniq_onset = active_chords.OnsetTime.unique()\n",
    "          \n",
    "            for i in range(uniq_onset.shape[0]):\n",
    "                cur_pitches = active_chords[active_chords.iloc[:,0] == uniq_onset[i]]\n",
    "                cur_chord = m21.chord.Chord(np.array(cur_pitches.iloc[:,2])).pitchedCommonName\n",
    "            \n",
    "                # ...find the ids of all time indexes that fall after the onset...\n",
    "                more = time_idx[time_idx[0] >= uniq_onset[i]].index.tolist()\n",
    "                            \n",
    "                #...and the ids of all time indexes that fall before the offset\n",
    "                less = time_idx[time_idx[0] < cur_pitches.iloc[0,1]].index.tolist()\n",
    "            \n",
    "                # the intersection are the id's of time indexes where a pitch was active\n",
    "                net = np.intersect1d(more, less, assume_unique=False)\n",
    "        \n",
    "                # if it's a valid chord,...\n",
    "                if cur_chord in CHORD_CLASSES:\n",
    "                    # ...cycle thru time indexes...\n",
    "                    for j in range(len(net)):\n",
    "                        # ...flipping the corresponding chord column\n",
    "                        Y_chord.loc[time_idx.iloc[net[j],0], cur_chord] = 1\n",
    "                        Y_chord.loc[time_idx.iloc[net[j],0], NOT_TGT_LABEL] = 0\n",
    "                    \n",
    "                    rval = 1\n",
    "                \n",
    "        #return([rval])\n",
    "        return(Y_pitch, Y_chord)\n",
    "        \n",
    "# test music\n",
    "#ptch, crd = txt_to_y('data/maps/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Clean Chord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_chords(inChords, inFrmt):\n",
    "    \n",
    "    if inFrmt == 'madChordDF':\n",
    "        outChords = inChords.iloc[:,0:2]\n",
    "        notClass = [float('nan'), None, float('nan'), None, None, None, None]\n",
    "        dels = []\n",
    "        pitchClasses = range(NUM_PITCH_CLASSES) # Integer vales 0-11, where C=0, C#=1, D=2...B=11 per M21\n",
    "        newLines = []\n",
    "        \n",
    "        for i in range(inChords.shape[0]):\n",
    "            newLine = []\n",
    "            \n",
    "            # pitch class\n",
    "            if inChords.iloc[i,2][0] == 'N':\n",
    "                newLines.append(notClass)\n",
    "                dels.append(i)\n",
    "                continue\n",
    "                '''for lines identified as 'N', add placeholder which will be subsequently deleted. these are \n",
    "                chords for which the onset is known, but the classification is not known (trained). by deleting\n",
    "                we re losing info. however, evaluation chokes on records w/ nans or nones. further, given how \n",
    "                effective note rnn is at spotting chord onset AND classifying chords, chord CNN/DNN will be \n",
    "                used to augment its predictions'''\n",
    "            elif inChords.iloc[i,2][1] == ':': \n",
    "                newLine.append(pitchClasses[PITCH_CLASSES.index(inChords.iloc[i,2][0])])\n",
    "                # accidental\n",
    "                newLine.append('accidental natural')\n",
    "            elif inChords.iloc[i,2][1] == '#':\n",
    "                '''flats \"-\" madmon trained their network w/ 25 classes representing:\n",
    "                1. 12 pitch classes: 'A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#'\n",
    "                2. 2 qualities 'maj', 'min'\n",
    "                3. N for none of the above\n",
    "                \n",
    "                Problem is that, while they occur often in music, flats aren't included. Looking at akpnbcht,\n",
    "                see that E- and B- chords are classed as D# and A# by the algorithm. Specifically:\n",
    "                1. of 13.5k chords, 1.9k (14%) were flats. 1,112 E-, 820 B-. \n",
    "                2. of 1,112 E-, 208 (100% of those classed by the algo) were classed as D#. \n",
    "                3. of 820 B-, 128 (100% of those classed by the algo) were classed as A#. \n",
    "                4. of mad chords classed D, 376 were classed by m21 as D, 208 as E.\n",
    "                5. of mad chords classed A, 354 were classed by m21 as A, 128 as B.\n",
    "                \n",
    "                so, assuming akpnbcht is a representative dataset:\n",
    "                1. mad classifies flats as next pitch class down\n",
    "                2. this requires changes to:\n",
    "                    a. the mad chord outputs to bring them in inline w/ m21-based classifications (below)\n",
    "                    b. the m21-based y records\n",
    "                '''\n",
    "                if inChords.iloc[i,2][0] == 'D':\n",
    "                    newLine.append(pitchClasses[PITCH_CLASSES.index('E')])\n",
    "                    newLine.append('accidental flat')\n",
    "                elif inChords.iloc[i,2][0] == 'A':\n",
    "                    newLine.append(pitchClasses[PITCH_CLASSES.index('B')])\n",
    "                    newLine.append('accidental flat')\n",
    "                else: \n",
    "                    newLine.append(pitchClasses[PITCH_CLASSES.index(inChords.iloc[i,2][0:1])])\n",
    "                    newLine.append('accidental sharp')\n",
    "            elif inChords.iloc[i,2][1] == '-':\n",
    "                newLine.append(pitchClasses[PITCH_CLASSES.index(inChords.iloc[i,2][0])+1])\n",
    "                newLine.append('accidental flat')\n",
    "            else:\n",
    "                print('unrecognized accidental on chord input file at line', i); end\n",
    "            \n",
    "            # octave\n",
    "            newLine.append(-10)\n",
    "            \n",
    "            # triad\n",
    "            newLine.append(True)\n",
    "            \n",
    "            # quality\n",
    "            s = inChords.iloc[i,2] \n",
    "            try: \n",
    "                start = s.find(':') + 1\n",
    "                end = len(s)\n",
    "                found = s[start:end]\n",
    "            except:\n",
    "                print(\"chord record without maj/min quality:\", i)\n",
    "                end\n",
    "            if found == 'maj': newLine.append(\"major\")\n",
    "            elif found == 'min': newLine.append(\"minor\")\n",
    "            else: print(\"chord record with non maj/min quality:\", i); end\n",
    "            \n",
    "            # m21 pitched common name\n",
    "            newLine.append(None)\n",
    "            \n",
    "            # mad chord label\n",
    "            newLine.append(inChords.iloc[i,2])\n",
    "            newLines.append(newLine)\n",
    "    \n",
    "    elif inFrmt == \"m21noteDF\":\n",
    "        # going from stream of notes to m21 chords\n",
    "        outChords = pd.DataFrame(inChords.drop_duplicates(subset=\"OnsetTime\", keep='first'))\n",
    "        outChords = outChords.iloc[:,0:2]\n",
    "        dels = []; newLines = []\n",
    "        \n",
    "        for i in range(outChords.shape[0]):\n",
    "            newLine = []\n",
    "            curPitches = inChords.MidiPitch[inChords.OnsetTime == outChords.OnsetTime.iloc[i]]\n",
    "            crd = m21.chord.Chord(np.array(curPitches))\n",
    "            root = crd.root()\n",
    "            newLine.append(root.pitchClass)\n",
    "                \n",
    "            s = str(root.accidental)\n",
    "            \n",
    "            try: \n",
    "                start = s.find('<') + 1\n",
    "                end = s.find('>', start)\n",
    "                found = s[start:end]\n",
    "            except:\n",
    "                found = root.accidental\n",
    "                print(\"accidental w/ out < or >:\", i)\n",
    "                print(root.accidental)\n",
    "    \n",
    "            newLine.append(found)\n",
    "            newLine.append(root.octave)\n",
    "            newLine.append(crd.containsTriad())\n",
    "            newLine.append(crd.quality)\n",
    "            newLine.append(crd.pitchedCommonName)\n",
    "                \n",
    "            if crd.containsTriad() == True:\n",
    "                '''based on the assumption that the basic unit of chord construction is the major/minor triad, \n",
    "                I check for triad here, then determine the appropriate (as near as I can tell) madmom root. This\n",
    "                reverses the process used to map from madmom chords to m21 chords above. \n",
    "                '''\n",
    "                # if root is flat,...\n",
    "                if root.accidental == 'accidental flat':\n",
    "                    # ... move down one note\n",
    "                    if root.pitchClass >= 1:\n",
    "                        pcStr = PITCH_CLASSES[root.pitchClass-1]\n",
    "                    else:\n",
    "                        pcStr = PITCH_CLASSES[-1]\n",
    "                else:\n",
    "                    pcStr = PITCH_CLASSES[root.pitchClass]\n",
    "                    \n",
    "                if crd.quality == \"major\": madChordLabel = pcStr + ':maj'\n",
    "                elif crd.quality == \"minor\": madChordLabel = pcStr + ':min'\n",
    "                else: madChordLabel = 'N'\n",
    "                        \n",
    "            else:\n",
    "                madChordLabel = 'N'\n",
    "                    \n",
    "            newLine.append(madChordLabel)\n",
    "            newLines.append(newLine)\n",
    "            \n",
    "    newLines = pd.DataFrame(newLines, index=outChords.index,\n",
    "                            columns=[\"m21RootPitchClass\", \"m21RootPitchAccidental\", \"m21RootOctave\",\n",
    "                                     \"m21ContainsTriad\", \"m21ChordQuality\", \"m21PitchedCommonName\",\n",
    "                                     \"madChordLabel\"])\n",
    "    \n",
    "    outChords = outChords.join(newLines)\n",
    "    outChords = outChords.drop(outChords.index[dels])\n",
    "        \n",
    "    return(outChords)\n",
    "    \n",
    "# test it\n",
    "#dnn_chord_rec(inWav='data/wip/tmp/MAPS_MUS-bach_846_AkPnBcht.wav', \n",
    "#              savTo='data/wip/tmp/MAPS_MUS-bach_846_AkPnBcht.chords.dnn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for cleaning up chords\n",
    "\n",
    "tmp = glob.glob('data/wip/AkPnBcht/MUS/*.chord.y.txt')\n",
    "# 'data/wip/AkPnBcht/MUS/*.chords.dnn.txt'\n",
    "rcdCtr = 0\n",
    "flatCtr = 0\n",
    "m21Ltr = []\n",
    "madLtr = []\n",
    "m21dSharpLtr = []\n",
    "m21aSharpLtr = []\n",
    "\n",
    "for line in tmp:\n",
    "    madChords = load_chords(line)\n",
    "    \n",
    "    for i in range(madChords.shape[0]):\n",
    "        rcdCtr = rcdCtr + 1\n",
    "        if madChords.iloc[i,7][1] == '-':\n",
    "            flatCtr = flatCtr +1\n",
    "            m21Ltr.append(madChords.iloc[i,7][0]) \n",
    "            madLtr.append(madChords.iloc[i,8][0])\n",
    "\n",
    "        if madChords.iloc[i,8][0] == 'D':\n",
    "            m21dSharpLtr.append(madChords.iloc[i,7][0:1]) \n",
    "            \n",
    "        if madChords.iloc[i,8][0] == 'A':\n",
    "            m21aSharpLtr.append(madChords.iloc[i,7][0:1]) \n",
    "            \n",
    "    \n",
    "print(\"rcds:\", rcdCtr, \"flats:\", flatCtr, \"pct:\", flatCtr / rcdCtr)\n",
    "print\n",
    "\n",
    "print(\"when classed '-' by m21, what is the madmom class?\")\n",
    "for i in range(len(PITCH_CLASSES)):\n",
    "    print(PITCH_CLASSES[i], \": m21:\", m21Ltr.count(PITCH_CLASSES[i]), \": mad:\", madLtr.count(PITCH_CLASSES[i]))\n",
    "print\n",
    "    \n",
    "print(\"when classed D# by madmom, what is the m21 class?\")\n",
    "for i in range(len(PITCH_CLASSES)):\n",
    "    print(PITCH_CLASSES[i], \":\" \"m21:\", m21dSharpLtr.count(PITCH_CLASSES[i]))\n",
    "print\n",
    "\n",
    "print(\"when classed A# by madmom, what is the m21 class?\")\n",
    "for i in range(len(PITCH_CLASSES)):\n",
    "    print(PITCH_CLASSES[i], \":\" \"m21:\", m21aSharpLtr.count(PITCH_CLASSES[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = glob.glob('data/wip/AkPnBcht/MUS/*.chords.cnn.txt')\n",
    "pitchClasses = range(12)\n",
    "\n",
    "for line in tmp:\n",
    "    \n",
    "    inChords = load_chords(line)\n",
    "    \n",
    "    for i in range(inChords.shape[0]):\n",
    "        \n",
    "        if inChords.iloc[i,8][1] == '#':\n",
    "            if inChords.iloc[i,8][0] == 'D':\n",
    "                print(\"in D, assigning:\", pitchClasses.index[PITCH_CLASSES == 'E'], \"s/b 5\")\n",
    "                inChords.iloc[i,2] = pitchClasses[PITCH_CLASSES == 'E']\n",
    "                inChords.iloc[i,3] = 'accidental flat'\n",
    "            elif inChords.iloc[i,8][0] == 'A':\n",
    "                print(\"in A, assigning:\", pitchClasses.index[PITCH_CLASSES == 'B'], \"s/b 11\")\n",
    "                inChords.iloc[i,2] = pitchClasses[PITCH_CLASSES == 'B']\n",
    "                inChords.iloc[i,3] = 'accidental flat'\n",
    "                \n",
    "    save_chords(line, inChords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Note Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_note_and_chord_rec(mode, inFile, outFileBase=None):\n",
    "    \n",
    "    if outFileBase is not None:\n",
    "        outNoteFile = outFileBase + '.notes.rnn.txt'\n",
    "        outChordFile = outFileBase + '.chords.rnn.txt'\n",
    "    \n",
    "    # generate and save note transcription from .wav file\n",
    "    if mode == \"single\":\n",
    "        if outFileBase == None: \n",
    "            rtrn = ! PianoTranscriptor single {inFile}\n",
    "            return(rtrn)\n",
    "        else:\n",
    "            ! PianoTranscriptor single {inFile} -o {outNoteFile}\n",
    "    \n",
    "    elif mode == \"batch\":\n",
    "        if outFileBase == None:\n",
    "            print(\"ERROR: need an outFile (really dir) when using using batch mode.\")\n",
    "        else:\n",
    "            # assumes inFile is a list of files. assumes outFile is a directory\n",
    "            for i in range(len(inFile)): \n",
    "                ! PianoTranscriptor batch {inFile[i]} -o {outNoteFile}\n",
    "    \n",
    "    # generate and save chord transcription from note transcription\n",
    "    try:\n",
    "        # MOVE THIS TO RE-FORMATTER\n",
    "            # note save format is onset + midi (as used by eval).\n",
    "            # note -> chord format is onset + offset + midi\n",
    "        rnnNotes = pd.DataFrame(mad.features.notes.load_notes(outNoteFile)) # outNm + '.notes.rnn.txt'\n",
    "        rnnNotes.insert(1, \"Offset\", pd.Series(np.zeros(rnnNotes.shape[0]), index=rnnNotes.index))\n",
    "        rnnNotes.columns = ['OnsetTime', 'OffsetTime', 'MidiPitch']\n",
    "        rnnNotes = rnnNotes.sort_values(['OnsetTime', 'MidiPitch'], # 'OffsetTime', \n",
    "                                        axis=0, ascending=True, inplace=False,\n",
    "                                        kind='quicksort', na_position='last')\n",
    "        rnnNotes[\"MidiPitch\"] = rnnNotes['MidiPitch'].astype(int)\n",
    "    \n",
    "        # get chord preds using RNN notes\n",
    "        N, C = txt_to_y(rnnNotes, \"thisFile\")\n",
    "    \n",
    "        if C is not None: \n",
    "            save_chords(outChordFile, C)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # https://www.safaribooksonline.com/blog/2014/02/12/using-shell-commands-effectively-ipython/\n",
    "\n",
    "#, 'data/maps/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.wav']\n",
    "#rnn_note_and_chord_rec(\"single\", \n",
    "#                       'data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav', \n",
    "#                       'data/wip/tmp/MAPS_MUS-alb_se3_AkPnBcht')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chord Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''achieves 45% major/minor triad recogniation. significantly slower than dnn model'''\n",
    "\n",
    "def cnn_chord_rec(inWav, savTo=None):\n",
    "    \n",
    "    '''from Filip Korzeniowski and Gerhard Widmer, “A Fully Convolutional Deep Auditory Model for \n",
    "    Musical Chord Recognition”, Proceedings of IEEE International Workshop on Machine Learning for\n",
    "    Signal Processing (MLSP), 2016..'''\n",
    "    \n",
    "    # instantiate madmom CNNChordFeatureProcessor\n",
    "    featproc = mad.features.chords.CNNChordFeatureProcessor()\n",
    "    \n",
    "    # create DeepChromaChordRecognitionProcessor to decode chord sequence from extracted chromas\n",
    "    decode = mad.features.chords.CRFChordRecognitionProcessor()\n",
    "    \n",
    "    chordrec = mad.processors.SequentialProcessor([featproc, decode])\n",
    "    \n",
    "    madChords = chordrec(inWav)\n",
    "    \n",
    "    rtrn = to_chords(pd.DataFrame(madChords), 'madChordDF')\n",
    "    # when you decide to expand the output classes:\n",
    "        # 1. use the code here: https://github.com/CPJKU/madmom/blob/master/madmom/features/chords.py\n",
    "        # 2. this: DeepChromaChordRecognitionProcessor() calls this: majmin_targets_to_chord_labels()\n",
    "        # 3. the latter implements 25 classes (including N for no chord) using pred_to_cl(pred)\n",
    "        # 4. so, the net is outputing preds for these classes. class preds are translated to 0-23 + 24 labels\n",
    "        # 5. to go beyond these classes you're going to need to either:\n",
    "            # a. roll-your-own net w/ more classes\n",
    "            # b. cross vector using output from other algos i.e., RNN note or CNN chord... START HERE.\n",
    "    \n",
    "    if savTo is not None:\n",
    "        save_chords(savTo, rtrn) \n",
    "    else:\n",
    "        return(rtrn)\n",
    "\n",
    "#cnn_chord_rec('data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav',\n",
    "#              'data/wip/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.madChordCnn.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''achieves:\n",
    "    1. 22% precision and 6% fscore on collection of map music\n",
    "    2. 35% major/minor triad recognition, \n",
    "    so, clearly the 70% fscore numbers they're giving are on major/minor triads w/in a target range'''\n",
    "\n",
    "def dnn_chord_rec(inWav, savTo=None):\n",
    "    \n",
    "    '''from Filip Korzeniowski and Gerhard Widmer, “Feature Learning for Chord Recognition: The Deep\n",
    "    Chroma Extractor”, Proceedings of the 17th International Society for Music Information Retrieval\n",
    "    Conference (ISMIR), 2016.'''\n",
    "    \n",
    "    # instantiate madmom deep chroma processor to extract chroma vectors\n",
    "    dcp = mad.audio.chroma.DeepChromaProcessor()\n",
    "    \n",
    "    # create DeepChromaChordRecognitionProcessor to decode chord sequence from extracted chromas\n",
    "    decode = mad.features.chords.DeepChromaChordRecognitionProcessor()\n",
    "    \n",
    "    # SequentialProcessor links dcp and decode steps to transcribe chord(s)\n",
    "    chordrec = mad.processors.SequentialProcessor([dcp, decode])\n",
    "    \n",
    "    madChords = chordrec(inWav)\n",
    "    \n",
    "    rtrn = to_chords(pd.DataFrame(madChords), 'madChordDF')\n",
    "    \n",
    "    if savTo is not None:\n",
    "        save_chords(savTo, rtrn)  \n",
    "    else:\n",
    "        return(rtrn)\n",
    "\n",
    "#dnn_rslts = dnn_chord_rec('data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav')\n",
    "\n",
    "#dcc_rslts = dcc_chord_rec('data/maps/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.wav',\n",
    "#                          'data/wip/AkPnBcht/MUS/MAPS_MUS-alb_se3_AkPnBcht.madChordDcc.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Find Common Chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get most commonly occuring chords in a dataset\n",
    "\n",
    "chordFiles = glob.glob('data/wip/AkPnBcht/MUS/*.chords.y.txt')\n",
    "\n",
    "for i in range(len(chordFiles)):\n",
    "    chords = load_chords(chordFiles[i])\n",
    "    #print(chords)\n",
    "    if i == 0:\n",
    "        allChords = chords.m21PitchedCommonName\n",
    "    else:\n",
    "        allChords = allChords.append(chords.m21PitchedCommonName)\n",
    "\n",
    "allChords = allChords.tolist(); print(len(allChords))\n",
    "allChords.sort()\n",
    "uniqueChords = list(set(allChords))\n",
    "uniqueChords.sort()\n",
    "\n",
    "counts = []\n",
    "for i in range(len(uniqueChords)):\n",
    "    counts.append(allChords.count(uniqueChords[i]))\n",
    "\n",
    "print(sum(counts))\n",
    "\n",
    "outP = zip(uniqueChords, counts)\n",
    "outP.sort(key=lambda tup: tup[1], reverse=True)\n",
    "print(pd.DataFrame(outP[0:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chord rec notes:\n",
    "'''http://www.audiocommons.org/assets/files/AC-WP4-UPF-D4.1%20Report%20on%20the%20analysis%20and%20compilation%20of%20state-of-the-art%20methods%20for%20the%20automatic%20annotation%20of%20music%20pieces%20and%20music%20samples.pdf\n",
    "Audio Commons, deliverable d4: Report on the analysis and compilation of state-of-the-art methods \n",
    "for the automatic annotation of music pieces and music samples:\n",
    "\n",
    "Recent work utilize deep learning to learn alternative features for replacing chroma features [Zhou15]. In\n",
    "their work, authors investigate two types of architectures for the neural net, a common one in which the\n",
    "amount of neurons is the same in every layer, and a bottleneck-shaped architecture in which the middle\n",
    "layer has fewer neurons. Grézl et al. claim [Grézl07] that Bottleneck architecture is more suitable to\n",
    "learn high-level features than common one, and that it reduces overfitting. Moreover, following Zhou and\n",
    "Lerch results [Zhou15], it leads to better results for chord recognition than a common architecture.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Evaluate Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTES\n",
    "# 'data/wip/AkPnBcht/MUS/*.note.y.txt'\n",
    "# 'data/wip/AkPnBcht/MUS/*.notes.rnn.txt'\n",
    "noteAnnots = glob.glob('data/wip/AkPnBcht/UCHO/I60-68/*.note.y.txt')\n",
    "noteDetects = glob.glob('data/wip/AkPnBcht/UCHO/I60-68/*.notes.rnn.txt')\n",
    "\n",
    "if len(noteAnnots) == len(noteDetects):\n",
    "    eval_objs = []\n",
    "    for i in range(len(noteAnnots)):\n",
    "        annotations = mad.features.notes.load_notes(noteAnnots[i])\n",
    "        detections = mad.features.notes.load_notes(noteDetects[i])\n",
    "        \n",
    "        eval_objs.append(mad.evaluation.notes.NoteEvaluation(detections, annotations, window=0.025, delay=0))\n",
    "    \n",
    "    print('***** Note performance:')\n",
    "    print(mad.evaluation.notes.NoteSumEvaluation(eval_objs, name=None).tostring())\n",
    "    \n",
    "else:\n",
    "    print(\"missing Annots or Detects\")\n",
    "    end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Evaluate Chords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Onset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def onset_evaluation(detections, annotations, window=WINDOW):\n",
    "    \"\"\"\n",
    "    Determine the true/false positive/negative detections.\n",
    "    Parameters\n",
    "    ----------\n",
    "    detections : numpy array\n",
    "        Detected notes.\n",
    "    annotations : numpy array\n",
    "        Annotated ground truth notes.\n",
    "    window : float, optional\n",
    "        Evaluation window [seconds].\n",
    "    Returns\n",
    "    -------\n",
    "    tp : numpy array, shape (num_tp,)\n",
    "        True positive detections.\n",
    "    fp : numpy array, shape (num_fp,)\n",
    "        False positive detections.\n",
    "    tn : numpy array, shape (0,)\n",
    "        True negative detections (empty, see notes).\n",
    "    fn : numpy array, shape (num_fn,)\n",
    "        False negative detections.\n",
    "    errors : numpy array, shape (num_tp,)\n",
    "        Errors of the true positive detections wrt. the annotations.\n",
    "    Notes\n",
    "    -----\n",
    "    The returned true negative array is empty, because we are not interested\n",
    "    in this class, since it is magnitudes bigger than true positives array.\n",
    "    \"\"\"\n",
    "    # make sure the arrays have the correct types and dimensions\n",
    "    detections = np.asarray(detections) # , dtype=np.float\n",
    "    annotations = np.asarray(annotations) # , dtype=np.float\n",
    "    # TODO: right now, it only works with 1D arrays\n",
    "    if detections.ndim > 1 or annotations.ndim > 1:\n",
    "        raise NotImplementedError('please implement multi-dim support')\n",
    "\n",
    "    # init TP, FP, FN and errors\n",
    "    tp = np.zeros(0)\n",
    "    fp = np.zeros(0)\n",
    "    tn = np.zeros(0)  # we will not alter this array\n",
    "    fn = np.zeros(0)\n",
    "    errors = np.zeros(0)\n",
    "\n",
    "    # if neither detections nor annotations are given\n",
    "    if len(detections) == 0 and len(annotations) == 0:\n",
    "        # return the arrays as is\n",
    "        return tp, fp, tn, fn, errors\n",
    "    # if only detections are given\n",
    "    elif len(annotations) == 0:\n",
    "        # all detections are FP\n",
    "        return tp, detections, tn, fn, errors\n",
    "    # if only annotations are given\n",
    "    elif len(detections) == 0:\n",
    "        # all annotations are FN\n",
    "        return tp, fp, tn, annotations, errors\n",
    "\n",
    "    # window must be greater than 0\n",
    "    if float(window) <= 0:\n",
    "        raise ValueError('window must be greater than 0')\n",
    "\n",
    "    # sort the detections and annotations\n",
    "    det = np.sort(detections)\n",
    "    ann = np.sort(annotations)\n",
    "    # cache variables\n",
    "    det_length = len(detections)\n",
    "    ann_length = len(annotations)\n",
    "    det_index = 0\n",
    "    ann_index = 0\n",
    "    # iterate over all detections and annotations\n",
    "    while det_index < det_length and ann_index < ann_length:\n",
    "        # fetch the first detection\n",
    "        d = det[det_index]\n",
    "        # fetch the first annotation\n",
    "        a = ann[ann_index]\n",
    "        # compare them\n",
    "        if abs(d - a) <= window:\n",
    "            # TP detection\n",
    "            tp = np.append(tp, d)\n",
    "            # append the error to the array\n",
    "            errors = np.append(errors, d - a)\n",
    "            # increase the detection and annotation index\n",
    "            det_index += 1\n",
    "            ann_index += 1\n",
    "        elif d < a:\n",
    "            # FP detection\n",
    "            fp = np.append(fp, d)\n",
    "            # increase the detection index\n",
    "            det_index += 1\n",
    "            # do not increase the annotation index\n",
    "        elif d > a:\n",
    "            # we missed a annotation: FN\n",
    "            fn = np.append(fn, a)\n",
    "            # do not increase the detection index\n",
    "            # increase the annotation index\n",
    "            ann_index += 1\n",
    "        else:\n",
    "            # can't match detected with annotated onset\n",
    "            raise AssertionError('can not match % with %', d, a)\n",
    "    # the remaining detections are FP\n",
    "    fp = np.append(fp, det[det_index:])\n",
    "    # the remaining annotations are FN\n",
    "    fn = np.append(fn, ann[ann_index:])\n",
    "    # check calculations\n",
    "    if len(tp) + len(fp) != len(detections):\n",
    "        raise AssertionError('bad TP / FP calculation')\n",
    "    if len(tp) + len(fn) != len(annotations):\n",
    "        raise AssertionError('bad FN calculation')\n",
    "    if len(tp) != len(errors):\n",
    "        raise AssertionError('bad errors calculation')\n",
    "    # convert to numpy arrays and return them\n",
    "    return np.array(tp), np.array(fp), tn, np.array(fn), np.array(errors)    \n",
    "    \n",
    "def ChordOnsetEvaluation(detections, annotations, window):\n",
    "    \n",
    "    # init TP, FP, TN and FN lists\n",
    "    tp = np.zeros((0, 2))\n",
    "    fp = np.zeros((0, 2))\n",
    "    tn = np.zeros((0, 2))  # this will not be altered\n",
    "    fn = np.zeros((0, 2))\n",
    "    errors = np.zeros((0, 2))\n",
    "    \n",
    "    # get a list of all chords detected / annotated\n",
    "    chords = np.unique(np.concatenate((detections[:,1],\n",
    "                                       annotations[:,1]))).tolist()\n",
    "    #print(\"chords:\")\n",
    "    #print(chords)\n",
    "    # iterate over all chords\n",
    "    for chord in chords:\n",
    "        # perform normal onset detection on each chord\n",
    "        det = detections[detections[:, 1] == chord]\n",
    "        ann = annotations[annotations[:, 1] == chord]\n",
    "        tp_, fp_, _, fn_, err_ = onset_evaluation(det[:, 0], ann[:, 0], window)\n",
    "        \n",
    "        # convert returned arrays to lists and append the detections and\n",
    "        # annotations to the correct lists\n",
    "        tp = np.vstack((tp, det[np.in1d(det[:, 0], tp_)]))\n",
    "        fp = np.vstack((fp, det[np.in1d(det[:, 0], fp_)]))\n",
    "        fn = np.vstack((fn, ann[np.in1d(ann[:, 0], fn_)]))\n",
    "        \n",
    "        # append the chord number to the errors\n",
    "        err_ = np.vstack((np.array(err_),\n",
    "                          np.repeat(np.asarray([chord]), len(err_)))).T\n",
    "        errors = np.vstack((errors, err_))\n",
    "        \n",
    "    # check calculations\n",
    "    #print(\"len(tp):\", len(tp))\n",
    "    #print(\"len(fp):\", len(fp))\n",
    "    #print(\"len(detections:)\", len(detections))\n",
    "    if len(tp) + len(fp) != len(detections):\n",
    "        raise AssertionError('bad TP / FP calculation')\n",
    "    if len(tp) + len(fn) != len(annotations):\n",
    "        raise AssertionError('bad FN calculation')\n",
    "    if len(tp) != len(errors):\n",
    "        raise AssertionError('bad errors calculation')\n",
    "        \n",
    "    # sort the arrays\n",
    "    # Note: The errors must have the same sorting order as the TPs, so they\n",
    "    #       must be done first (before the TPs get sorted)\n",
    "    errors = errors[tp[:, 0].argsort()]\n",
    "    tp = tp[tp[:, 0].argsort()]\n",
    "    fp = fp[fp[:, 0].argsort()]\n",
    "    fn = fn[fn[:, 0].argsort()]\n",
    "    \n",
    "    # return the arrays\n",
    "    return tp, fp, tn, fn, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Chord Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassEvaluation(mad.evaluation.Evaluation):\n",
    "    \"\"\"\n",
    "    Evaluation class for measuring Precision, Recall and F-measure based on\n",
    "    2D numpy arrays with true/false positive/negative detections.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tp : list of tuples or numpy array, shape (num_tp, 2)\n",
    "        True positive detections.\n",
    "    fp : list of tuples or numpy array, shape (num_fp, 2)\n",
    "        False positive detections.\n",
    "    tn : list of tuples or numpy array, shape (num_tn, 2)\n",
    "        True negative detections.\n",
    "    fn : list of tuples or numpy array, shape (num_fn, 2)\n",
    "        False negative detections.\n",
    "    name : str\n",
    "        Name to be displayed.\n",
    "    Notes\n",
    "    -----\n",
    "    The second item of the tuples or the second column of the arrays denote\n",
    "    the class the detection belongs to.\n",
    "    \"\"\"\n",
    "    def __init__(self, tp=None, fp=None, tn=None, fn=None, **kwargs):\n",
    "        # set default values\n",
    "        if tp is None:\n",
    "            tp = np.zeros((0, 2))\n",
    "        if fp is None:\n",
    "            fp = np.zeros((0, 2))\n",
    "        if tn is None:\n",
    "            tn = np.zeros((0, 2))\n",
    "        if fn is None:\n",
    "            fn = np.zeros((0, 2))\n",
    "        super(MultiClassEvaluation, self).__init__(**kwargs)\n",
    "        self.tp = np.asarray(tp) # , dtype=np.float\n",
    "        self.fp = np.asarray(fp) # , dtype=np.float\n",
    "        self.tn = np.asarray(tn) # , dtype=np.float\n",
    "        self.fn = np.asarray(fn) # , dtype=np.float\n",
    "\n",
    "    def tostring(self, verbose=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Format the evaluation metrics as a human readable string.\n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : bool\n",
    "            Add evaluation for individual classes.\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Evaluation metrics formatted as a human readable string.\n",
    "        \"\"\"\n",
    "        ret = ''\n",
    "\n",
    "        if verbose:\n",
    "            # extract all classes\n",
    "            classes = []\n",
    "            if self.tp.any():\n",
    "                classes = np.append(classes, np.unique(self.tp[:, 1]))\n",
    "            if self.fp.any():\n",
    "                classes = np.append(classes, np.unique(self.fp[:, 1]))\n",
    "            if self.tn.any():\n",
    "                classes = np.append(classes, np.unique(self.tn[:, 1]))\n",
    "            if self.fn.any():\n",
    "                classes = np.append(classes, np.unique(self.fn[:, 1]))\n",
    "            for cls in sorted(np.unique(classes)):\n",
    "                # extract the TP, FP, TN and FN of this class\n",
    "                tp = self.tp[self.tp[:, 1] == cls]\n",
    "                fp = self.fp[self.fp[:, 1] == cls]\n",
    "                tn = self.tn[self.tn[:, 1] == cls]\n",
    "                fn = self.fn[self.fn[:, 1] == cls]\n",
    "                # evaluate them\n",
    "                e = Evaluation(tp, fp, tn, fn, name='Class %s' % cls)\n",
    "                # append to the output string\n",
    "                ret += '  %s\\n' % e.tostring(verbose=False)\n",
    "        # normal formatting\n",
    "        ret += 'Annotations: %5d TP: %5d FP: %4d FN: %4d ' \\\n",
    "               'Precision: %.3f Recall: %.3f F-measure: %.3f Acc: %.3f' % \\\n",
    "               (self.num_annotations, self.num_tp, self.num_fp, self.num_fn,\n",
    "                self.precision, self.recall, self.fmeasure, self.accuracy)\n",
    "        # return\n",
    "        return ret\n",
    "\n",
    "    # for chord evaluation with Precision, Recall, F-measure use the Evaluation\n",
    "# class and just define the evaluation function\n",
    "# TODO: extend to also report the measures without octave errors\n",
    "\n",
    "class ChordEvaluation(MultiClassEvaluation):\n",
    "    \"\"\"\n",
    "        Evaluation class for measuring Precision, Recall and F-measure of chords.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        detections : str, list or numpy array\n",
    "            Detected chords.\n",
    "            \n",
    "        annotations : str, list or numpy array\n",
    "            Annotated ground truth chords.\n",
    "            \n",
    "        window : float, optional\n",
    "            F-measure evaluation window [seconds]\n",
    "            \n",
    "        delay : float, optional\n",
    "            Delay the detections `delay` seconds for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, detections, annotations, evalCols, window=WINDOW, delay=0,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # load the chord detections and annotations\n",
    "        detections = pd.read_table(detections)\n",
    "        annotations = pd.read_table(annotations)\n",
    "        \n",
    "        # shift the detections if needed\n",
    "        if delay != 0:\n",
    "            detections.iloc[:, 0] += delay\n",
    "        \n",
    "        # build input array        \n",
    "        if len(evalCols) == 2:\n",
    "            outDet = np.array(detections.iloc[:,evalCols])\n",
    "            outAnn = np.array(annotations.iloc[:,evalCols])\n",
    "            \n",
    "        else:\n",
    "            outDet = detections.iloc[:,evalCols[0]]\n",
    "            outAnn = annotations.iloc[:,evalCols[0]]\n",
    "            \n",
    "            classDet = detections.iloc[:,evalCols[1]].tolist()\n",
    "            classAnn = annotations.iloc[:,evalCols[1]].tolist()\n",
    "\n",
    "            for i in range(2,len(evalCols)):\n",
    "                newDet = detections.iloc[:,evalCols[i]]\n",
    "                newAnn = annotations.iloc[:,evalCols[i]]\n",
    "                \n",
    "                for j in range(len(classDet)):\n",
    "                    classDet[j] = str(classDet[j]) + str(newDet[j])\n",
    "                \n",
    "                for j in range(len(classAnn)):\n",
    "                    classAnn[j] = str(classAnn[j]) + str(newAnn[j])\n",
    "            \n",
    "            outDet = np.array(pd.DataFrame({'a': outDet, 'b':classDet}))\n",
    "            outAnn = np.array(pd.DataFrame({'a': outAnn, 'b':classAnn}))\n",
    "        \n",
    "        # evaluate onsets (passing only onset and classifier)\n",
    "        perf = ChordOnsetEvaluation(outDet, outAnn, window)\n",
    "        tp, fp, tn, fn, errors = perf\n",
    "\n",
    "        super(ChordEvaluation, self).__init__(tp, fp, tn, fn, **kwargs)\n",
    "        self.errors = errors\n",
    "        \n",
    "        # save them for the individual chord evaluation\n",
    "        self.detections = detections\n",
    "        self.annotations = annotations\n",
    "        self.window = window\n",
    "    \n",
    "    @property\n",
    "    def mean_error(self):\n",
    "        \"\"\"Mean of the errors.\"\"\"\n",
    "        warnings.warn('mean_error is given for all chords, this will change!')\n",
    "        if len(self.errors) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return np.nanmean(self.errors[:,0].astype(np.float))\n",
    "    \n",
    "    @property\n",
    "    def std_error(self):\n",
    "        \"\"\"Standard deviation of the errors.\"\"\"\n",
    "        warnings.warn('std_error is given for all chords, this will change!')\n",
    "        if len(self.errors) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return np.nanstd(self.errors[:,0].astype(np.float))\n",
    "    \n",
    "    def tostring(self, chords=False, **kwargs):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ----------\n",
    "            chords : bool, optional\n",
    "                Display detailed output for all individual chords.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            str\n",
    "                Evaluation metrics formatted as a human readable string.\n",
    "        \"\"\"\n",
    "        \n",
    "        ret = ''\n",
    "        if self.name is not None:\n",
    "            ret += '%s\\n  ' % self.name\n",
    "        \n",
    "        # add statistics for the individual chord\n",
    "        if chords:\n",
    "            \n",
    "            # determine which chords are present\n",
    "            chords = []\n",
    "            if self.tp.any():\n",
    "                chords = np.append(chords, np.unique(self.tp[:, 1]))\n",
    "            if self.fp.any():\n",
    "                chords = np.append(chords, np.unique(self.fp[:, 1]))\n",
    "            if self.tn.any():\n",
    "                chords = np.append(chords, np.unique(self.tn[:, 1]))\n",
    "            if self.fn.any():\n",
    "                chords = np.append(chords, np.unique(self.fn[:, 1]))\n",
    "\n",
    "            # evaluate them individually\n",
    "            for chord in sorted(np.unique(chords)):\n",
    "                \n",
    "                # detections and annotations for this chord (only onset times)\n",
    "                det = self.detections[self.detections[:, 1] == chord][:, 0]\n",
    "                ann = self.annotations[self.annotations[:, 1] == chord][:, 0]\n",
    "                name = 'chord %s' % chord\n",
    "                e = mad.evaluation.onsets.OnsetEvaluation(det, ann, self.window, name=name)\n",
    "                \n",
    "                # append to the output string\n",
    "                ret += '  %s\\n' % e.tostring(chords=False)\n",
    "                    \n",
    "        # normal formatting\n",
    "        ret += 'chords: %5d TP: %5d FP: %4d FN: %4d ' \\\n",
    "            'Precision: %.3f Recall: %.3f F-measure: %.3f ' \\\n",
    "                'Acc: %.3f mean: %5.1f ms std: %5.1f ms' % \\\n",
    "                    (self.num_annotations, self.num_tp, self.num_fp, self.num_fn,\n",
    "                     self.precision, self.recall, self.fmeasure, self.accuracy,\n",
    "                     self.mean_error * 1000., self.std_error * 1000.)\n",
    "        # return\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii. Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChordSumEvaluation(mad.evaluation.SumEvaluation, ChordEvaluation):\n",
    "    \"\"\"\n",
    "        Class for summing chord evaluations.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def errors(self):\n",
    "        \"\"\"Errors of the true positive detections wrt. the ground truth.\"\"\"\n",
    "        if not self.eval_objects:\n",
    "            # return empty array\n",
    "            return np.zeros((0, 2))\n",
    "        return np.concatenate([e.errors for e in self.eval_objects])\n",
    "\n",
    "\n",
    "class ChordMeanEvaluation(mad.evaluation.MeanEvaluation, ChordSumEvaluation):\n",
    "    \"\"\"\n",
    "        Class for averaging chord evaluations.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def mean_error(self):\n",
    "        \"\"\"Mean of the errors.\"\"\"\n",
    "        warnings.warn('mean_error is given for all chords, this will change!')\n",
    "        return np.nanmean([e.mean_error for e in self.eval_objects])\n",
    "    \n",
    "    @property\n",
    "    def std_error(self):\n",
    "        \"\"\"Standard deviation of the errors.\"\"\"\n",
    "        warnings.warn('std_error is given for all chords, this will change!')\n",
    "        return np.nanmean([e.std_error for e in self.eval_objects])\n",
    "    \n",
    "    def tostring(self, **kwargs):\n",
    "        \"\"\"\n",
    "            Format the evaluation metrics as a human readable string.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            str\n",
    "                Evaluation metrics formatted as a human readable string.\n",
    "        \"\"\"\n",
    "        \n",
    "        # format with floats instead of integers\n",
    "        ret = ''\n",
    "        if self.name is not None:\n",
    "            ret += '%s\\n  ' % self.name\n",
    "        ret += 'Chords: %5.2f TP: %5.2f FP: %5.2f FN: %5.2f ' \\\n",
    "            'Precision: %.3f Recall: %.3f F-measure: %.3f ' \\\n",
    "                'Acc: %.3f mean: %5.1f ms std: %5.1f ms' % \\\n",
    "                    (self.num_annotations, self.num_tp, self.num_fp, self.num_fn,\n",
    "                     self.precision, self.recall, self.fmeasure, self.accuracy,\n",
    "                     self.mean_error * 1000., self.std_error * 1000.)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHORDS\n",
    "\n",
    "# Evaluate chords\n",
    "'''In music theory, the concept of root denotes the idea that a chord can be represented and named by one of its notes. It is linked to harmonic thinking, that is, to the idea that vertical aggregates of notes can form a single unit, a chord. It is in this sense that one can speak of a \"C chord\", or a \"chord on C\", a chord built from \"C\" and of which the note (or pitch) \"C\" is the root. When a C chord is referred to in Classical music or popular music without a reference to what type of chord it is (either Major or minor, in most cases), this chord is assumed to be a C major triad, which contains the notes C, E and G. The root needs not be the bass note, the lowest note of the chord: the concept of root is linked to that of the inversion of chords, which is derived from the notion of invertible counterpoint. In this concept, chords can be inverted while still retaining their root.\n",
    "\n",
    "'''\n",
    "\n",
    "chdRnnDet = glob.glob('data/wip/AkPnBcht/MUS/*.chords.rnn.txt')\n",
    "chdCnnDet = glob.glob('data/wip/AkPnBcht/MUS/*.chords.cnn.txt')\n",
    "chdDnnDet = glob.glob('data/wip/AkPnBcht/MUS/*.chords.dnn.txt')\n",
    "chdAnnot = glob.glob('data/wip/AkPnBcht/MUS/*.chords.y.txt')\n",
    "\n",
    "fSets = [chdRnnDet, chdCnnDet, chdDnnDet]\n",
    "fSetTtl = ['Note RNN Chord Pred', 'CNN Chord Preds', 'DNN Chord Preds']\n",
    "evalCols = [(0,2),(0,2,3),(0,4),(0,2,3,4),(0,6),(0,8),(0,7)]\n",
    "\n",
    "for h in range(len(fSets)):\n",
    "    pcObjs = []; pObjs = []; oObjs = []; nObjs = []; qObjs = []; madObjs = []; m21Objs = []\n",
    "    \n",
    "    if h == 0: \n",
    "        delay = 0.01 # RNN delay\n",
    "        window = 0.05\n",
    "    elif h == 1:\n",
    "        delay = 0.1 # CNN delay\n",
    "        window = 0.5\n",
    "    elif h == 2:\n",
    "        delay = 0.01 # DNN delay\n",
    "        window = 0.5\n",
    "    \n",
    "    for i in range(len(fSets[h])): \n",
    "        # pitch class performance\n",
    "        pcObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[0], window=window, delay=delay))\n",
    "        \n",
    "        # pitch (class + accidental) performance\n",
    "        pObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[1], window=window, delay=delay))\n",
    "        \n",
    "        # octave performance\n",
    "        oObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[2], window=window, delay=delay))\n",
    "        \n",
    "        # note (pitch + octave) performance\n",
    "        nObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[3], window=window, delay=delay))\n",
    "        \n",
    "        # quality performance\n",
    "        qObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[4], window=window, delay=delay))\n",
    "        \n",
    "        # mad performance\n",
    "        madObjs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[5], window=window, delay=delay))\n",
    "        \n",
    "        # m21 performance\n",
    "        m21Objs.append(ChordEvaluation(fSets[h][i], chdAnnot[i], evalCols[6], window=window, delay=delay))\n",
    "        \n",
    "        \n",
    "    print('**********', fSetTtl[h])\n",
    "    print('*** Pitch Class:')\n",
    "    print(ChordSumEvaluation(pcObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Pitch:')\n",
    "    print(ChordSumEvaluation(pObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Octave:')\n",
    "    print(ChordSumEvaluation(oObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Note:')\n",
    "    print(ChordSumEvaluation(nObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Quality:')\n",
    "    print(ChordSumEvaluation(qObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** Mad Class:')\n",
    "    print(ChordSumEvaluation(madObjs, name=None).tostring())\n",
    "    print\n",
    "    print('*** M21 Class:')\n",
    "    print(ChordSumEvaluation(m21Objs, name=None).tostring())\n",
    "    print\n",
    "    print\n",
    "    \n",
    "    '''note: for cnn / dnn chord rec, precision is the key metric i.e., when I predict, do I predict correctly.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_offset_window(inDetLsts, inDetNms, inAnnotLst):\n",
    "    \n",
    "    # technically should take 5% of rcds to determine best offest, window. not going to do that.    \n",
    "    dly = np.round(np.arange(start=0.01, stop=0.11, step=0.01),3).tolist() + \\\n",
    "    np.round(np.arange(start=0.12, stop=0.22, step=0.02),3).tolist()\n",
    "    # + np.round(np.arange(start=0.23, stop=0.5, step=0.03),3).tolist()\n",
    "    wndw = dly\n",
    "    \n",
    "    evalCols = (0,8)\n",
    "    rslts = []\n",
    "    \n",
    "    # for each prediction algorithm,...\n",
    "    for h in range(len(inDetLsts)):\n",
    "        \n",
    "        lnCtr = 0\n",
    "        rsltMtrx = pd.DataFrame(np.ndarray((len(dly)*len(wndw),7)),\n",
    "                                columns=[\"predictor\", \"dly\", \"wndw\", \"prec\", \"recall\", \"fScr\", \"acc\"])\n",
    "        evalObjs = []\n",
    "        \n",
    "        # ...try different onset delays and...\n",
    "        for i in range(len(dly)):\n",
    "            \n",
    "            # ...different window durations...\n",
    "            for j in range(len(wndw)):\n",
    "                \n",
    "                # ...across the sample of prediction files...\n",
    "                for k in range(len(inDetLsts[h])):\n",
    "                    \n",
    "                    evalObjs.append(ChordEvaluation(inDetLsts[h][k], inAnnotLst[k],\n",
    "                                                    evalCols, window=wndw[j], delay=dly[i]))\n",
    "                \n",
    "                # summarize results across files by window and delay\n",
    "                evalSum = ChordSumEvaluation(evalObjs, name=None)\n",
    "                \n",
    "                rsltMtrx.iloc[lnCtr,0] = inDetNms[h]\n",
    "                rsltMtrx.iloc[lnCtr,1] = dly[i]\n",
    "                rsltMtrx.iloc[lnCtr,2] = wndw[j]\n",
    "                rsltMtrx.iloc[lnCtr,3] = evalSum.precision\n",
    "                rsltMtrx.iloc[lnCtr,4] = evalSum.recall\n",
    "                rsltMtrx.iloc[lnCtr,5] = evalSum.fmeasure\n",
    "                rsltMtrx.iloc[lnCtr,6] = evalSum.accuracy\n",
    "                lnCtr = lnCtr + 1\n",
    "        \n",
    "        rsltMtrx = rsltMtrx.sort_values(by='fScr', axis=0, ascending=False, na_position='last').iloc[0:10,:]\n",
    "        rslts.append(rsltMtrx)\n",
    "    \n",
    "    return(rslts)\n",
    "                \n",
    "rnnDet = ['data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.chords.rnn.txt', \n",
    "          'data/wip/AkPnBcht/MUS/MAPS_MUS-schumm-6_AkPnBcht.chords.rnn.txt'] #glob.glob('data/wip/AkPnBcht/MUS/*.chords.rnn.txt')\n",
    "\n",
    "cnnDet = ['data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.chords.cnn.txt',\n",
    "         'data/wip/AkPnBcht/MUS/MAPS_MUS-schumm-6_AkPnBcht.chords.cnn.txt'] #glob.glob('data/wip/AkPnBcht/MUS/*.chords.cnn.txt')\n",
    "\n",
    "dnnDet = ['data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.chords.dnn.txt',\n",
    "         'data/wip/AkPnBcht/MUS/MAPS_MUS-schumm-6_AkPnBcht.chords.dnn.txt'] #glob.glob('data/wip/AkPnBcht/MUS/*.chords.dnn.txt')\n",
    "\n",
    "fSet = [rnnDet, cnnDet, dnnDet]\n",
    "\n",
    "fSetNm = ['RNN Chord Preds', 'CNN Chord Preds', 'DNN Chord Preds']\n",
    "\n",
    "annot = ['data/wip/AkPnBcht/MUS/MAPS_MUS-bach_846_AkPnBcht.chords.y.txt',\n",
    "        'data/wip/AkPnBcht/MUS/MAPS_MUS-schumm-6_AkPnBcht.chords.y.txt']    #glob.glob('data/wip/AkPnBcht/MUS/*.chords.y.txt')\n",
    "\n",
    "rslt = find_offset_window(fSet, fSetNm, annot)\n",
    "\n",
    "'''Looking for best delay and window, I find that by max'ing window up to 0.5 seconds, I still only achieve\n",
    "0.044 and 0.033 F Scores for CNN and DNN chord recognition algos vs. 0.9 for RNN note -> chord rec. The basic\n",
    "problem is: \n",
    "1. that the CNN and DNN chord recognition algos work only in a narrow range of:\n",
    "    a. major/minor triads, \n",
    "    b. in mid-range keys e.g., midi 60-70. even then they only achieve 0.4-ish and 0.3-ish F Scores.\n",
    "2. while the most popular, they represent a very small portion of total chords played.\n",
    "3. the more accurate CNN algo is prohibitively slow, inappropriate for online processing.\n",
    "\n",
    "It's unlikely that they will contribute meaningfully to an improvement to the RNN Chord Rec algo. So, rather\n",
    "than attempt to combine records then use a GBM, time is better focused on:\n",
    "1. operationalizing the RNN, then\n",
    "2. introducing the language model... using index search to find song so that next note/chord expectation is known, then\n",
    "3. swinging back at some point to train the algos on a broader set of keys, speed processing, etc. when you do this, \n",
    "here are the top 30 chords w/ window = 0.01 rounding:\n",
    "0    C4-interval class 4  136\n",
    "1         G2-major triad  135\n",
    "2              A2-unison  120\n",
    "3         F4-major triad  119\n",
    "4   B-4-interval class 4  116\n",
    "5    D4-interval class 3  115\n",
    "6   E-4-interval class 4  110\n",
    "7    E4-interval class 3  108\n",
    "8              B2-unison  107\n",
    "9    G4-interval class 4  106\n",
    "10   C5-interval class 4  105\n",
    "11  G#4-interval class 3  104\n",
    "12   C4-interval class 3  103\n",
    "13  G#3-interval class 3  100\n",
    "14   B3-interval class 3   97\n",
    "15             F3-unison   97\n",
    "16  F#4-interval class 3   96\n",
    "17  E-5-interval class 4   94\n",
    "18            G#2-unison   92\n",
    "19   G3-interval class 4   92\n",
    "20        G4-major triad   92\n",
    "21            C#3-unison   91\n",
    "22             E2-unison   91\n",
    "23             A3-unison   89\n",
    "24  C#4-interval class 3   89\n",
    "25             D2-unison   88\n",
    "26   A4-interval class 3   87\n",
    "27       E-4-major triad   87\n",
    "28  B-3-interval class 4   85\n",
    "29       E-3-major triad   84\n",
    "#def combined_chord_pred():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Read / Write Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_maps_note_file(txt_file):\n",
    "    # reads text file into dataframe, sort and round\n",
    "    \n",
    "    lines = [line.rstrip('\\n').split('\\t') for line in open(txt_file, 'U')]\n",
    "    \n",
    "    headers = lines[0]; lines = lines[1:len(lines)]\n",
    "    \n",
    "    lines = pd.DataFrame(lines, columns=[headers[0], headers[1], \n",
    "                                         headers[2]]).convert_objects(convert_numeric=True)\n",
    "    \n",
    "    lines = lines.round({'OnsetTime': 2, 'OffsetTime': 2, 'MidiPitch': 0})\n",
    "    \n",
    "    lines = lines.sort_values(['OnsetTime', 'MidiPitch'], # 'OffsetTime', \n",
    "                              axis=0, ascending=True, inplace=False, \n",
    "                              kind='quicksort', na_position='last')\n",
    "    \n",
    "    '''sort is tricky. some chord notes have same onset, different offsets. technically, the shorter note\n",
    "    should probably appear first (i.e., ascending sort: onset, offset, pitch). problem is you get an \n",
    "    expanding set of chord classes most of which don't sound musically different (i.e., imperceptible \n",
    "    differences in offset). so, i'm sorting above using ascending: onset, midi (after pitch, offset does not matter)''' \n",
    "    \n",
    "    \n",
    "    return(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Music / Voice Discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Configure Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. In From Microphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Establish Microphone Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# this and next are components of __init__.py\n",
    "\n",
    "\"\"\"Library for performing speech recognition, with support for several engines and APIs, online and offline.\"\"\"\n",
    "\n",
    "__author__ = \"Anthony Zhang (Uberi)\"\n",
    "__version__ = \"3.4.6\"\n",
    "__license__ = \"BSD\"\n",
    "\n",
    "import io, os, subprocess, wave, aifc, math, audioop\n",
    "import collections, threading\n",
    "import platform, stat\n",
    "import json, hashlib, hmac, time, base64, random, uuid\n",
    "import tempfile, shutil\n",
    "\n",
    "try: # attempt to use the Python 2 modules\n",
    "    from urllib import urlencode\n",
    "    from urllib2 import Request, urlopen, URLError, HTTPError\n",
    "except ImportError: # use the Python 3 modules\n",
    "    from urllib.parse import urlencode\n",
    "    from urllib.request import Request, urlopen\n",
    "    from urllib.error import URLError, HTTPError\n",
    "\n",
    "# define exceptions\n",
    "class WaitTimeoutError(Exception): pass\n",
    "class RequestError(Exception): pass\n",
    "class UnknownValueError(Exception): pass\n",
    "\n",
    "class AudioSource(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\"this is an abstract class\")\n",
    "\n",
    "    def __enter__(self):\n",
    "        raise NotImplementedError(\"this is an abstract class\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        raise NotImplementedError(\"this is an abstract class\")\n",
    "\n",
    "class Microphone(AudioSource):\n",
    "    \"\"\"\n",
    "    Creates a new ``Microphone`` instance, which represents a physical microphone on the computer. Subclass of\n",
    "    ``AudioSource``. This will throw an ``AttributeError`` if you don't have PyAudio 0.2.9 or later installed.\n",
    "    If ``device_index`` is unspecified or ``None``, the default microphone is used as the audio source. Otherwise,\n",
    "    ``device_index`` should be the index of the device to use for audio input. A device index is an integer between\n",
    "    0 and ``pyaudio.get_device_count() - 1`` (assume we have used ``import pyaudio`` beforehand) inclusive. It \n",
    "    represents an audio device such as a microphone or speaker. See the `PyAudio documentation <http://people.csail.mit.edu/hubert/pyaudio/docs/>`__ .\n",
    "    \n",
    "    The microphone audio is recorded in chunks of ``chunk_size`` and samples, at a rate of ``sample_rate`` samples\n",
    "    per second (Hertz). Higher ``sample_rate`` values result in better audio quality, but also more bandwidth \n",
    "    (and therefore, slower recognition). Additionally, some machines, such as some Raspberry Pi models, can't \n",
    "    keep up if this value is too high. Higher ``chunk_size`` values help avoid triggering on rapidly changing \n",
    "    ambient noise, but also makes detection less sensitive. This value, generally, should be left at its default.\n",
    "    \"\"\"\n",
    "    def __init__(self, device_index = None, sample_rate = 44100, chunk_size = 1024): #sample_rate = 16000\n",
    "        # CONFIGURE MICROPHONE TO JACK. THEN PASS JACK INDEX AS DEVICE_INDEX\n",
    "        # DEFAULT SAMPLE S/B 22,050 OR 44,100\n",
    "        # set up PyAudio\n",
    "        self.pyaudio_module = self.get_pyaudio()\n",
    "\n",
    "        assert device_index is None or isinstance(device_index, int), \"Device index must be None or an integer\"\n",
    "        if device_index is not None: # ensure device index is in range\n",
    "            audio = self.pyaudio_module.PyAudio()\n",
    "            try:\n",
    "                count = audio.get_device_count() # obtain device count\n",
    "            except:\n",
    "                audio.terminate()\n",
    "                raise\n",
    "            assert 0 <= device_index < count, \"Device index out of range ({0} devices available; device index should be between 0 and {1} inclusive)\".format(count, count - 1)\n",
    "        assert isinstance(sample_rate, int) and sample_rate > 0, \"Sample rate must be a positive integer\"\n",
    "        assert isinstance(chunk_size, int) and chunk_size > 0, \"Chunk size must be a positive integer\"\n",
    "        self.device_index = device_index\n",
    "        self.format = self.pyaudio_module.paInt16 # 16-bit int sampling THIS SHOULD PROBABLY CHANGE TO PA.JACK\n",
    "        self.SAMPLE_WIDTH = self.pyaudio_module.get_sample_size(self.format) # size of each sample\n",
    "        print(\"microphone:init:SAMPLE_WIDTH:\",self.SAMPLE_WIDTH)\n",
    "        self.SAMPLE_RATE = sample_rate # sampling rate in Hertz\n",
    "        print(\"microphone:init:sample_rate:\",self.SAMPLE_RATE)\n",
    "        self.CHUNK = chunk_size # number of frames stored in each buffer\n",
    "        print(\"microphone:init:chunk_size:\",self.CHUNK)\n",
    "        self.audio = None\n",
    "        self.stream = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pyaudio():\n",
    "        \"\"\"\n",
    "        Imports the pyaudio module and checks its version. Throws exceptions if pyaudio can't be found or a wrong\n",
    "        version is installed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import pyaudio\n",
    "        except ImportError:\n",
    "            raise AttributeError(\"Could not find PyAudio; check installation\")\n",
    "        from distutils.version import LooseVersion\n",
    "        if LooseVersion(pyaudio.__version__) < LooseVersion(\"0.2.9\"):\n",
    "            raise AttributeError(\"PyAudio 0.2.9 or later is required (found version {0})\".format(pyaudio.__version__))\n",
    "        return pyaudio\n",
    "\n",
    "    @staticmethod\n",
    "    def list_microphone_names():\n",
    "        \"\"\"\n",
    "        Returns a list of the names of all available microphones. For microphones where the name can't be retrieved,\n",
    "        the list entry contains ``None`` instead. The index of each microphone's name is the same as its device index\n",
    "        when creating a ``Microphone`` instance - indices in this list can be used as values of ``device_index``.\n",
    "        \"\"\"\n",
    "        audio = Microphone.get_pyaudio().PyAudio()\n",
    "        try:\n",
    "            result = []\n",
    "            for i in range(audio.get_device_count()):\n",
    "                device_info = audio.get_device_info_by_index(i)\n",
    "                result.append(device_info.get(\"name\"))\n",
    "        finally:\n",
    "            audio.terminate()\n",
    "        return result\n",
    "\n",
    "    def __enter__(self):\n",
    "        assert self.stream is None, \"This audio source is already inside a context manager\"\n",
    "        self.audio = self.pyaudio_module.PyAudio()\n",
    "        try:\n",
    "            self.stream = Microphone.MicrophoneStream(\n",
    "                self.audio.open(\n",
    "                    input_device_index = self.device_index, channels = 1,\n",
    "                    format = self.format, rate = self.SAMPLE_RATE, frames_per_buffer = self.CHUNK,\n",
    "                    input = True, # stream is an input stream\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            self.audio.terminate()\n",
    "            raise\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        try:\n",
    "            self.stream.close()\n",
    "        finally:\n",
    "            self.stream = None\n",
    "            self.audio.terminate()\n",
    "\n",
    "    class MicrophoneStream(object):\n",
    "        def __init__(self, pyaudio_stream):\n",
    "            self.pyaudio_stream = pyaudio_stream\n",
    "\n",
    "        def read(self, size):\n",
    "            return self.pyaudio_stream.read(size, exception_on_overflow = False)\n",
    "\n",
    "        def close(self):\n",
    "            try:\n",
    "                # sometimes, if the stream isn't stopped, closing the stream throws an exception\n",
    "                if not self.pyaudio_stream.is_stopped():\n",
    "                    self.pyaudio_stream.stop_stream()\n",
    "            finally:\n",
    "                self.pyaudio_stream.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Process Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioFile(AudioSource):\n",
    "    \"\"\"\n",
    "    Creates a new ``AudioFile`` instance given a WAV/AIFF/FLAC audio file `filename_or_fileobject`. Subclass of\n",
    "    ``AudioSource``. If ``filename_or_fileobject`` is a string, then it is interpreted as a path to an audio file\n",
    "    on the filesystem. Otherwise, ``filename_or_fileobject`` should be a file-like object such as ``io.BytesIO`` \n",
    "    or similar.\n",
    "    \n",
    "    Note: io. is the core python toolset for working with streams https://docs.python.org/2/library/io.html.\n",
    "    \n",
    "    Note that functions that read from the audio (such as ``recognizer_instance.record`` or \n",
    "    ``recognizer_instance.listen``) will move ahead in the stream. For example, if you execute \n",
    "    ``recognizer_instance.record(audiofile_instance, duration=10)`` twice, the first time it will return the first\n",
    "    10 seconds of audio, and the second time it will return the 10 seconds of audio right after that. This is always\n",
    "    reset to the beginning when entering an ``AudioFile`` context.\n",
    "    \n",
    "    WAV files must be in PCM/LPCM format; WAVE_FORMAT_EXTENSIBLE and compressed WAV are not supported and may \n",
    "    result in undefined behaviour. Both AIFF and AIFF-C (compressed AIFF) formats are supported.\n",
    "    FLAC files must be in native FLAC format; OGG-FLAC is not supported and may result in undefined behaviour.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename_or_fileobject):\n",
    "        if str is bytes: # Python 2 - if a file path is specified, it must either be a `str` instance or a `unicode` instance\n",
    "            assert isinstance(filename_or_fileobject, (str, unicode)) or hasattr(filename_or_fileobject, \"read\"), \"Given audio file must be a filename string or a file-like object\"\n",
    "        else: # Python 3 - if a file path is specified, it must be a `str` instance\n",
    "            assert isinstance(filename_or_fileobject, str) or hasattr(filename_or_fileobject, \"read\"), \"Given audio file must be a filename string or a file-like object\"\n",
    "        self.filename_or_fileobject = filename_or_fileobject\n",
    "        self.stream = None\n",
    "        self.DURATION = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        assert self.stream is None, \"This audio source is already inside a context manager\"\n",
    "        try:\n",
    "            # attempt to read the file as WAV\n",
    "            self.audio_reader = wave.open(self.filename_or_fileobject, \"rb\")\n",
    "            self.little_endian = True \n",
    "            # RIFF WAV is a little-endian format (most ``audioop`` operations assume frames are stored in little-endian form)\n",
    "        except wave.Error:\n",
    "            try:\n",
    "                # attempt to read the file as AIFF\n",
    "                self.audio_reader = aifc.open(self.filename_or_fileobject, \"rb\")\n",
    "                self.little_endian = False # AIFF is a big-endian format\n",
    "            except aifc.Error:\n",
    "                # attempt to read the file as FLAC\n",
    "                if hasattr(self.filename_or_fileobject, \"read\"):\n",
    "                    flac_data = self.filename_or_fileobject.read()\n",
    "                else:\n",
    "                    with open(self.filename_or_fileobject, \"rb\") as f: flac_data = f.read()\n",
    "\n",
    "                # run the FLAC converter with the FLAC data to get the AIFF data\n",
    "                flac_converter = get_flac_converter()\n",
    "                process = subprocess.Popen([\n",
    "                    flac_converter,\n",
    "                    \"--stdout\", \"--totally-silent\", # put the resulting AIFF file in stdout, and make sure it's not mixed with any program output\n",
    "                    \"--decode\", \"--force-aiff-format\", # decode the FLAC file into an AIFF file\n",
    "                    \"-\", # the input FLAC file contents will be given in stdin\n",
    "                ], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "                aiff_data, stderr = process.communicate(flac_data)\n",
    "                aiff_file = io.BytesIO(aiff_data)\n",
    "                try:\n",
    "                    self.audio_reader = aifc.open(aiff_file, \"rb\")\n",
    "                except aifc.Error:\n",
    "                    assert False, \"Audio file could not be read as WAV, AIFF, or FLAC; check if file is corrupted\"\n",
    "                self.little_endian = False # AIFF is a big-endian format\n",
    "        assert 1 <= self.audio_reader.getnchannels() <= 2, \"Audio must be mono or stereo\"\n",
    "        self.SAMPLE_WIDTH = self.audio_reader.getsampwidth()\n",
    "\n",
    "        # 24-bit audio needs some special handling for old Python versions (workaround for https://bugs.python.org/issue12866)\n",
    "        samples_24_bit_pretending_to_be_32_bit = False\n",
    "        if self.SAMPLE_WIDTH == 3: # 24-bit audio\n",
    "            try: audioop.bias(b\"\", self.SAMPLE_WIDTH, 0) # test whether this sample width is supported (for example, ``audioop`` in Python 3.3 and below don't support sample width 3, while Python 3.4+ do)\n",
    "            except audioop.error: # this version of audioop doesn't support 24-bit audio (probably Python 3.3 or less)\n",
    "                samples_24_bit_pretending_to_be_32_bit = True # while the ``AudioFile`` instance will outwardly appear to be 32-bit, it will actually internally be 24-bit\n",
    "                self.SAMPLE_WIDTH = 4 # the ``AudioFile`` instance should present itself as a 32-bit stream now, since we'll be converting into 32-bit on the fly when reading\n",
    "\n",
    "        self.SAMPLE_RATE = self.audio_reader.getframerate()\n",
    "        self.CHUNK = 4096\n",
    "        self.FRAME_COUNT = self.audio_reader.getnframes()\n",
    "        self.DURATION = self.FRAME_COUNT / float(self.SAMPLE_RATE)\n",
    "        self.stream = AudioFile.AudioFileStream(self.audio_reader, self.little_endian, samples_24_bit_pretending_to_be_32_bit)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if not hasattr(self.filename_or_fileobject, \"read\"): # only close the file if it was opened by this class in the first place (if the file was originally given as a path)\n",
    "            self.audio_reader.close()\n",
    "        self.stream = None\n",
    "        self.DURATION = None\n",
    "\n",
    "    class AudioFileStream(object):\n",
    "        def __init__(self, audio_reader, little_endian, samples_24_bit_pretending_to_be_32_bit):\n",
    "            self.audio_reader = audio_reader # an audio file object (e.g., a `wave.Wave_read` instance)\n",
    "            self.little_endian = little_endian # whether the audio data is little-endian (when working with big-endian things, we'll have to convert it to little-endian before we process it)\n",
    "            self.samples_24_bit_pretending_to_be_32_bit = samples_24_bit_pretending_to_be_32_bit # this is true if the audio is 24-bit audio, but 24-bit audio isn't supported, so we have to pretend that this is 32-bit audio and convert it on the fly\n",
    "\n",
    "        def read(self, size = -1):\n",
    "            buffer = self.audio_reader.readframes(self.audio_reader.getnframes() if size == -1 else size)\n",
    "            if not isinstance(buffer, bytes): buffer = b\"\" # workaround for https://bugs.python.org/issue24608\n",
    "\n",
    "            sample_width = self.audio_reader.getsampwidth()\n",
    "            if not self.little_endian: # big endian format, convert to little endian on the fly\n",
    "                if hasattr(audioop, \"byteswap\"): # ``audioop.byteswap`` was only added in Python 3.4 (incidentally, that also means that we don't need to worry about 24-bit audio being unsupported, since Python 3.4+ always has that functionality)\n",
    "                    buffer = audioop.byteswap(buffer, sample_width)\n",
    "                else: # manually reverse the bytes of each sample, which is slower but works well enough as a fallback\n",
    "                    buffer = buffer[sample_width - 1::-1] + b\"\".join(buffer[i + sample_width:i:-1] for i in range(sample_width - 1, len(buffer), sample_width))\n",
    "\n",
    "            # workaround for https://bugs.python.org/issue12866\n",
    "            if self.samples_24_bit_pretending_to_be_32_bit: # we need to convert samples from 24-bit to 32-bit before we can process them with ``audioop`` functions\n",
    "                buffer = b\"\".join(\"\\x00\" + buffer[i:i + sample_width] for i in range(0, len(buffer), sample_width)) # since we're in little endian, we prepend a zero byte to each 24-bit sample to get a 32-bit sample\n",
    "            if self.audio_reader.getnchannels() != 1: # stereo audio\n",
    "                buffer = audioop.tomono(buffer, sample_width, 1, 1) # convert stereo audio data to mono\n",
    "            return buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Extract Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioData(object):\n",
    "    def __init__(self, frame_data, sample_rate, sample_width):\n",
    "        assert sample_rate > 0, \"Sample rate must be a positive integer\"\n",
    "        assert sample_width % 1 == 0 and 1 <= sample_width <= 4, \"Sample width must be between 1 and 4 inclusive\"\n",
    "        self.frame_data = frame_data\n",
    "        self.sample_rate = sample_rate\n",
    "        self.sample_width = int(sample_width)\n",
    "\n",
    "    def get_raw_data(self, convert_rate = None, convert_width = None):\n",
    "        \"\"\"\n",
    "        Returns a byte string representing the raw frame data for the audio represented by the ``AudioData`` instance.\n",
    "        \n",
    "        If ``convert_rate`` is specified and the audio sample rate is not ``convert_rate`` Hz, the resulting audio \n",
    "        is resampled to match.\n",
    "        \n",
    "        If ``convert_width`` is specified and the audio samples are not ``convert_width`` bytes each, the resulting \n",
    "        audio is converted to match. Writing these bytes directly to a file results in a valid `RAW/PCM audio file\n",
    "        <https://en.wikipedia.org/wiki/Raw_audio_format>`__.\n",
    "        \"\"\"\n",
    "        assert convert_rate is None or convert_rate > 0, \"Sample rate to convert to must be a positive integer\"\n",
    "        assert convert_width is None or (convert_width % 1 == 0 and 1 <= convert_width <= 4), \"Sample width to convert to must be between 1 and 4 inclusive\"\n",
    "\n",
    "        raw_data = self.frame_data\n",
    "\n",
    "        # make sure unsigned 8-bit audio (which uses unsigned samples) is handled like higher sample width audio (which uses signed samples)\n",
    "        if self.sample_width == 1:\n",
    "            raw_data = audioop.bias(raw_data, 1, -128) # subtract 128 from every sample to make them act like signed samples\n",
    "\n",
    "        # resample audio at the desired rate if specified\n",
    "        if convert_rate is not None and self.sample_rate != convert_rate:\n",
    "            raw_data, _ = audioop.ratecv(raw_data, self.sample_width, 1, self.sample_rate, convert_rate, None)\n",
    "\n",
    "        # convert samples to desired sample width if specified\n",
    "        if convert_width is not None and self.sample_width != convert_width:\n",
    "            if convert_width == 3: # we're converting the audio into 24-bit (workaround for https://bugs.python.org/issue12866)\n",
    "                raw_data = audioop.lin2lin(raw_data, self.sample_width, 4) # convert audio into 32-bit first, which is always supported\n",
    "                try: audioop.bias(b\"\", 3, 0) # test whether 24-bit audio is supported (for example, ``audioop`` in Python 3.3 and below don't support sample width 3, while Python 3.4+ do)\n",
    "                except audioop.error: # this version of audioop doesn't support 24-bit audio (probably Python 3.3 or less)\n",
    "                    raw_data = b\"\".join(raw_data[i + 1:i + 4] for i in range(0, len(raw_data), 4)) # since we're in little endian, we discard the first byte from each 32-bit sample to get a 24-bit sample\n",
    "                else: # 24-bit audio fully supported, we don't need to shim anything\n",
    "                    raw_data = audioop.lin2lin(raw_data, self.sample_width, convert_width)\n",
    "            else:\n",
    "                raw_data = audioop.lin2lin(raw_data, self.sample_width, convert_width)\n",
    "\n",
    "        # if the output is 8-bit audio with unsigned samples, convert the samples we've been treating as signed to unsigned again\n",
    "        if convert_width == 1:\n",
    "            raw_data = audioop.bias(raw_data, 1, 128) # add 128 to every sample to make them act like unsigned samples again\n",
    "\n",
    "        return raw_data\n",
    "\n",
    "    def get_wav_data(self, convert_rate = None, convert_width = None):\n",
    "        \"\"\"\n",
    "        Returns a byte string representing the contents of a WAV file containing the audio represented by the \n",
    "        ``AudioData`` instance. If ``convert_width`` is specified and the audio samples are not ``convert_width`` \n",
    "        bytes each, the resulting audio is converted to match. If ``convert_rate`` is specified and the audio sample\n",
    "        rate is not ``convert_rate`` Hz, the resulting audio is resampled to match. Writing these bytes directly to \n",
    "        a file results in a valid `WAV file <https://en.wikipedia.org/wiki/WAV>`__.\n",
    "        \"\"\"\n",
    "        raw_data = self.get_raw_data(convert_rate, convert_width)\n",
    "        sample_rate = self.sample_rate if convert_rate is None else convert_rate\n",
    "        sample_width = self.sample_width if convert_width is None else convert_width\n",
    "        \n",
    "        # generate the WAV file contents\n",
    "        wav_writer = wave.open('data/wip/tmp/vrTmp.wav', 'wb')\n",
    "        try:\n",
    "            wav_writer.setframerate(sample_rate)\n",
    "            wav_writer.setsampwidth(sample_width)\n",
    "            wav_writer.setnchannels(1)\n",
    "            wav_writer.writeframes(raw_data) # multiplying by 10 serves to repeat the file 10x, not increase vol.\n",
    "        finally:  # make sure resources are cleaned up\n",
    "            wav_writer.close()\n",
    "                \n",
    "        # generate the WAV file contents\n",
    "        with io.BytesIO() as wav_file:\n",
    "            wav_writer = wave.open(wav_file, \"wb\")\n",
    "            try: # note that we can't use context manager, since that was only added in Python 3.4\n",
    "                wav_writer.setframerate(sample_rate)\n",
    "                wav_writer.setsampwidth(sample_width)\n",
    "                wav_writer.setnchannels(1)\n",
    "                wav_writer.writeframes(raw_data)       \n",
    "                wav_data = wav_file.getvalue()\n",
    "            finally:  # make sure resources are cleaned up\n",
    "                wav_writer.close()\n",
    "                        \n",
    "        return wav_data\n",
    "\n",
    "    def get_aiff_data(self, convert_rate = None, convert_width = None):\n",
    "        \"\"\"\n",
    "        Returns a byte string representing the contents of an AIFF-C file containing the audio represented by \n",
    "        the ``AudioData`` instance. If ``convert_width`` is specified and the audio samples are not ``convert_width``\n",
    "        bytes each, the resulting audio is converted to match. If ``convert_rate`` is specified and the audio sample\n",
    "        rate is not ``convert_rate`` Hz, the resulting audio is resampled to match. Writing these bytes directly to\n",
    "        a file results in a valid `AIFF-C file <https://en.wikipedia.org/wiki/Audio_Interchange_File_Format>`__.\n",
    "        \"\"\"\n",
    "        raw_data = self.get_raw_data(convert_rate, convert_width)\n",
    "        sample_rate = self.sample_rate if convert_rate is None else convert_rate\n",
    "        sample_width = self.sample_width if convert_width is None else convert_width\n",
    "\n",
    "        # the AIFF format is big-endian, so we need to covnert the little-endian raw data to big-endian\n",
    "        if hasattr(audioop, \"byteswap\"): # ``audioop.byteswap`` was only added in Python 3.4\n",
    "            raw_data = audioop.byteswap(raw_data, sample_width)\n",
    "        else: # manually reverse the bytes of each sample, which is slower but works well enough as a fallback\n",
    "            raw_data = raw_data[sample_width - 1::-1] + b\"\".join(raw_data[i + sample_width:i:-1] for i in range(sample_width - 1, len(raw_data), sample_width))\n",
    "\n",
    "        # generate the AIFF-C file contents\n",
    "        with io.BytesIO() as aiff_file:\n",
    "            aiff_writer = aifc.open(aiff_file, \"wb\")\n",
    "            try: # note that we can't use context manager, since that was only added in Python 3.4\n",
    "                aiff_writer.setframerate(sample_rate)\n",
    "                aiff_writer.setsampwidth(sample_width)\n",
    "                aiff_writer.setnchannels(1)\n",
    "                aiff_writer.writeframes(raw_data)\n",
    "                aiff_data = aiff_file.getvalue()\n",
    "            finally:  # make sure resources are cleaned up\n",
    "                aiff_writer.close()\n",
    "        return aiff_data\n",
    "\n",
    "    def get_flac_data(self, convert_rate = None, convert_width = None):\n",
    "        \"\"\"\n",
    "        Returns a byte string representing the contents of a FLAC file containing the audio represented by the \n",
    "        ``AudioData`` instance. Note that 32-bit FLAC is not supported. If the audio data is 32-bit and \n",
    "        ``convert_width`` is not specified, then the resulting FLAC will be a 24-bit FLAC. If ``convert_rate`` \n",
    "        is specified and the audio sample rate is not ``convert_rate`` Hz, the resulting audio is resampled to match.\n",
    "        If ``convert_width`` is specified and the audio samples are not ``convert_width`` bytes each, the resulting \n",
    "        audio is converted to match. Writing these bytes directly to a file results in a valid `FLAC file \n",
    "        <https://en.wikipedia.org/wiki/FLAC>`__.\n",
    "        \"\"\"\n",
    "        assert convert_width is None or (convert_width % 1 == 0 and 1 <= convert_width <= 3), \"Sample width to convert to must be between 1 and 3 inclusive\"\n",
    "\n",
    "        if self.sample_width > 3 and convert_width is None: # resulting WAV data would be 32-bit, which is not convertable to FLAC using our encoder\n",
    "            convert_width = 3 # the largest supported sample width is 24-bit, so we'll limit the sample width to that\n",
    "\n",
    "        # run the FLAC converter with the WAV data to get the FLAC data\n",
    "        wav_data = self.get_wav_data(convert_rate, convert_width)\n",
    "        flac_converter = get_flac_converter()\n",
    "        process = subprocess.Popen([\n",
    "            flac_converter,\n",
    "            \"--stdout\", \"--totally-silent\", # put the resulting FLAC file in stdout, and make sure it's not mixed with any program output\n",
    "            \"--best\", # highest level of compression available\n",
    "            \"-\", # the input FLAC file contents will be given in stdin\n",
    "        ], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        flac_data, stderr = process.communicate(wav_data)\n",
    "        return flac_data       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Out To MuseScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Register Jack Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jack.Port('system:capture_1'), jack.Port('system:capture_2'), jack.Port('system:playback_1'), jack.Port('system:monitor_1'), jack.Port('system:playback_2'), jack.Port('system:monitor_2'), jack.MidiPort('mscore:mscore-midi-1'), jack.MidiPort('mscore:mscore-midiin-1')]\n",
      "music_client\n",
      "44100\n",
      "512\n",
      "<jack.Status 0x0: no flags set>\n",
      "True\n",
      "215\n",
      "3968718\n",
      "3968512\n",
      "1.06996965408\n"
     ]
    }
   ],
   "source": [
    "import jack\n",
    "\n",
    "'''Client object is a context manager. Can be used in a with statement to activate() in the beginning and \n",
    "deactivate() and close() at end. parms: name (str, desired client name), use_exact_name (bool, raise error if name\n",
    "is not unique), no_start_server (bool, don't start JACK server when it is not already running), servername (str, \n",
    "server names are unique to each user, if unspecified, use \"default\" unless JACK_DEFAULT_SERVER is defined in the \n",
    "process environment), session_id (str, pass SessionID Token allowing sessionmanager to identify client again. 4 \n",
    "types of client ports: Client.inports, Client.outports, Client.midi_inports and Client.midi_outports. \n",
    "https://jackclient-python.readthedocs.io/en/0.4.1/#jack.Port'''\n",
    "    \n",
    "client = jack.Client(\"music_client\") # client is object name used in code. 'music_client' is port name in Jack server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Register Ports on Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jack.Port('system:capture_1'), jack.Port('system:capture_2'), jack.Port('system:playback_1'), jack.Port('system:monitor_1'), jack.Port('system:playback_2'), jack.Port('system:monitor_2'), jack.MidiPort('mscore:mscore-midi-1'), jack.MidiPort('mscore:mscore-midiin-1'), jack.OwnMidiPort('music_client:midi_in'), jack.OwnMidiPort('music_client:midi_out')]\n",
      "[]\n",
      "[]\n",
      "[jack.OwnMidiPort('music_client:midi_in')]\n",
      "[jack.OwnMidiPort('music_client:midi_out')]\n",
      "[jack.Port('system:capture_1'), jack.Port('system:capture_2')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# create two audio in and audio out ports\n",
    "#for number in 1, 2: # Jack Audio recognizes input and output placing them in server as appropriate\n",
    "#    client.inports.register(\"input_{0}\".format(number))\n",
    "#    client.outports.register(\"output_{0}\".format(number))\n",
    "\n",
    "# create two midi ports\n",
    "midiInPort = client.midi_inports.register(\"midi_in\")\n",
    "midiOutPort = client.midi_outports.register(\"midi_out\")\n",
    "\n",
    "# listing ports by kind\n",
    "#print(client.inports); print(client.outports)\n",
    "#print(client.midi_inports); print(client.midi_outports)\n",
    "# list ports by characteristic\n",
    "#print(client.get_port_by_name('music_client:input_1')) # port is referred to as client:port_name\n",
    "#print(client.get_ports(is_audio=True, is_output=True, is_physical=True))\n",
    "#print(client.get_ports(\"mus.*1$\")) # You can even use regular expressions to search for ports:\n",
    "# list connections for a port\n",
    "#print(client.get_all_connections('music_client:input_1')) # connections not for a client, but port on client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Connect Ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack.STOPPED\n",
      "0\n",
      "(jack.STOPPED, {'frame': 0, 'frame_rate': 44100, 'usecs': 447067900807L})\n",
      "(0, <cdata 'jack_position_t *' owning 136 bytes>)\n"
     ]
    }
   ],
   "source": [
    "# client management\n",
    "\n",
    "# Tell the JACK server that the program is ready to start processing audio.\n",
    "client.activate() \n",
    "\n",
    "# Establish a connection between two ports. When connection exists, data written to source port is available\n",
    "# to destination port. Audio ports can not be connected with MIDI ports.\n",
    "#client.connect(\"system:capture_1\", \"music_client:input_1\")\n",
    "#client.connect(\"music_client:output_1\", \"system:playback_1\")\n",
    "#client.connect(\"system:capture_2\", \"music_client:input_2\")\n",
    "#client.connect(\"music_client:output_2\", \"system:playback_2\") \n",
    "\n",
    "client.connect(midiOutPort, 'mscore:mscore-midiin-1')\n",
    "client.transport_start()# Start JACK transport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Client Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "('name:', u'music_client')\n",
      "port list:\n",
      "[jack.Port('system:capture_1'), jack.Port('system:capture_2'), jack.Port('system:playback_1'), jack.Port('system:monitor_1'), jack.Port('system:playback_2'), jack.Port('system:monitor_2'), jack.MidiPort('mscore:mscore-midi-1'), jack.MidiPort('mscore:mscore-midiin-1'), jack.OwnMidiPort('music_client:midi_in'), jack.OwnMidiPort('music_client:midi_out')]\n",
      "('sample rate:', 44100)\n",
      "('block size:', 512)\n",
      "\n",
      "Client Status:\n",
      "('status:', <jack.Status 0x0: no flags set>)\n",
      "('realtime:', True)\n",
      "('frames since:', 192)\n",
      "('frame time:', 118512309)\n",
      "('last frame time:', 118512128)\n",
      "('cup load:', 0.3934246301651001)\n",
      "\n",
      "Transport Status:\n",
      "('trans state:', jack.ROLLING)\n",
      "('trans frame:', 94644932)\n",
      "('trans query:', (jack.ROLLING, {'frame': 94644736, 'frame_rate': 44100, 'usecs': 449224047608L}))\n",
      "('trans struct:', (1, <cdata 'jack_position_t *' owning 136 bytes>))\n"
     ]
    }
   ],
   "source": [
    "def get_client_status(inClient):\n",
    "    print(\"Settings:\")\n",
    "    # client settings\n",
    "    print(\"name:\", inClient.name)\n",
    "    print(\"port list:\") # gets all ports for client (instance of class)\n",
    "    print(inClient.get_ports())\n",
    "    print(\"sample rate:\", inClient.samplerate) # sample rate of the JACK system (read-only).\n",
    "    print(\"block size:\", inClient.blocksize) \n",
    "    ''' Must be a power of 2. Max size ever passed. Query before activate(). Size may change. Clients that depend\n",
    "    on it must register a callback with set_blocksize_callback() so they will be notified if it does. Changing \n",
    "    blocksize stops JACK engine process, then calls all registered callback functions before restarting.'''\n",
    "    print\n",
    "    print(\"Client Status:\")\n",
    "    print(\"status:\", inClient.status)\n",
    "    print(\"realtime:\", inClient.realtime)\n",
    "    print(\"frames since:\", inClient.frames_since_cycle_start)\n",
    "    print(\"frame time:\", inClient.frame_time) \n",
    "    '''Estimated current time in frames. Used in other threads (not the process callback). Return value can be\n",
    "    compared with the value of last_frame_time to relate time in other threads to JACK time.'''\n",
    "    print(\"last frame time:\", inClient.last_frame_time) \n",
    "    print(\"cup load:\", inClient.cpu_load()) \n",
    "    print\n",
    "    print(\"Transport Status:\")\n",
    "    print(\"trans state:\", inClient.transport_state)\n",
    "    print(\"trans frame:\", inClient.transport_frame) # Returns estimate of the current transport frame. Assign frame number repositions the JACK transport.\n",
    "    print(\"trans query:\", inClient.transport_query()) # Query the current transport state and position. \n",
    "    print(\"trans struct:\", inClient.transport_query_struct()) # Query the current transport state and position. Realtime-safe.\n",
    "    #client.transport_reposition_struct(position)\n",
    "    \n",
    "get_client_status(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_callback',\n",
       " '_get_port_ptr',\n",
       " '_inports',\n",
       " '_keepalive',\n",
       " '_midi_inports',\n",
       " '_midi_outports',\n",
       " '_outports',\n",
       " '_port_list_from_pointers',\n",
       " '_position',\n",
       " '_ptr',\n",
       " '_register_port',\n",
       " '_status',\n",
       " '_wrap_port_ptr',\n",
       " 'activate',\n",
       " 'blocksize',\n",
       " 'close',\n",
       " 'connect',\n",
       " 'cpu_load',\n",
       " 'deactivate',\n",
       " 'disconnect',\n",
       " 'frame_time',\n",
       " 'frames_since_cycle_start',\n",
       " 'get_all_connections',\n",
       " 'get_client_name_by_uuid',\n",
       " 'get_port_by_name',\n",
       " 'get_ports',\n",
       " 'get_uuid_for_client_name',\n",
       " 'inports',\n",
       " 'last_frame_time',\n",
       " 'midi_inports',\n",
       " 'midi_outports',\n",
       " 'name',\n",
       " 'outports',\n",
       " 'owns',\n",
       " 'realtime',\n",
       " 'samplerate',\n",
       " 'set_blocksize_callback',\n",
       " 'set_client_registration_callback',\n",
       " 'set_freewheel',\n",
       " 'set_freewheel_callback',\n",
       " 'set_graph_order_callback',\n",
       " 'set_port_connect_callback',\n",
       " 'set_port_registration_callback',\n",
       " 'set_port_rename_callback',\n",
       " 'set_process_callback',\n",
       " 'set_samplerate_callback',\n",
       " 'set_shutdown_callback',\n",
       " 'set_timebase_callback',\n",
       " 'set_xrun_callback',\n",
       " 'status',\n",
       " 'transport_frame',\n",
       " 'transport_locate',\n",
       " 'transport_query',\n",
       " 'transport_query_struct',\n",
       " 'transport_reposition_struct',\n",
       " 'transport_start',\n",
       " 'transport_state',\n",
       " 'transport_stop']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'music21.midi.MidiFile'>\n",
      "1\n",
      "<class 'music21.stream.Score'>\n",
      "216\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'MidiFile' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-3223e45ed4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#for offset, indata in inport.incoming_midi_events():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mmidiOutPort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_midi_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pass through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#if len(indata) == 3:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'MidiFile' object is not iterable"
     ]
    }
   ],
   "source": [
    "# to musescore\n",
    "# astounding. new m21 version (v3) as of Aug 2016: http://web.mit.edu/music21/. \n",
    "\n",
    "fname = 'data/maps/AkPnBcht/ISOL/CH/MAPS_ISOL_CH0.1_M_AkPnBcht.mid'\n",
    "mf = m21.midi.MidiFile()\n",
    "mf.open(fname)\n",
    "mf.read()\n",
    "mf.close()\n",
    "print(type(mf))\n",
    "print(len(mf.tracks))\n",
    "# m21.convert\n",
    "\n",
    "s = m21.midi.translate.midiFileToStream(mf)\n",
    "s\n",
    "print(type(s))\n",
    "print(len(s.flat.notesAndRests))\n",
    "\n",
    "#notes = np.array([[0.1, 50, 0.3, 60], [0.2, 62, 0.4, 90]])\n",
    "#t = mad.utils.midi.MIDITrack.from_notes(notes)\n",
    "# First 4 bits of status byte:\n",
    "#NOTEON = 0x9\n",
    "#NOTEOFF = 0x8\n",
    "#INTERVALS = 3, 7  # minor triad\n",
    "\n",
    "#@client.set_process_callback\n",
    "#def process(frames):\n",
    "midiOutPort.clear_buffer()\n",
    "\n",
    "#for offset, indata in inport.incoming_midi_events():\n",
    "for event in mf:\n",
    "    midiOutPort.write_midi_event(event)  # pass through\n",
    "    #if len(indata) == 3:\n",
    "    #    status, pitch, vel = struct.unpack('3B', indata)\n",
    "    #    if status >> 4 in (NOTEON, NOTEOFF):\n",
    "    #        for i in INTERVALS:\n",
    "    #            # Note: This may raise an exception:\n",
    "    #            outport.write_midi_event(offset, (status, pitch + i, vel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'close',\n",
       " 'open',\n",
       " 'openFileLike',\n",
       " 'read',\n",
       " 'readstr',\n",
       " 'write',\n",
       " 'writeMThdStr',\n",
       " 'writestr']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(m21.midi.MidiFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify client details\n",
    "client.set_port_registration_callback(),\n",
    "client.set_port_connect_callback()\n",
    "client.set_port_rename_callback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Deactivate Ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.transport_stop() # Stop JACK transport.\n",
    "\n",
    "client.outports.clear()  # unregister all audio output ports\n",
    "# or client.disconnect(source, destination) to remove individual connections between two ports.\n",
    "\n",
    "client.deactivate(ignore_errors=True) # Tell the JACK server to remove self from the process graph. \n",
    "# Also, disconnect all ports belonging to it, since inactive clients have no port connections.\n",
    "\n",
    "client.close(ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Process Sound Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Listen / Record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Recognize Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Recognizer(AudioSource):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Recognizer`` instance, which represents a collection of speech recognition functionality.\n",
    "        \"\"\"\n",
    "        self.energy_threshold = 300 # minimum audio energy to consider for recording\n",
    "        self.dynamic_energy_threshold = False\n",
    "        self.dynamic_energy_adjustment_damping = 0.15\n",
    "        self.dynamic_energy_ratio = 1.5\n",
    "        self.pause_threshold = 0.8 # seconds of non-speaking audio before a phrase is considered complete\n",
    "        # PROBABLY NEEDS TO COME DOWN TO 0.6-ISH\n",
    "        self.phrase_threshold = 0.3 \n",
    "        # minimum seconds of speaking audio before we consider the speaking audio a phrase - values below this are ignored (for filtering out clicks and pops)\n",
    "        self.non_speaking_duration = 0.5 # seconds of non-speaking audio to keep on both sides of the recording\n",
    "\n",
    "    def record(self, source, duration = None, offset = None):\n",
    "        \"\"\"\n",
    "        Records up to ``duration`` seconds of audio from ``source`` (an ``AudioSource`` instance) starting at \n",
    "        ``offset`` (or at the beginning if not specified) into an ``AudioData`` instance, which it returns.\n",
    "        If ``duration`` is not specified, then it will record until there is no more audio input.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n",
    "        assert source.stream is not None, \"Audio source must be entered before recording, see documentation for `AudioSource`; are you using `source` outside of a `with` statement?\"\n",
    "\n",
    "        frames = io.BytesIO()\n",
    "        seconds_per_buffer = (source.CHUNK + 0.0) / source.SAMPLE_RATE\n",
    "        elapsed_time = 0\n",
    "        offset_time = 0\n",
    "        offset_reached = False\n",
    "        while True: # loop for the total number of chunks needed\n",
    "            if offset and not offset_reached:\n",
    "                offset_time += seconds_per_buffer\n",
    "                if offset_time > offset:\n",
    "                    offset_reached = True\n",
    "\n",
    "            buffer = source.stream.read(source.CHUNK)\n",
    "            if len(buffer) == 0: break\n",
    "\n",
    "            if offset_reached or not offset:\n",
    "                elapsed_time += seconds_per_buffer\n",
    "                if duration and elapsed_time > duration: break\n",
    "\n",
    "                frames.write(buffer)\n",
    "\n",
    "        frame_data = frames.getvalue()\n",
    "        frames.close()\n",
    "        return AudioData(frame_data, source.SAMPLE_RATE, source.SAMPLE_WIDTH)\n",
    "\n",
    "    def adjust_for_ambient_noise(self, source, duration = 1):\n",
    "        \"\"\"\n",
    "        Adjusts the energy threshold dynamically using audio from ``source`` (an ``AudioSource`` instance) to \n",
    "        account for ambient noise. Intended to calibrate the energy threshold with the ambient energy level. \n",
    "        Should be used on periods of audio without speech - will stop early if any speech is detected. The \n",
    "        ``duration`` parameter is the maximum number of seconds that it will dynamically adjust the threshold\n",
    "        for before returning. This value should be at least 0.5 in order to get a representative sample of the \n",
    "        ambient noise.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n",
    "        assert source.stream is not None, \"Audio source must be entered before adjusting, see documentation for `AudioSource`; are you using `source` outside of a `with` statement?\"\n",
    "        assert self.pause_threshold >= self.non_speaking_duration >= 0\n",
    "\n",
    "        seconds_per_buffer = (source.CHUNK + 0.0) / source.SAMPLE_RATE\n",
    "        elapsed_time = 0\n",
    "\n",
    "        # adjust energy threshold until a phrase starts\n",
    "        while True:\n",
    "            elapsed_time += seconds_per_buffer\n",
    "            if elapsed_time > duration: break\n",
    "            buffer = source.stream.read(source.CHUNK)\n",
    "            energy = audioop.rms(buffer, source.SAMPLE_WIDTH) # energy of the audio signal\n",
    "\n",
    "            # dynamically adjust the energy threshold using assymmetric weighted average\n",
    "            damping = self.dynamic_energy_adjustment_damping ** seconds_per_buffer # account for different chunk sizes and rates\n",
    "            target_energy = energy * self.dynamic_energy_ratio\n",
    "            self.energy_threshold = self.energy_threshold * damping + target_energy * (1 - damping)\n",
    "\n",
    "    def listen(self, source, timeout = None):       \n",
    "        \"\"\"\n",
    "        Records a single phrase from ``source`` (an ``AudioSource`` instance) into an ``AudioData`` instance, which\n",
    "        it returns. This is done by waiting until the audio has an energy above ``recognizer_instance.energy_threshold``\n",
    "        (the user has started speaking), and then recording until it encounters ``recognizer_instance.pause_threshold``\n",
    "        seconds of non-speaking or there is no more audio input. The ending silence is not included.\n",
    "        \n",
    "        The ``timeout`` parameter is the maximum number of seconds that it will wait for a phrase to start before \n",
    "        giving up and throwing an ``speech_recognition.WaitTimeoutError`` exception. If ``timeout`` is ``None``, \n",
    "        it will wait indefinitely.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n",
    "        assert source.stream is not None, \"Audio source must be entered before listening, see documentation for `AudioSource`; are you using `source` outside of a `with` statement?\"\n",
    "        assert self.pause_threshold >= self.non_speaking_duration >= 0\n",
    "\n",
    "        seconds_per_buffer = (source.CHUNK + 0.0) / source.SAMPLE_RATE\n",
    "        pause_buffer_count = int(math.ceil(self.pause_threshold / seconds_per_buffer)) \n",
    "        # number of buffers of non-speaking audio before the phrase is complete\n",
    "        phrase_buffer_count = int(math.ceil(self.phrase_threshold / seconds_per_buffer)) \n",
    "        # minimum number of buffers of speaking audio before we consider the speaking audio a phrase\n",
    "        non_speaking_buffer_count = int(math.ceil(self.non_speaking_duration / seconds_per_buffer)) \n",
    "        # maximum number of buffers of non-speaking audio to retain before and after\n",
    "\n",
    "        # read audio input for phrases until there is a phrase that is long enough\n",
    "        elapsed_time = 0 # number of seconds of audio read\n",
    "        buffer = b\"\" # an empty buffer means that the stream has ended and there is no data left to read\n",
    "        while True:\n",
    "            frames = collections.deque()\n",
    "\n",
    "            # store audio input until the phrase starts\n",
    "\n",
    "            while True:\n",
    "                \"\"\"JUST LISTENING.\"\"\"\n",
    "                elapsed_time += seconds_per_buffer\n",
    "                if timeout and elapsed_time > timeout: # handle timeout if specified\n",
    "                    raise WaitTimeoutError(\"listening timed out\")\n",
    "\n",
    "                buffer = source.stream.read(source.CHUNK)\n",
    "                if len(buffer) == 0: break # reached end of the stream\n",
    "                frames.append(buffer)\n",
    "                if len(frames) > non_speaking_buffer_count: # ensure we only keep the needed amount of non-speaking buffers\n",
    "                    frames.popleft()\n",
    "\n",
    "                # detect whether speaking has started on audio input\n",
    "                energy = audioop.rms(buffer, source.SAMPLE_WIDTH) # energy of the audio signal\n",
    "                \"\"\"ENSURE THAT MUSIC THRESHOLD > VOICE THRESHOLD. OTHERWISE ADJUST\"\"\"\n",
    "                if energy > self.energy_threshold: break\n",
    "\n",
    "                # dynamically adjust the energy threshold using assymmetric weighted average\n",
    "                if self.dynamic_energy_threshold:\n",
    "                    damping = self.dynamic_energy_adjustment_damping ** seconds_per_buffer # account for different chunk sizes and rates\n",
    "                    target_energy = energy * self.dynamic_energy_ratio\n",
    "                    self.energy_threshold = self.energy_threshold * damping + target_energy * (1 - damping)\n",
    "\n",
    "            # read audio input until the phrase ends\n",
    "            pause_count, phrase_count = 0, 0\n",
    "            while True:\n",
    "                \"\"\"RECORDING WHAT IT HEARS\"\"\"\n",
    "                elapsed_time += seconds_per_buffer\n",
    "\n",
    "                buffer = source.stream.read(source.CHUNK)\n",
    "                if len(buffer) == 0: break # reached end of the stream\n",
    "                frames.append(buffer)\n",
    "                phrase_count += 1\n",
    "\n",
    "                # check if speaking has stopped for longer than the pause threshold on the audio input\n",
    "                energy = audioop.rms(buffer, source.SAMPLE_WIDTH) # energy of the audio signal\n",
    "                \n",
    "                if energy > self.energy_threshold:\n",
    "                    pause_count = 0\n",
    "                else:\n",
    "                    pause_count += 1\n",
    "                if pause_count > pause_buffer_count: # end of the phrase\n",
    "                    break\n",
    "\n",
    "            # check how long the detected phrase is, and retry listening if the phrase is too short\n",
    "            phrase_count -= pause_count # exclude the buffers for the pause before the phrase\n",
    "            if phrase_count >= phrase_buffer_count or len(buffer) == 0: break # phrase is long enough or we've reached the end of the stream, so stop listening\n",
    "\n",
    "        # obtain frame data\n",
    "        for i in range(pause_count - non_speaking_buffer_count): frames.pop() # remove extra non-speaking frames at the end\n",
    "        frame_data = b\"\".join(list(frames))\n",
    "\n",
    "        return AudioData(frame_data, source.SAMPLE_RATE, source.SAMPLE_WIDTH)\n",
    "    \n",
    "    \n",
    "    def listen_in_background(self, source, callback):\n",
    "        \"\"\"\n",
    "        Spawns a thread to repeatedly record phrases from ``source`` (an ``AudioSource`` instance) into an ``AudioData``\n",
    "        instance and call ``callback`` with that ``AudioData`` instance as soon as each phrase are detected.\n",
    "        \n",
    "        Returns a function object that, when called, requests that the background listener thread stop, and waits until\n",
    "        it does before returning. The background thread is a daemon and will not stop the program from exiting if there\n",
    "        are no other non-daemon threads.\n",
    "        \n",
    "        Phrase recognition uses the exact same mechanism as ``recognizer_instance.listen(source)``.\n",
    "        The ``callback`` parameter is a function that should accept two parameters - the ``recognizer_instance``, \n",
    "        and an ``AudioData`` instance representing the captured audio. Note that ``callback`` function will be called\n",
    "        from a non-main thread.\n",
    "        \"\"\"\n",
    "        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n",
    "        running = [True]\n",
    "        def threaded_listen():\n",
    "            with source as s:\n",
    "                while running[0]:\n",
    "                    try: # listen for 1 second, then check again if the stop function has been called\n",
    "                        audio = self.listen(s, 1)\n",
    "                    except WaitTimeoutError: # listening timed out, just try again\n",
    "                        pass\n",
    "                    else:\n",
    "                        if running[0]: callback(self, audio)\n",
    "        def stopper():\n",
    "            running[0] = False\n",
    "            listener_thread.join() # block until the background thread is done, which can be up to 1 second\n",
    "        listener_thread = threading.Thread(target=threaded_listen)\n",
    "        listener_thread.daemon = True\n",
    "        listener_thread.start()\n",
    "        return stopper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_flac_converter():\n",
    "    # determine which converter executable to use\n",
    "    system = platform.system()\n",
    "    \n",
    "    path = os.path.dirname(os.path.abspath('music_study_aid')) # __file__\n",
    "    # directory of the current module file, where all the FLAC bundled binaries are stored\n",
    "    \n",
    "    flac_converter = shutil_which(\"flac\") # check for installed version first\n",
    "    if flac_converter is None: # flac utility is not installed\n",
    "        compatible_machine_types = [\"i686\", \"i786\", \"x86\", \"x86_64\", \"AMD64\"] \n",
    "        # whitelist of machine types our bundled binaries are compatible with\n",
    "        if system == \"Windows\" and platform.machine() in compatible_machine_types:\n",
    "            flac_converter = os.path.join(path, \"flac-win32.exe\")\n",
    "        elif system == \"Linux\" and platform.machine() in compatible_machine_types:\n",
    "            flac_converter = os.path.join(path, \"flac-linux-x86\")\n",
    "        elif system == \"Darwin\" and platform.machine() in compatible_machine_types:\n",
    "            flac_converter = os.path.join(path, \"flac-mac\")\n",
    "        else:\n",
    "            raise OSError(\"FLAC conversion utility not available - consider installing the FLAC command line application using `brew install flac` or your operating system's equivalent\")\n",
    "\n",
    "    # mark FLAC converter as executable if possible\n",
    "    try:\n",
    "        stat_info = os.stat(flac_converter)\n",
    "        os.chmod(flac_converter, stat_info.st_mode | stat.S_IEXEC)\n",
    "    except OSError: pass\n",
    "\n",
    "    return flac_converter\n",
    "\n",
    "def shutil_which(pgm):\n",
    "    \"\"\"Python 2 compatibility: backport of ``shutil.which()`` from Python 3\"\"\"\n",
    "    path = os.getenv('PATH')\n",
    "    for p in path.split(os.path.pathsep):\n",
    "        p = os.path.join(p, pgm)\n",
    "        if os.path.exists(p) and os.access(p, os.X_OK):\n",
    "            return p\n",
    "\n",
    "class tempfile_TemporaryDirectory(object):\n",
    "    \"\"\"Python 2 compatibility: backport of ``tempfile.TemporaryDirectory`` from Python 3\"\"\"\n",
    "    def __enter__(self):\n",
    "        self.name = tempfile.mkdtemp()\n",
    "        return self.name\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        shutil.rmtree(self.name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classify Sound Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Continuous Frequency Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from essentia.standard import *\n",
    "from numpy import inf\n",
    "from types import *\n",
    "\n",
    "# to do:\n",
    "#1. figure out how to pass a stream rather than a file address\n",
    "#2. figure out how to make this persist as a stream process i.e., not instantiating loader each time.\n",
    "#3. code below is a 1st draft. too many nested \"for\" loops. needs vector optimization.\n",
    "\n",
    "def cfa_classifier(inAudio, sourceType=\"file\", numPeaks=5, showGraphics=False):\n",
    "    \n",
    "    '''The following two approaches for separating voice commands from music are based on the observation that\n",
    "    speech signals usually display patterns of harmonics influenced by the shape of the vocal tract. Within a \n",
    "    time frame, they manifest themselves as peaks within the spectrum. Further, the partials can be found at the\n",
    "    fundamental frequency of a tone and also near its integer multiples. Finally, the harmonics are sustained over\n",
    "    a certain span of time in which they are likely to vary in frequency. This last characteristic is highly \n",
    "    discriminative vis a vis both noise and music. Stated simply, voice spectrograms exhibit characteristic curved\n",
    "    trajectories. Music spectrograms exhibit strictly horizontal and minor vertical structures and noise looks\n",
    "    like... noise. Noticed as early as 1993 by M. Hawley @ MIT.     \n",
    "        \n",
    "    Both approaches have three basic steps.First, they slice the audio stream into small frames and extract\n",
    "    features for each frame. Second, they train a classifier on a distinct training set. The classifier learns\n",
    "    to distinguish two classes (i.e., music/no-music and voice/no-voice). Third, the classifier is used to predict\n",
    "    class labels for all the frames in the test set. Finally, classification results are smoothed in a post-\n",
    "    processing step to obtain a label sequence for continuous audio segments. \n",
    "        \n",
    "    In their 2007 work, Seyerlehner et al recognized that music can be differentiated by structural properties \n",
    "    like harmony and rhythm. If clarity and consistency of paritial frequency emissions are evidence of music,\n",
    "    then a feature could be developed to reliably detect continuous frequency activations... even in the presence\n",
    "    of other audio signals. That feature was Continuous Frequency Activation (CFA). The computation of CFA can be\n",
    "    subdivided into:\n",
    "    1. Conversion of the input audio stream into 11 kHz mono.'''\n",
    "    \n",
    "    #if inType == 'fileAddr':\n",
    "    loader = essentia.standard.MonoLoader(downmix='mix', filename=inAudio, sampleRate=44100)\n",
    "    inAudio = loader()\n",
    "    #inAudio = inAudio[1*44100:180*44100]\n",
    "        \n",
    "    '''\n",
    "    2. Computation of the power spectrum using a Hanning window function and a window size of 1024 samples\n",
    "    (roughly) 100ms of audio. A hop-size of 256 samples is used, resulting in an overlap of 75% percent. \n",
    "    After the conversion to decibel, we obtain a standard spectrogram representation.'''\n",
    "    \n",
    "    w = essentia.standard.Windowing(type = 'hann') #'hamming'?\n",
    "    power = essentia.standard.PowerSpectrum()\n",
    "    spectrum = essentia.standard.Spectrum()\n",
    "    \n",
    "    pwrSpec=[]\n",
    "    for frame in FrameGenerator(inAudio, frameSize = 1024, hopSize = 256):\n",
    "        pwrSpec.append(spectrum(w(frame))) # NOTE: USING SPECTRUM. power returns: power spectrum of input         \n",
    "    \n",
    "    pwrSpec = essentia.array(pwrSpec).T #transpose, then convert list to an essentia.array first (== numpy.array of floats)\n",
    "    \n",
    "    if showGraphics == True: \n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.subplot(2,3,1); plt.plot(inAudio)\n",
    "        plt.subplot(2,3,2); plt.imshow(pwrSpec[:100,:], aspect = 'auto')   \n",
    "    \n",
    "    pwrSpec = np.square(pwrSpec)\n",
    "    pwrSpec = 10*np.log10(pwrSpec)\n",
    "    pwrSpec[pwrSpec == -inf] = 0\n",
    "    \n",
    "    # NOTE: CBA DOES A NORMALIZATION STEP AFTER HANNING. GIVEN LOCAL NORMALIZATION BELOW, IT SEEMS REDUNDANT\n",
    "    '''\n",
    "    3. Emphasize local peaks within each frame of the STFT by subtracting from the power spectrum of each frame\n",
    "    the running average using a window size of N = 21 frequency bins:\n",
    "        x_emph = x_i - 1/N * Sigma_k=-N/2... Xmin(max(k,1),N\n",
    "    Were Xi denotes the energy of the i-th frequency component of the current frame. This step is useful to \n",
    "    emphasize very soft tones, belonging to background music. The perceivable horizontal bars in the spectogram\n",
    "    are compositions of consecutive local maxima. Thus, we try to emphasize these soft bars by emphasizing \n",
    "    all local maxima in the spectrum of a frame.'''\n",
    "    \n",
    "    wndw = 21    \n",
    "    for i in range(pwrSpec.shape[1]):\n",
    "        if i < int(wndw/2): pwrSpec[:,i] = pwrSpec[:,0] # set left side to initial value\n",
    "        elif i > (pwrSpec.shape[1] - int(wndw/2)): pwrSpec[:,i] = pwrSpec[:,-1] # right side to initial value\n",
    "        else: pwrSpec[:,i] = pwrSpec[:,i] - np.mean(pwrSpec[:,(i-int(wndw/2)):(i+int(wndw/2))]) # de-mean center\n",
    "\n",
    "    '''\n",
    "    4. Binarize the frequency component to eliminate strength of activation (energy) in a given frame j,\n",
    "    X_emph_ij by comparing to a fixed binarization threshold of 0.1 keeps even soft activations in the spectogram.\n",
    "    But, inactive frequency bins are set to 0 using this low threshold.'''\n",
    "    \n",
    "    binThresh = 0.1\n",
    "    \n",
    "    pwrSpec[pwrSpec >= binThresh] = 1\n",
    "    pwrSpec[pwrSpec < binThresh] = 0\n",
    "\n",
    "    if showGraphics == True: plt.subplot(2,3,4); plt.imshow(pwrSpec[:100,:], aspect = 'auto')\n",
    "    \n",
    "    '''\n",
    "    5. Compute frequency activtaion. Process the binarized power spectrum in terms of blocks. Each block consists\n",
    "    of F = 100 frames and blocks overlap by 50%, which means that a block is an excerpt of the binarized \n",
    "    spectrogram corresponding to 2.6 seconds of audio. For each block we compute the frequency activation \n",
    "    function Activation(i). For each frequency bin i, the frequency activation function measures how often a \n",
    "    frequency component is active in a block. We obtain the frequency activation function for a block by simply\n",
    "    summing up the binarized values for each frequency bin i:\n",
    "        Activation(i) = 1/F * Sigma_j=1^F Bij'''\n",
    "    \n",
    "    timeBlockSize = 30\n",
    "    timeIncr = timeBlockSize #/2\n",
    "    numTimeBlocks = (int(pwrSpec.shape[1] / timeIncr)) #-1\n",
    "    pwrAct = np.zeros((pwrSpec.shape[0],numTimeBlocks))\n",
    "    \n",
    "    for freqRow in range(pwrSpec.shape[0]):\n",
    "        timePosCtr = 0\n",
    "        for timeCol in range(numTimeBlocks):\n",
    "            pwrAct[freqRow,timeCol] = np.sum(pwrSpec[freqRow,timePosCtr:(timePosCtr+timeBlockSize)])\n",
    "            timePosCtr = timePosCtr + timeIncr\n",
    "        \n",
    "    if showGraphics == True:\n",
    "        plt.subplot(2,3,5); plt.imshow(pwrAct[0:100,:], aspect = 'auto')\n",
    "        freqSum = np.sum(pwrAct,1)\n",
    "        plt.subplot(2,3,6); plt.plot(freqSum[0:100])\n",
    "    \n",
    "    '''  \n",
    "    6. Detect strong peaks. Peaks in the frequency activation function of a given block indicate steady activations\n",
    "    of narrow frequency bands. The “spikier” the activation function, the more likely horizontal bars, which are\n",
    "    characteristic of sustained musical tones, are present. Even one large peak is quite a good indicator for the\n",
    "    presence of a tone. The peakiness of the frequency activation function is consequently a good indicator for \n",
    "    the presence of music. To extract the peaks we use the following simple peak picking algorithm.\n",
    "        a. Collect all local peaks, starting from the lowest frequency. Each local maximum of the activation \n",
    "        function is a potential peak.\n",
    "        b. For each peak x_p, compute its height-to-width index or peak value pv(xp) = h(xp)/w(xp), where the\n",
    "        height h(xp) is defined as min[f(p)−f(xl),f(p)− f (xr )], with f (x) the value of the activation \n",
    "        function at point (frequency bin) x and xl and xr are closest local minima of f to the left and right \n",
    "        of xp, respec- tively. The width w(xp) of the peak is given by:\n",
    "            w(x_p) = p - x_l, f(p)−f(xl) < f(p)−f(xr) ELSE x_r - p otherwise'''\n",
    "    \n",
    "    outPeaks = []\n",
    "    \n",
    "    for freqRow in range(pwrAct.shape[0]):\n",
    "        \n",
    "        tmpRow = pwrAct[freqRow,:]\n",
    "        N = len(tmpRow)\n",
    "        peaks = np.zeros((N))\n",
    "        maxList = np.zeros((N))\n",
    "        minList = np.zeros((N))\n",
    "        \n",
    "        for i in range(N):\n",
    "            maxList[i] = -1\n",
    "            minList[i] = -1\n",
    "        lastMaxIndex = 0; lastMinIndex = 0\n",
    "        direction = 0\n",
    "        cf = 0\n",
    "        \n",
    "        # advance if steady initial value\n",
    "        while ((cf < (N-1)) and (tmpRow[cf] == tmpRow[cf + 1])):\n",
    "            cf = cf + 1\n",
    "        \n",
    "        # in some cases that takes you to end of input row. So, check.\n",
    "        if(cf < (N-1)):\n",
    "            # if start is a max or min, account for it\n",
    "            if tmpRow[cf] > tmpRow[cf+1]:\n",
    "                maxList[cf] = tmpRow[cf]\n",
    "                lastMaxIndex = cf\n",
    "                direction = 0\n",
    "            \n",
    "            elif tmpRow[cf] < tmpRow[cf+1]:\n",
    "                minList[cf] = tmpRow[cf]\n",
    "                lastMinIndex = cf\n",
    "                direction = 1\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        count = 1\n",
    "        \n",
    "        for i in range(1,N):\n",
    "            \n",
    "            if ((tmpRow[i] > tmpRow[i-1]) and (direction == 0)): # minimum detected\n",
    "                \n",
    "                # scan backards for earliest occurrence of that minimum\n",
    "                cb = i-1\n",
    "                while ((cb > 1) and (tmpRow[cb-1] == tmpRow[cb])):\n",
    "                    cb = cb - 1\n",
    "                    \n",
    "                # save minimum to list\n",
    "                minList[cb] = tmpRow[cb] # save first minimuim\n",
    "                minList[i-1] = tmpRow[i-1] # save second minimum\n",
    "                count = count + 1\n",
    "                direction = 1\n",
    "                \n",
    "                # calculate area of the peak\n",
    "                if count < 3: # first value was a max\n",
    "                    peaks[lastMaxIndex] = (maxList[lastMaxIndex] - minList[cb]) / (cb - lastMaxIndex)\n",
    "                    # (hmax - hmin) / w\n",
    "                \n",
    "                else:\n",
    "                    if minList[lastMinIndex] > minList[cb]: # hminL > hminR\n",
    "                        peaks[lastMaxIndex] = (maxList[lastMaxIndex] - minList[lastMinIndex])/(lastMaxIndex - lastMinIndex)\n",
    "                        # (hmax - hminL) / w\n",
    "                    else: \n",
    "                        peaks[lastMaxIndex] = (maxList[lastMaxIndex] - minList[cb])/(cb-lastMaxIndex)\n",
    "                        # (hmsx - hminR) / w\n",
    "                    \n",
    "                lastMinIndex = i - 1\n",
    "                \n",
    "            elif ((tmpRow[i] < tmpRow[i-1]) and (direction == 1)):\n",
    "                \n",
    "                # save maximum to list\n",
    "                maxList[i-1] = tmpRow[i-1]\n",
    "                count = count + 1\n",
    "                direction = 0\n",
    "                lastMaxIndex = i - 1\n",
    "    \n",
    "        '''        \n",
    "        7. Quantify the CFA of the activation function of a block, the pv values of all detected peaks are \n",
    "        sorted in descending order, and the sum of the five largest peak values is taken to characterize the\n",
    "        overall “peakiness” of the activation function.'''\n",
    "    \n",
    "        # sort peaks\n",
    "        for i in range(len(peaks)):\n",
    "            outPeaks.append(peaks[i])\n",
    "        \n",
    "    outPeaks = sorted(outPeaks,reverse=True)\n",
    "    outPeaks = outPeaks[0:numPeaks]\n",
    "    #print(outPeaks)\n",
    "    print(sum(outPeaks))\n",
    "    if sourceType == \"file\":\n",
    "        if sum(outPeaks) > 85:\n",
    "            return(\"music\")\n",
    "        else:\n",
    "            return(\"voice\")\n",
    "    elif sourceType == \"mic\":\n",
    "        if sum(outPeaks) > 130:\n",
    "            return(\"music\")\n",
    "        else:\n",
    "            return(\"voice\")\n",
    "    else: print(\"error: invalid source type\")\n",
    "            \n",
    "    '''Thus we obtain one numeric value for each block of frames, which quantifies the presence of steady frequency\n",
    "    components within the current audio segment.'''\n",
    "        \n",
    "        \n",
    "#sound_classifier(inAudio='data/wip/tmp/MAPS_MUS-bach_846_AkPnBcht.wav', sourceType = \"file\", \n",
    "#                 numPeaks = 5, showGraphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Curved Frequency Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''The second, Curved Frequency Trajectory (CFT) , was developed in 2012 to detect voices in xyz. It identifies \n",
    "human voices within mixed audio signals by detecting the curved frequency trajectory of the harmonics over a \n",
    "certain time periods. So, taking the opposite view of CFA, it uses change in observed frequencies as an indicator\n",
    "of human voice.'''\n",
    "\n",
    "def cft_classifier(inAudio, sourceType=\"file\", numPeaks=5, showGraphics=False):\n",
    "    '''\n",
    "    1. Compute cross-correlation between the two time frames Xt and Xt+offset. The cross-correlation can be used\n",
    "    to estimate the degree of correlation between shifted versions of these vectors, for a range of so-called lags l.\n",
    "    \n",
    "    2. CROSS_CORRELATION: Define rxcorr as the maximum cross-correlation over a range of lags: \n",
    "    rxcorr(X_t,X_t+offset) = maxRX_t,X_t+offset where l ∈ [−lmax,lmax] denotes the lag (frequency shift) in terms\n",
    "    of frequency bins.\n",
    "    \n",
    "    3. CORRELATION: r as a special case of the cross-correlation above, with lag l = 0 i.e., the instance of \n",
    "    cross-correlation that coresponds to simple correlation: r(X_t, X_t+offset ) = RX_t,X_t+offset (0)\n",
    "    \n",
    "    4. LOG TRANSFORM: Harmonic frequencies are multiples of the fundamental frequency. So, shifting spectral \n",
    "    patterns along the frequency axis cannot be done on a linear scale. However, they can be represented on a\n",
    "    logarithmic scale. On log scale harmonics are at constant offsets to their fundamental frequency and continuous\n",
    "    frequency changes can be captured by cross-correlation.\n",
    "    \n",
    "    5. Sample input to 22.05 kHz monaural audio. Then transform the audio input to the frequency domain by applying\n",
    "    a Short Time Fourier Transform with a Kaiser window of a size of 4096 samples.\n",
    "    \n",
    "    7. Follow the preprocessing steps as performed above, by:\n",
    "            a. compute the magnitude spectrum |X(f)|\n",
    "            b. map the STFT magnitude spectrum to a perceptual scale... the logarithmic cent scale representation \n",
    "            of STFT spectrograms. \n",
    "            c. consider the lower 150 cent-scaled frequency bins of the spectrum only (i.e., frequencies of up to 802 Hz)\n",
    "            discarding the upper bins.\n",
    "            \n",
    "    8. set The hop size chosen for the STFT corresponds to a time difference of approximately 23 ms, too small \n",
    "    for harmonics’ frequencies to vary significantly. Therefore, use offset of 3 in computation of cross-correlation.\n",
    "    \n",
    "    9. maximum lag \"l max\" used for computing the cross-correlation should not allow for shift of an entire semitone.\n",
    "    \"l max\" of 3 prevents the feature from reporting high values at times where a musical instrument plays along a\n",
    "    chromatic scale.\n",
    "    \n",
    "        10. calculate correlation gain: rxcorr − r to identify areas that exhibit strong curved patterns.\n",
    "        11. compute the feature from the audio signal according to a decision frequency of 5 Hz (i.e., one feature\n",
    "        value every 200 ms), where we center an observation window of width 50 STFT blocks (approximately 1.3s) \n",
    "        around each decision position, in order to also capture some context.\n",
    "        12. For each of the N −offset pairs of STFT blocks (Xt , Xt+offset ) in the observation window of length N,\n",
    "        two vectors xc, c of feature values, one for the cross-correlation and one for the correlation results, are\n",
    "        computed, both having a length of N − offset, where N is the number of STFT blocks of the observation window\n",
    "        (50). The element-wise difference of the vectors gives the feature vector r = xc − c. \n",
    "        13. smooth r using a rectangular window of width 5\n",
    "        14. the index of the dominant frequency bin within the observation window is appended to the feature vector\n",
    "        as an additional feature.\n",
    "        15. the final result is a vector of 48 feature values (50 − 3 smoothed correlation gain values, and 1 \n",
    "        frequency bin index) per decision point.\n",
    "        16. train a classifier on annotated ground truth (radio broadcast recordings with segment boundary indications)\n",
    "        for two-class, contains speech and does not contain speech. \n",
    "        17. random forest ensemble classifier of decision trees parameterized to use 200 decision trees, and 10 random\n",
    "        features per tree. The classifier outputs class probabilities, which are transformed to binary decisions using \n",
    "        simple thresholding.\n",
    "        18. three data sets: \n",
    "            a. a training set, to be used as training material for the classifier; \n",
    "            b. a validation set, which will be used to perform systematic parameter studies, and select the final\n",
    "            pa- rameter setting (in particular, the decision threshold)\n",
    "            c. and aninde- pendent test set, on which the final classifier will then be evaluated.\n",
    "        19. The classifier is trained to classify individual time points in the audio stream — each training example\n",
    "        is one point in the audio, represented in terms of a feature vector of 48 feature values where the feature \n",
    "        values characterize the signal at the current point, and its local context. Training, validation, and test\n",
    "        data are processed with a feature extraction frequency of 5 Hz, which means that the classifier produces \n",
    "        predictions at a rate of 5 labels per second of audio. \n",
    "        20. As a post-processing step, the sequence of predicted class labels is smoothed using a median filter\n",
    "        with a window size of 52 labels.\n",
    "        21. The classifier turns out to be extraordinarily stable for a wide range of decision thresholds between \n",
    "        0.2 and 0.8. As a consequence, we rather arbitrarily selected a threshold of 0.5 for our remaining evaluation\n",
    "        on independent test sets.\n",
    "        22. In practice, feature values tend to be rather small num- bers, often in the range of 0 to 10−3. The \n",
    "        feature values could be transformed into the interval [0.0, 1.0] by representing the result as: \n",
    "        1.0 − r/rxcorr for every result with correlation gain, and zero otherwise.\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Classification Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sound_classifier(inAudio='data/wip/tmp/dcp1.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/weird-al.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/dcp2.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/mudhole.wav', sourceType=\"file\",\n",
    "                 numPeaks = 5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/dcp3.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)\n",
    "\n",
    "sound_classifier(inAudio='data/wip/tmp/window.wav', sourceType=\"file\",\n",
    "                 numPeaks=5, showGraphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recognize_google(self, audio_data, key = None, language = \"en-US\", show_all = False):\n",
    "    \"\"\"\n",
    "    Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Google Speech Recognition\n",
    "    API. The Google Speech Recognition API key is specified by ``key``. If not specified, it uses a generic key \n",
    "    that works out of the box. This should generally be used for personal or testing purposes only, as it \n",
    "    **may be revoked by Google at any time**.\n",
    "    \n",
    "    To obtain your own API key, simply follow the steps on the `API Keys <http://www.chromium.org/developers/how-tos/api-keys>`__ page \n",
    "    at the Chromium Developers site. In the Google Developers Console, Google Speech Recognition is listed as \"Speech API\".\n",
    "        \n",
    "    The recognition language is determined by ``language``, an RFC5646 language tag like ``\"en-US\"`` (US English)\n",
    "    or ``\"fr-FR\"`` (International French), defaulting to US English. A list of supported language values can be \n",
    "    found in this `StackOverflow answer <http://stackoverflow.com/a/14302134>`__.\n",
    "        \n",
    "    Returns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the raw API \n",
    "    response as a JSON dictionary.\n",
    "        \n",
    "    Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a \n",
    "    ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't \n",
    "    valid, or if there is no internet connection.\n",
    "    \"\"\"\n",
    "    assert isinstance(audio_data, AudioData), \"`audio_data` must be audio data\"\n",
    "    assert key is None or isinstance(key, str), \"`key` must be `None` or a string\"\n",
    "    assert isinstance(language, str), \"`language` must be a string\"\n",
    "\n",
    "    flac_data = audio_data.get_flac_data(\n",
    "        convert_rate = None if audio_data.sample_rate >= 8000 else 8000, # audio samples must be at least 8 kHz\n",
    "        convert_width = 2 # audio samples must be 16-bit\n",
    "    )\n",
    "    if key is None: key = \"AIzaSyBOti4mM-6x9WDnZIjIeyEU21OpBXqWBgw\"\n",
    "    url = \"http://www.google.com/speech-api/v2/recognize?{0}\".format(urlencode({\n",
    "                \"client\": \"chromium\",\n",
    "                \"lang\": language,\n",
    "                \"key\": key,\n",
    "            }))\n",
    "    request = Request(url, data = flac_data, headers = {\"Content-Type\": \"audio/x-flac; rate={0}\".format(audio_data.sample_rate)})\n",
    "\n",
    "    # obtain audio transcription results\n",
    "    try:\n",
    "        response = urlopen(request)\n",
    "    except HTTPError as e:\n",
    "        raise RequestError(\"recognition request failed: {0}\".format(getattr(e, \"reason\", \"status {0}\".format(e.code)))) # use getattr to be compatible with Python 2.6\n",
    "    except URLError as e:\n",
    "        raise RequestError(\"recognition connection failed: {0}\".format(e.reason))\n",
    "    response_text = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # ignore any blank blocks\n",
    "    actual_result = []\n",
    "    for line in response_text.split(\"\\n\"):\n",
    "        if not line: continue\n",
    "        result = json.loads(line)[\"result\"]\n",
    "        if len(result) != 0:\n",
    "            actual_result = result[0]\n",
    "            break\n",
    "\n",
    "    # return results\n",
    "    if show_all: return actual_result\n",
    "    if \"alternative\" not in actual_result: raise UnknownValueError()\n",
    "    for entry in actual_result[\"alternative\"]:\n",
    "        if \"transcript\" in entry:\n",
    "            return entry[\"transcript\"]\n",
    "    raise UnknownValueError() # no transcriptions available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Process Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = range(10)\n",
    "Y = range(10)\n",
    "Z = np.zeros((len(X), len(Y)))\n",
    "from random import randint\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(Y)):\n",
    "        Z[i,j]=randint(0,9)\n",
    "    \n",
    "plt.contour(X,Y,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x. Join Wav Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print dir(essentia.standard)\n",
    "\n",
    "fnames = ['data/wip/tmp/window.wav'] + ['data/maps/AkPnBcht/ISOL/TR1/MAPS_ISOL_TR1_F_S0_M47_AkPnBcht.wav'] + ['data/wip/tmp/cops.wav']\n",
    "\n",
    "wavs = []\n",
    "outfile = \"data/wip/tmp/stringTest.wav\"\n",
    "blank = essentia.array(np.repeat(0.0, 1*44100, axis=0))\n",
    "\n",
    "for fname in fnames:\n",
    "    loader = essentia.standard.MonoLoader(downmix='mix', filename=fname, sampleRate=44100)\n",
    "    w = loader()\n",
    "    wavs.append(blank)\n",
    "    if(fname == 'data/maps/AkPnBcht/ISOL/TR1/MAPS_ISOL_TR1_F_S0_M47_AkPnBcht.wav'):\n",
    "        w = 10*w\n",
    "    wavs.append(w)\n",
    "    \n",
    "out = np.concatenate(wavs, axis=0)\n",
    "essentia.standard.MonoWriter(filename=outfile, format='wav', sampleRate=44100)(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Manage Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main\n",
    "#recognizer_instance.record(audiofile_instance, duration=10)\n",
    "import contextlib\n",
    "\n",
    "fname = 'data/maps/AkPnBcht/ISOL/CH/MAPS_ISOL_CH0.1_M_AkPnBcht.wav'\n",
    "#'data/wip/tmp/stringTest.wav'\n",
    "sourceType = \"file\"\n",
    "signOff = False\n",
    "clipDuration = None # None if no set duration i.e., listen until eof or pause\n",
    "pastDuration = 0\n",
    "\n",
    "if sourceType == \"mic\": \n",
    "    m = Microphone()\n",
    "else:\n",
    "    m = AudioFile(fname)\n",
    "    #print(m.__dict__)\n",
    "    with contextlib.closing(wave.open(fname,'r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        fileDuration = frames / float(rate)\n",
    "            \n",
    "r = Recognizer()\n",
    "\n",
    "try:\n",
    "    with m as source:\n",
    "        if sourceType == \"mic\":\n",
    "            print(\"Me: Calibrating background noise...\")\n",
    "            #with m as source: r.adjust_for_ambient_noise(source)\n",
    "            r.adjust_for_ambient_noise(source)\n",
    "            print\n",
    "            print(\"Me: Setting min energy to {}\".format(r.energy_threshold))\n",
    "    \n",
    "        while signOff == False:\n",
    "            print\n",
    "            print(\"Me: Listening...\")\n",
    "        \n",
    "            if sourceType == \"mic\":\n",
    "                #with m as source: audio = r.listen(source)\n",
    "                audio = r.listen(source)\n",
    "                \n",
    "            elif sourceType == \"file\":\n",
    "                if clipDuration == None: # you will listen to entire clip\n",
    "                    signOff = True\n",
    "                else:\n",
    "                    pastDuration = pastDuration + clipDuration\n",
    "                \n",
    "                if (fileDuration - pastDuration) <= clipDuration:\n",
    "                    clipDuration = (fileDuration - pastDuration)\n",
    "                    signOff = True\n",
    "                \n",
    "                audio = r.record(source, duration=clipDuration)\n",
    "        \n",
    "            audio.get_wav_data()\n",
    "            print\n",
    "            print(\"Me: Determining sound type...\")\n",
    "            print\n",
    "            if sound_classifier('data/wip/tmp/vrTmp.wav', sourceType, numPeaks=5, showGraphics=False) == \"voice\":\n",
    "                try:\n",
    "                    print(\"Me: Interpreting command...\")\n",
    "                    # recognize speech using Google Speech Recognition\n",
    "                    value = r.recognize_google(audio)\n",
    "                    print(u\"You: {}\".format(value).encode(\"utf-8\"))\n",
    "            \n",
    "                    nextAction, actionText = command_interpreter(value)\n",
    "                    if nextAction == \"end\":\n",
    "                        signOff = True\n",
    "                    else:\n",
    "                        print(u\"Me: {}\".format(actionText).encode(\"utf-8\"))\n",
    "            \n",
    "                except UnknownValueError:\n",
    "                    print\n",
    "                    print(\"Me: Say again?\")\n",
    "                except RequestError as e:\n",
    "                    print(\"Me: Uh oh! Couldn't request results from Google Speech Recognition service; {0}\".format(e))\n",
    "                \n",
    "            else:\n",
    "                print(\"Me: Calling music recognition...\")\n",
    "                print(\"Me: You played...\")\n",
    "                print(np.array(rnn_note_and_chord_rec('single', 'data/wip/tmp/vrTmp.wav', outFileBase=None)))\n",
    "        print\n",
    "        print(\"Me: Thanks for playing! Ending now.\")\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Command Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def command_interpreter(command):\n",
    "    \n",
    "    if \"time to stop\" in command:\n",
    "        nextAction = \"end\"\n",
    "        actionText = \"Thanks for playing! Ending now.\"\n",
    "    elif (\"back\" in command) or (\"repeat\" in command):\n",
    "        notes = [int(s) for s in command.split() if s.isdigit()]\n",
    "        nextAction = \"continue\"\n",
    "        actionText = \"Going back \" + str(notes[0]) + \" notes\"\n",
    "    else:\n",
    "        nextAction = \"continue\"\n",
    "        actionText = \"...\"\n",
    "        \n",
    "    return(nextAction, actionText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Process Study Aid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build Session Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store user, start time, stop time\n",
    "# identify song from input. assign it a name\n",
    "# store songs played, start / stop times, link to session\n",
    "# score session\n",
    "\n",
    "class Session():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Session`` instance, which represents a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq\n",
    "        \n",
    "    def xyz():\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lay Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analyze song midi indexing bars, lines\n",
    "# coordinate wav and midi (musescore)\n",
    "# coordinate master\n",
    "# maintain track stats e.g., time spent, errors,...\n",
    "\n",
    "class Track():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Track`` instance, which represents a song played in a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq\n",
    "        \n",
    "    def xyz():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Save Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save and index played bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Replace Fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### c. Assemble Track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Score Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Score():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Score`` instance, which represents a song played in a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq\n",
    "        \n",
    "    def xyz():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Configure MuseScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set defaults for ux including continuous view, mentronome, tempo\n",
    "# load score standard\n",
    "\n",
    "class Musescore_context():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Session`` instance, which represents a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Write Midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notes = np.array([[0.1, 50, 0.3, 60], [0.2, 62, 0.4, 90]])\n",
    "t = mad.utils.midi.MIDITrack.from_notes(notes)\n",
    "print(t.events)\n",
    "\n",
    "outport.clear_buffer()\n",
    "    for offset, indata in inport.incoming_midi_events():\n",
    "        # Note: This may raise an exception:\n",
    "        outport.write_midi_event(offset, indata)  # pass through\n",
    "        if len(indata) == 3:\n",
    "            status, pitch, vel = struct.unpack('3B', indata)\n",
    "            if status >> 4 in (NOTEON, NOTEOFF):\n",
    "                for i in INTERVALS:\n",
    "                    # Note: This may raise an exception:\n",
    "                    outport.write_midi_event(offset, (status, pitch + i, vel))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MIDI_writer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a new ``Session`` instance, which represents a music practice session.\n",
    "        \"\"\"\n",
    "        self.tbd = xyz # pdq\n",
    "        \n",
    "    def annotate_standard_midi():\n",
    "        '''Standard score mode is used to teach the basics of note reading and sequencing. It starts with an \n",
    "        existing midi score and writes notes, as played, over that baseline. Therefore, it is not concerned with\n",
    "        waits, tempo or other temporal characteristics.'''\n",
    "\n",
    "    def write_original_midi():\n",
    "        '''Original score mode layers waits and tempo on Standard mode.'''\n",
    "    \n",
    "# position on score\n",
    "# write note\n",
    "\n",
    "madmom.audio.filters.hz2midi(f, fref=440.0)\n",
    "Convert frequencies to the corresponding MIDI notes.\n",
    "Parameters f : numpy array\n",
    "Input frequencies [Hz].\n",
    "fref : float, optional\n",
    "Tuning frequency of A4 [Hz].\n",
    "Returns m : numpy array\n",
    "MIDI notes\n",
    "Notes\n",
    "For details see: at http://www.phys.unsw.edu.au/jw/notes.html This function does not necessarily return a valid\n",
    "MIDI Note, you may need to round it to the nearest integer\n",
    "\n",
    "\n",
    "madmom.features.notes.write_midi(notes, filename, duration=0.6, velocity=100)\n",
    "Write the notes to a MIDI file.\n",
    "Parameters notes : numpy array, shape (num_notes, 2)\n",
    "Notes, one per row (column definition see notes).\n",
    "filename : str\n",
    "Output MIDI file.\n",
    "duration : float, optional\n",
    "Note duration if not defined by notes.\n",
    "velocity : int, optional\n",
    "Note velocity if not defined by notes.\n",
    "Returns numpy array\n",
    "Notes (including note length and velocity).\n",
    "Notes\n",
    "The note columns format must be (duration and velocity being optional):\n",
    "‘note_time’ ‘MIDI_note’ [’duration’ [’MIDI_velocity’]]\n",
    "\n",
    "10.1.1 madmom.utils.midi\n",
    "This module contains MIDI functionality.\n",
    "Almost all code is taken from Giles Hall’s python-midi package: https://github.com/vishnubob/python-midi\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Navigate Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# position on score\n",
    "# playback loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make it a Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate / score\n",
    "# report / compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pa.get_format_from_width(4, unsigned=True)\n",
    "\n",
    "#pa.get_portaudio_version()\n",
    "p = pyaudio.PyAudio()\n",
    "print(\"host api:\")\n",
    "print(pa.PyAudio.get_default_host_api_info(p))\n",
    "print\n",
    "print(\"default input device:\")\n",
    "print(pa.PyAudio.get_default_input_device_info(p))\n",
    "print\n",
    "print(\"default output device:\")\n",
    "print(pa.PyAudio.get_default_output_device_info(p))\n",
    "print\n",
    "print(\"device count\")\n",
    "print(pa.PyAudio.get_device_count(p))\n",
    "print\n",
    "print(\"jack api num\")\n",
    "print(pa.paJACK)\n",
    "print\n",
    "print(\"host api count\")\n",
    "print(pa.PyAudio.get_host_api_count(p))\n",
    "print\n",
    "print(\"host by type\")\n",
    "print(pa.PyAudio.get_host_api_info_by_type(p, paJACK))\n",
    "print\n",
    "print(\"host by index\")\n",
    "print(pa.PyAudio.get_host_api_info_by_index(p))\n",
    "print\n",
    "\n",
    "\n",
    "#get_device_info_by_host_api_device_index()\n",
    "\n",
    "\n",
    "#jackStrm = pyaudio.PaMacCoreStreamInfo()\n",
    "#pyaudio.PaMacCoreStreamInfo(flags=get_flags(), channel_map=get_channel_map())\n",
    "#input_host_api_specific_stream_info(jackStrm)\n",
    "# Return the channel map set at instantiation.\n",
    "# Return the flags set at instantiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from microphone to \n",
    "#if len(sys.argv) < 2:\n",
    "#    print(\"Plays a wave file.\\n\\nUsage: %s filename.wav\" % sys.argv[0])\n",
    "#    sys.exit(-1)\n",
    "\n",
    "#wf = wave.open(sys.argv[1], 'rb')\n",
    "\n",
    "'''PaMacCoreStreamInfo is a PortAudio Host API Specific Stream Info data structure for specifying Mac OS X-only \n",
    "settings. Instantiate this class and pass the instance as the argument in PyAudio.open() to parameters \n",
    "input_host_api_specific_stream_info or output_host_api_specific_stream_info. (See Stream.__init__().)'''\n",
    "\n",
    "strm = pa.PaMacCoreStreamInfo()\n",
    "jackStrm = pa.PaMacCoreStreamInfo(flags=get_flags(), channel_map=get_channel_map())\n",
    "\n",
    "input_host_api_specific_stream_info(jackStrm)\n",
    "# Return the channel map set at instantiation.\n",
    "# Return the flags set at instantiation.\n",
    "\n",
    "'''pyaudio.PyAudio Python interface to PortAudio. Provides methods to: initialize and terminate PortAudio, \n",
    "open and close streams, query and inspect the available PortAudio Host APIs, query and inspect the available\n",
    "PortAudio audio devices. Use this class to open and close streams.'''\n",
    "\n",
    "# instantiate PyAudio (1)\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# define callback (2)\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    data = wf.readframes(frame_count)\n",
    "    return (data, pyaudio.paContinue)\n",
    "\n",
    "# open stream using callback (3)\n",
    "stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                channels=wf.getnchannels(),\n",
    "                rate=wf.getframerate(),\n",
    "                output=True,\n",
    "                stream_callback=callback)\n",
    "\n",
    "'''Use PyAudio.open() to make a new Stream. pyaudio.Stream(PA_manager, rate, channels, format, input=False, output=False, input_device_index=None, \n",
    "output_device_index=None, frames_per_buffer=1024, start=True, input_host_api_specific_stream_info=None, \n",
    "output_host_api_specific_stream_info=None, stream_callback=None)'''\n",
    "\n",
    "# start the stream (4)\n",
    "stream.start_stream()\n",
    "\n",
    "# wait for stream to finish (5)\n",
    "while stream.is_active():\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# stop stream (6)\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "wf.close()\n",
    "\n",
    "# close PyAudio (7)\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### music search and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Audio fingerprinting and recognition in Python: https://github.com/worldveil/dejavu \n",
    "# landmark based Landmark-based audio fingerprinting: https://github.com/dpwe/audfprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_chords(savTo, inDF):\n",
    "    np.savetxt(savTo, inDF,\n",
    "               fmt=['%.3f', '%.3f', '%.0f', '%s', '%.0f', '%s', '%s', '%s', '%s'], delimiter='\\t')\n",
    "    \n",
    "def load_chords(inFname):   \n",
    "    names = ['OnsetTime', 'OffsetTime', 'm21RootPitchClass', \n",
    "             'm21RootPitchAccidental', 'm21RootOctave', \n",
    "             'm21ContainsTriad', 'm21ChordQuality', \n",
    "             'm21PitchedCommonName', 'madChordLabel']\n",
    "    \n",
    "    return pd.read_table(inFname, names = names)\n",
    "\n",
    "# test:\n",
    "# tmp = load_chords('data/wip/AkPnBcht/MUS/MAPS_MUS-ty_mai_AkPnBcht.chords.dnn.txt')\n",
    "\n",
    "def save_list(toSave, toDir, toName, toType):\n",
    "    fullName = ''.join([toDir, toName, toType])\n",
    "    with open(fullName, 'w') as f:\n",
    "        for item in toSave:\n",
    "            f.write(item + '\\n')\n",
    "\n",
    "def pickle_it(toPick, toDir, toName):\n",
    "    with open(toDir + toName, 'wb') as f:\n",
    "        pickle.dump(toPick, f)\n",
    "        print(\"pickled:\", toName)\n",
    "\n",
    "def unpickle_it(frmDir, frmName):\n",
    "    with open(frmDir + frmName, 'rb') as f:\n",
    "        return(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NEW_CHORD_DTYPE = [('OnsetTime', '%.3f'), ('OffsetTime', '%.3f'), \n",
    "#                       ('m21RootPitchClass', np.int), ('m21RootPitchAccidental', np.str),\n",
    "#                       ('m21RootOctave', np.int), ('m21ContainsTriad', np.bool),\n",
    "#                       ('m21ChordQuality', np.str),('m21PitchedCommonName', np.str),\n",
    "#                       ('madChordLabel', np.str)] # 'U32'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put in \"format\" or \"transform\" function\n",
    "\n",
    "# txt_to_y format: [OnsetTime, OffsetTime, Midipitch]\n",
    "\n",
    "def flip_formats(inFile, frmFrmt, toFrmt):\n",
    "    \n",
    "    # valid to/froms: madNote, mapNote, madChord, m21Chord\n",
    "    \n",
    "    inHt, inWd = inFile.shape\n",
    "    \n",
    "    # data types\n",
    "    MAD_NOTE_HEADER_1 = ['note_time', 'MIDI_note']\n",
    "    MAD_NOTE_HEADER_2 = ['note_time', 'MIDI_note', 'duration']\n",
    "    MAD_NOTE_HEADER_3 = ['note_time', 'MIDI_note', 'duration', 'MIDI_note']\n",
    "        \n",
    "    MAD_NOTE_DTYPE_1 = [('note_time', np.float), ('MIDI_note', np.int)]\n",
    "    MAD_NOTE_DTYPE_2 = [('note_time', np.float), ('MIDI_note', np.int), ('duration', np.float)]\n",
    "    MAD_NOTE_DTYPE_3 = [('note_time', np.float), ('MIDI_note', np.int), ('duration', np.float), ('MIDI_note', np.int)]\n",
    "    \n",
    "    MAD_CHORD_DTYPE = [('start', np.float), ('end', np.float), ('label', 'U32')]\n",
    "\n",
    "    MAP_NOTE_HEADER = ['OnsetTime', 'OffsetTime', 'MidiPitch']\n",
    "    MAP_CHORD_HEADER = ['OnsetTime', 'OffsetTime', 'ChordLabel']\n",
    "    \n",
    "    #M21_NOTE_DTYPE = 'f4,f4,int'\n",
    "    #M21_CHORD_DTYPE = 'f4,f4,S10'\n",
    "    \n",
    "    if frmFrmt == \"madNote\":\n",
    "        \n",
    "        if toFrmt == \"mapNote\" or toFrmt == \"m21Chord\" or toFrmt == \"madChord\":\n",
    "            \n",
    "            notes = np.zeros(shape=(inHt, 3), dtype=float) # dtype defaults to float64\n",
    "            notes[:,0] = inFile[:,0]\n",
    "            notes[:,2] = inFile[:,1].astype(int)\n",
    "            \n",
    "            # if no duration\n",
    "            if inFile.shape[1] == 2:\n",
    "                notes[:,1] = 0\n",
    "                \n",
    "            # if duration\n",
    "            else:\n",
    "                notes[:,1] = inFile[:,0] + inFile[:,2]\n",
    "                \n",
    "            if toFrmt == \"mapNote\":\n",
    "                return(notes)\n",
    "        \n",
    "            elif toFrmt == \"m21Chord\" or toFrmt == \"madChord\":\n",
    "                \n",
    "                notes = pd.DataFrame(notes, columns=['OnsetTime', 'OffsetTime', 'MidiPitch'])\n",
    "                notes = tmp.round({'OnsetTime': 2, 'OffsetTime': 2, 'MidiPitch': 0})\n",
    "                notes = tmp.sort_values(['OnsetTime', 'MidiPitch'],\n",
    "                                      axis=0, ascending=True, inplace=False,\n",
    "                                      kind='quicksort', na_position='last')\n",
    "                \n",
    "                notes[\"MidiPitch\"] = tmp['MidiPitch'].astype(int)\n",
    "                \n",
    "                notes, chords = txt_to_y(tmp, mode=\"thisFile\")\n",
    "                \n",
    "                if toFrmt == \"m21Chord\":\n",
    "                    return(chords)\n",
    "            \n",
    "                elif toFrmt == \"madChord\":\n",
    "                    # NOTE: UNTIL YOU FIND OUT HOW THEY'RE CLASSING THEIR CHORDS, DON'T SPEND TIME HERE.\n",
    "                    lines = [line.rstrip('\\n').split('\\t') for line in open('data/mad2m21map.txt', 'U')]\n",
    "                    headers = lines[0]; lines = lines[1:len(lines)]\n",
    "                    lines = pd.DataFrame(lines, columns= headers)\n",
    "                    \n",
    "                    d = dict(zip(lines.m21_chord_pitchedCommonName, lines.mad_chord))\n",
    "                    \n",
    "                    for i in range(chords.shape[0]):\n",
    "                        try:\n",
    "                            chords.ChordLabel.iloc[i] = d[chords.ChordLabel.iloc[i]]\n",
    "                        except:\n",
    "                            chords.ChordLabel.iloc[i] = 'N'\n",
    "                            \n",
    "                    return(chords)\n",
    "        # NOTE: DON'T SPEND TIME GOING FROM MAD.CHORD LABELS TO M21. JUST USE MODEL OUTPUTS TO PREDICT. \n",
    "                \n",
    "                \n",
    "\n",
    "#print(flip_formats(rnn_note_detect, \"madNote\", \"mapNote\"))\n",
    "print(flip_formats(rnn_note_detect, \"madNote\", \"m21Chord\"))\n",
    "print(flip_formats(rnn_note_detect, \"madNote\", \"madChord\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sonic visualizer: music analysis http://www.sonicvisualiser.org/\n",
    "vamp plugins: http://www.vamp-plugins.org/download.html\n",
    "bbc plugins: human/music, intensity, energy https://github.com/bbcrd/bbc-vamp-plugins/blob/master/README.md\n",
    "chordino: maj/min chord recognition http://www.isophonics.net/nnls-chroma\n",
    "music matching: http://www.eecs.qmul.ac.uk/~simond/match/index.html\n",
    "\n",
    "building plugins: http://www.vamp-plugins.org/develop.html\n",
    "annotations: http://www.vamp-plugins.org/sonic-annotator/\n",
    "\n",
    "musescore: score notation from midi, music xml https://github.com/musescore/MuseScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1: Import piano train / test data\n",
    "\n",
    "1. Take their lists of train and test data\n",
    "2. Create list object\n",
    "3. Feed it to a process to either iteratively or bulk load files from directory\n",
    "4. Perform log scale transform of input wav\n",
    "5. Come back to other features, chords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to map sound to notes\n",
    "\n",
    "1. Take professional music sound files\n",
    "2. Play, logging spectrum (frequency / time) and other attributes\n",
    "    a. https://github.com/tyiannak/pyAudioAnalysis/wiki/3.-Feature-Extraction, OR\n",
    "    b. \n",
    "3. Predict notes based on sound\n",
    "    a. input spectrum is the \"X\"\n",
    "    b. sheet music notes are the \"y\" (notes A, B, C, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1: Generate harp note / chord train / test files\n",
    "\n",
    "Garageband or other to generate:\n",
    "1. individual instrument note (pitch?), chord by major/minor, octave, inversion\n",
    "2. sequence files w/ varying amounts of spacing\n",
    "3. mp3's of classical music for which you can easily veryify the notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Standard: Fingerprint music files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n. Fingerprint music\n",
    "\n",
    "https://github.com/dpwe/audfprint\n",
    "\n",
    "Audfprint is a python (and Matlab) script that can take a list of soundfiles and create a database of landmarks, and then subsequently take one or more query audio files and match them against the previously-created database.  The fingerprint is robust to things like time skews, different encoding schemes, and even added noise. It can match small fragments of sound, down to 10 sec or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use FFT, MFCC, etc to create a fignerprint of each of the music files s/t when user starts playing, you can take notes they've played and match "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Streaming: Extract note Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: Streaming: Search / match sample to fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display mode\n",
    "\n",
    "1. Accept song selection\n",
    "2. Load / display sheet music\n",
    "3. Listen for start\n",
    "4. Recieve sounds / translate notes\n",
    "5. Track progress w/ vertical bar\n",
    "6. Spot repeats re-setting tracking bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/tyiannak/pyAudioAnalysis\n",
    "# http://essentia.upf.edu/documentation/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate mode\n",
    "\n",
    "1. display mode functionality including tracking progress\n",
    "2. record playing. overlay repeats. you're tracking stats. so, obj s/b:\n",
    "    a. to get through song re-playing pieces as required, THEN\n",
    "    b. to get through song cleanly\n",
    "3. comparing played to professional\n",
    "    a. option to play:\n",
    "        i. metrinome \n",
    "        ii. professional a low volume\n",
    "4. identify discrpancies (timing after prior note, incorrect note)\n",
    "    a. Gaia, a C++ library with python bindings which implement similarity measures and classification on the results of audio analysis, and generate classification models that Essentia can use to compute high-level description of music.\n",
    "5. show discrepancies\n",
    "    a. accept tolerances (+/- time, other?)\n",
    "    b. show played note in red (i.e., before/after, above/below).\n",
    "6. show / log statistics\n",
    "    a. accuracy\n",
    "    b. similarity\n",
    "    c. error types and frequency distribution\n",
    "        i. early,\n",
    "        ii. late\n",
    "        iii. wrong note\n",
    "    d. problem areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive mode\n",
    "\n",
    "1. evaluate mode functionality\n",
    "2. prompt session info and imprint voice of user for command interface\n",
    "    a. \"this is []. the date is []. i'll be practicing for about [] minutes.\"\n",
    "3. voice commands\n",
    "    a. \"replay [] notes\" - defaults to: 5 notes, played version\n",
    "    b. \"replay base [] notes\"\n",
    "    c. \"loop [] notes\" - \n",
    "        ii. Loop [] notes / Stop Loop\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "This project would not be possible without the invaluable assistance of:\n",
    "\n",
    "## Training data\n",
    "\n",
    "The MAPS piano data set. Roughly 40G of piano notes, chords, music assembled by V. Emiya for her PhD thesis at Telecom ParisTech/ENST in 2008 and in conjunction with R. Badeau, B. David for their paper \"Multipitch estimation of piano sounds using a new probabilistic spectral smoothness principle\"<cite data-cite=\"emiya2010multipitch\"></cite>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for installing latex, bibtex and pdf-ing jupyter notenooks: https://www.youtube.com/watch?v=m3o1KXA1Rjk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
